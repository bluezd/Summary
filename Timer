start_kernel()
   --> tick_init()

/**
 * tick_init - initialize the tick control
 *
 * Register the notifier with the clockevents framework
 */
void __init tick_init(void)
{
        clockevents_register_notifier(&tick_notifier);
}


static struct notifier_block tick_notifier = {
        .notifier_call = tick_notify,
};

/**
 * clockevents_register_notifier - register a clock events change listener
 */
int clockevents_register_notifier(struct notifier_block *nb)
{
        // nb = &tick_notifier
        unsigned long flags;
        int ret;

        spin_lock_irqsave(&clockevents_lock, flags);
        ret = raw_notifier_chain_register(&clockevents_chain, nb);
        spin_unlock_irqrestore(&clockevents_lock, flags);
        
        return ret;
}

/* Notification for clock events */
static RAW_NOTIFIER_HEAD(clockevents_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

#define RAW_NOTIFIER_INIT(name) {                               \
                .head = NULL }

struct raw_notifier_head {
        struct notifier_block *head;
};

struct notifier_block {
        int (*notifier_call)(struct notifier_block *, unsigned long, void *);
        struct notifier_block *next;
        int priority;
};


int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}

/*
 *      Notifier chain core routines.  The exported routines below
 *      are layered on top of these, with appropriate locking added.
 */

static int notifier_chain_register(struct notifier_block **nl,
                struct notifier_block *n)
{               
        // 此时 *nl = NULL

        while ((*nl) != NULL) {
                if (n->priority > (*nl)->priority)
                        break;
                nl = &((*nl)->next);
        }
        n->next = *nl;
        n->next = NULL;

        rcu_assign_pointer(*nl, n);
        *nl = n;
        // clockevents_chain->head = &tick_notifier;

        return 0;
}

/**
 * rcu_assign_pointer - assign (publicize) a pointer to a newly
 * initialized structure that will be dereferenced by RCU read-side
 * critical sections.  Returns the value assigned.
 *
 * Inserts memory barriers on architectures that require them
 * (pretty much all of them other than x86), and also prevents
 * the compiler from reordering the code that initializes the
 * structure after the pointer assignment.  More importantly, this
 * call documents which pointers will be dereferenced by RCU read-side
 * code.
 */             

#define rcu_assign_pointer(p, v) \
        ({ \    
                if (!__builtin_constant_p(v) || \
                    ((v) != NULL)) \
                        smp_wmb(); \
                (p) = (v); \
        })

当时钟事件设备信息发生变化时调用tick_notifier



-----------------------------------------------------------------------------------------------------------------------

start_kernel:
    init_IRQ()

void __init init_IRQ(void)
{
	x86_init.irqs.intr_init();
        // invoke native_init_IRQ() function
}

void __init native_init_IRQ(void)
{                                                                                                                                                          
        int i;

        /* Execute any quirks before the call gates are initialised: */
        x86_init.irqs.pre_vector_init();
        // invoke init_ISA_irqs()

        apic_intr_init();

        /*
         * Cover the whole vector space, no vector can escape
         * us. (some of these will be overridden and become
         * 'special' SMP interrupts)
         */
        for (i = FIRST_EXTERNAL_VECTOR; i < NR_VECTORS; i++) {
                /* IA32_SYSCALL_VECTOR could be used in trap_init already. */
                if (!test_bit(i, used_vectors))
                        set_intr_gate(i, interrupt[i-FIRST_EXTERNAL_VECTOR]);
        }
	// 设置中断门，处理程序指向 interrupt 数组

        if (!acpi_ioapic)
                setup_irq(2, &irq2);

#ifdef CONFIG_X86_32
        /*
         * External FPU? Set up irq13 if so, for
         * original braindamaged IBM FERR coupling.
         */
        if (boot_cpu_data.hard_math && !cpu_has_fpu)
                setup_irq(FPU_IRQ, &fpu_irq);

        irq_ctx_init(smp_processor_id());
#endif
}

#define FIRST_EXTERNAL_VECTOR           0x20
#define NR_VECTORS                       256

void __init init_ISA_irqs(void)
{      
        int i;

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        init_bsp_APIC();
        // 配置本地LAPIC芯片。该函数调用apic_read或apic_write调用全局变量apic的read和write方法
#endif 
        init_8259A(0);

        /*
         * 16 old-style INTA-cycle interrupts:
         */
        for (i = 0; i < NR_IRQS_LEGACY; i++) {
                struct irq_desc *desc = irq_to_desc(i);
               
                desc->status = IRQ_DISABLED;
                desc->action = NULL;
                desc->depth = 1;

                set_irq_chip_and_handler_name(i, &i8259A_chip,
                                              handle_level_irq, "XT");
        }
}

void __init init_bsp_APIC(void)
{
        unsigned int value;
        
        /*
         * Don't do the setup now if we have a SMP BIOS as the
         * through-I/O-APIC virtual wire mode might be active.
         */
        if (smp_found_config || !cpu_has_apic)
                return;
       
        /*
         * Do not trust the local APIC being empty at bootup.
         */
        clear_local_APIC();

        /*
         * Enable APIC.
         */
        value = apic_read(APIC_SPIV);
        value &= ~APIC_VECTOR_MASK;
        value |= APIC_SPIV_APIC_ENABLED;

#ifdef CONFIG_X86_32
        /* This bit is reserved on P4/Xeon and should be cleared */
        if ((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &&
            (boot_cpu_data.x86 == 15))
                value &= ~APIC_SPIV_FOCUS_DISABLED;
        else
#endif
                value |= APIC_SPIV_FOCUS_DISABLED;
        value |= SPURIOUS_APIC_VECTOR;
        apic_write(APIC_SPIV, value);

        /*
         * Set up the virtual wire mode.
         */
        apic_write(APIC_LVT0, APIC_DM_EXTINT);
        value = APIC_DM_NMI;
        if (!lapic_is_integrated())             /* 82489DX */
                value |= APIC_LVT_LEVEL_TRIGGER;
        apic_write(APIC_LVT1, value);
}

// apic 其代表一块LAPIC控制器芯片 init_bsp_APIC函数实际上调用native_apic_mem_read和native_apic_mem_write等方法
struct apic *apic = &apic_default;

static void __init apic_intr_init(void)
{      
        smp_intr_init();

        // 设置终端描述符表 IDT APIC 相关中断服务程序。也就是把位于32~255之间，除去系统调用外其他中断向量的中断处理程序设置为interrupt[i]

#ifdef CONFIG_X86_THERMAL_VECTOR
        alloc_intr_gate(THERMAL_APIC_VECTOR, thermal_interrupt);
#endif
#ifdef CONFIG_X86_MCE_THRESHOLD
        alloc_intr_gate(THRESHOLD_APIC_VECTOR, threshold_interrupt);
#endif
#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_LOCAL_APIC)
        alloc_intr_gate(MCE_SELF_VECTOR, mce_self_interrupt);
#endif  

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        /* self generated IPI for local APIC timer */
        alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
	/*
	 *将中断处理函数直接放在中断门得指向地址，这样只要中断到来，一旦通过中断门将直接跳像中断处理函数，而忽略了irq_d         * esc部分，不需要考虑怎么触发，不需要考虑怎么调度
	 * cat /proc/interrups
	 * LOC:   71070080   67165396   44751028   31680629   Local timer interrupts
	 *
	 * cat /proc/interrupts 也可以看到这些中断与众不同.这些中断左边显示的不是中断请求号，而是一个标识，右边显示的也         * 不是中断控制芯片的信息。
	 * 
	 */
        // 就是这里。。。。。。

        /* IPI for X86 platform specific use */
        alloc_intr_gate(X86_PLATFORM_IPI_VECTOR, x86_platform_ipi);

        /* IPI vectors for APIC spurious and error interrupts */
        alloc_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
        alloc_intr_gate(ERROR_APIC_VECTOR, error_interrupt);

        /* Performance monitoring interrupts: */
# ifdef CONFIG_PERF_EVENTS
        alloc_intr_gate(LOCAL_PENDING_VECTOR, perf_pending_interrupt);
# endif

#endif
}

#define LOCAL_TIMER_VECTOR              0xef   //239

static inline void alloc_intr_gate(unsigned int n, void *addr)
{      
        alloc_system_vector(n);
        set_intr_gate(n, addr);
}

tatic inline void alloc_system_vector(int vector)
{      
        if (!test_bit(vector, used_vectors)) {
                set_bit(vector, used_vectors);
                if (first_system_vector > vector)
                        first_system_vector = vector;
        } else
                BUG();
}

static inline void set_intr_gate(unsigned int n, void *addr)
{      
        BUG_ON((unsigned)n > 0xFF);
        _set_gate(n, GATE_INTERRUPT, addr, 0, 0, __KERNEL_CS);
}

static inline void _set_gate(int gate, unsigned type, void *addr,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate_desc s;
        pack_gate(&s, type, (unsigned long)addr, dpl, ist, seg);
        /*
         * does not need to be atomic because it is only done once at
         * setup time
         */
        write_idt_entry(idt_table, gate, &s);
}

/*
 * X86_64
 */
static inline void pack_gate(gate_desc *gate, unsigned type, unsigned long func,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate->offset_low = PTR_LOW(func);
        gate->segment = __KERNEL_CS;
        gate->ist = ist;
        gate->p = 1;         
        gate->dpl = dpl;     
        gate->zero0 = 0;
        gate->zero1 = 0;
        gate->type = type;
        gate->offset_middle = PTR_MIDDLE(func);
        gate->offset_high = PTR_HIGH(func);
}


BUILD_INTERRUPT(apic_timer_interrupt,LOCAL_TIMER_VECTOR)

#define BUILD_INTERRUPT(name, nr)       BUILD_INTERRUPT3(name, nr, smp_##name)

/*
 *  Irq entries should be protected against kprobes
 */
        .pushsection .kprobes.text, "ax"
#define BUILD_INTERRUPT3(name, nr, fn)  \
ENTRY(name)                             \
        RING0_INT_FRAME;                \
        pushl $~(nr);                   \
        CFI_ADJUST_CFA_OFFSET 4;        \
        SAVE_ALL;                       \
        TRACE_IRQS_OFF                  \
        movl %esp,%eax;                 \
        call fn;                        \
        jmp ret_from_intr;              \
        CFI_ENDPROC;                    \
ENDPROC(name)

当 cpu 执行完一条指令后会见查是否有中断发生，此时 assume Local APIC 发生中断请求，cpu 则根据中断号在中断描述符表中查找到段选择子，进而找到处理程序，此时就是执行 apic_timer_interrupt,然后 call smp_apic_timer_interrupt ...... 于是发生了一些很重要的事情。

-----------------------------------------------------------------------------------------------------------------------

start_kernel:
     init_timers();

void __init init_timers(void)
{       
        int err = timer_cpu_notify(&timers_nb, (unsigned long)CPU_UP_PREPARE,
                                (void *)(long)smp_processor_id());
        
        init_timer_stats();
        
        BUG_ON(err == NOTIFY_BAD);
        register_cpu_notifier(&timers_nb);
        open_softirq(TIMER_SOFTIRQ, run_timer_softirq);
        // 软中断 下半步 高精度 timer.
}

static struct notifier_block __cpuinitdata timers_nb = {
        .notifier_call  = timer_cpu_notify,
};

初始化指定CPU上的软时钟相关的数据结构
static int __cpuinit timer_cpu_notify(struct notifier_block *self,
                                unsigned long action, void *hcpu)
{       
        long cpu = (long)hcpu;
        switch(action) {
        case CPU_UP_PREPARE:
        case CPU_UP_PREPARE_FROZEN:
                if (init_timers_cpu(cpu) < 0)
                        return NOTIFY_BAD;
                break;
#ifdef CONFIG_HOTPLUG_CPU
        case CPU_DEAD:
        case CPU_DEAD_FROZEN:
                migrate_timers(cpu);
                break;
#endif
        default:
                break;
        }
        return NOTIFY_OK;
}

static int __cpuinit init_timers_cpu(int cpu)
{
        int j;
        struct tvec_base *base;
        static char __cpuinitdata tvec_base_done[NR_CPUS];

        if (!tvec_base_done[cpu]) {
                static char boot_done;

                if (boot_done) {
                        /*
                         * The APs use this path later in boot
                         */
                        base = kmalloc_node(sizeof(*base),
                                                GFP_KERNEL | __GFP_ZERO,
                                                cpu_to_node(cpu));
                        if (!base)
                                return -ENOMEM;

                        /* Make sure that tvec_base is 2 byte aligned */
                        if (tbase_get_deferrable(base)) {
                                WARN_ON(1);
                                kfree(base);
                                return -ENOMEM;
                        }
                        per_cpu(tvec_bases, cpu) = base;
                } else {
                        /*
                         * This is for the boot CPU - we use compile-time
                         * static initialisation because per-cpu memory isn't
                         * ready yet and because the memory allocators are not
                         * initialised either.
                         */
                        boot_done = 1;
                        base = &boot_tvec_bases;
                }
                tvec_base_done[cpu] = 1;
        } else {
                base = per_cpu(tvec_bases, cpu);
        }

        spin_lock_init(&base->lock);

        for (j = 0; j < TVN_SIZE; j++) {
                INIT_LIST_HEAD(base->tv5.vec + j);
                INIT_LIST_HEAD(base->tv4.vec + j);
                INIT_LIST_HEAD(base->tv3.vec + j);
                INIT_LIST_HEAD(base->tv2.vec + j);
        }
        for (j = 0; j < TVR_SIZE; j++)
                INIT_LIST_HEAD(base->tv1.vec + j);

        base->timer_jiffies = jiffies;
        // 设置成自系统启动以来产生的节拍数。
        base->next_timer = base->timer_jiffies;
        return 0;
}

struct tvec_base {
        spinlock_t lock;
        struct timer_list *running_timer;
        unsigned long timer_jiffies;
        unsigned long next_timer;
        struct tvec_root tv1;
        struct tvec tv2;
        struct tvec tv3;
        struct tvec tv4;
        struct tvec tv5;
} ____cacheline_aligned;


static inline uint32_t raid6_jiffies(void)
{               
        struct timeval tv;
        gettimeofday(&tv, NULL);
        return tv.tv_sec*1000 + tv.tv_usec/1000;
}    

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
};


static __always_inline int gettimeofday(struct timeval *tv, struct timezone *tz)
{
        int ret;
        asm volatile("syscall"
                : "=a" (ret)
                : "0" (__NR_gettimeofday),"D" (tv),"S" (tz)
                : __syscall_clobber );
        return ret;
}       


 +------------------------------+
 |                              |
 |    系统调用 gettimeofday     |
 |                              |
 +------------------------------+

SYSCALL_DEFINE2(gettimeofday, struct timeval __user *, tv,
                struct timezone __user *, tz)
{      
        if (likely(tv != NULL)) {                                                                                                              
                struct timeval ktv;                                                                                                            
                do_gettimeofday(&ktv);
                if (copy_to_user(tv, &ktv, sizeof(ktv)))
                        return -EFAULT;
                // 操作系统和驱动程序在内核空间运行，应用程序在用户空间运行，两者不能简单地使用指针传递数据. 因为Linux系统使用了虚拟内存机制，用户空间的内存可能被换出，当内核空间使用用户空间指针时，对应的数据可能不在内存中
        }
        if (unlikely(tz != NULL)) {
                if (copy_to_user(tz, &sys_tz, sizeof(sys_tz)))
                        return -EFAULT;
        }
        return 0;
}

#define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)



/**             
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:         pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{       
        struct timespec now;
                
        getnstimeofday(&now);
	// now 中返回当前时间 秒数以及纳秒数
	
        tv->tv_sec = now.tv_sec;
	// 秒数
        tv->tv_usec = now.tv_nsec/1000;
	// now.tv_nsec 不足一秒的纳秒数 转换为微妙数.
}       

CONFIG_GENERIC_TIME=y
// rhel6

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
}; 

/**
 * getnstimeofday - Returns the time of day in a timespec
 * @ts:         pointer to the timespec to be set
 *
 * Returns the time of day in a timespec.
 */
void getnstimeofday(struct timespec *ts)
{       
        unsigned long seq;
        s64 nsecs;

        WARN_ON(timekeeping_suspended);

        do {
                seq = read_seqbegin(&xtime_lock);
		// 加读锁，如果此时已经加上的写锁，则自旋

                *ts = xtime;
		// 此时很有可能更新 xtime.(时钟中断处理程序)
		
                nsecs = timekeeping_get_ns();

                /* If arch requires, add in gettimeoffset() */
                nsecs += arch_gettimeoffset(); // NULL function ?

        } while (read_seqretry(&xtime_lock, seq));
	// 顺序锁，判断是否重读. 因为此时可能某个进程写时钟源.这样读出来的值就不准确需要重读.

        timespec_add_ns(ts, nsecs);
        // add nsecs 到 struct timespec 结构.
}

 +--------------------------------------------------------------------------------------------------------------------+
 |                                                                                                                    |
 |无非三种情况:                                                                                                       |
 |  1 循环只执行一遍，准确获得当前时间                                                                                |
 |  2 顺利加上读锁，但是与此同时时钟中断到来需要更新xtime,加上写锁(有更高的优先级)，让后 read_seqretry 返回 1 表示时间|      已经不准却需要重新计算。在再次循环读取。
 |  3 读锁阻塞，因为此时已经加上了写锁(更新 xtime),等到写锁释放后便可读取.                                            |
 |                                                                                                                    |
 +--------------------------------------------------------------------------------------------------------------------+


truct timespec {
        time_t  tv_sec;         /* seconds */
        long    tv_nsec;        /* nanoseconds */
};

struct timespec xtime __attribute__ ((aligned (16))); 

/*                                                                                      
 * This read-write spinlock protects us from races in SMP while                         
 * playing with xtime.                                                                  
 */                                                                                     
__cacheline_aligned_in_smp DEFINE_SEQLOCK(xtime_lock);                   

#define DEFINE_SEQLOCK(x) \
                seqlock_t x = __SEQLOCK_UNLOCKED(x)                                     

typedef struct {                                                                        
        unsigned sequence;                                                    
        spinlock_t lock;                                                                
} seqlock_t;

// 记录了写着进程访问临界资源的过程 初始化为 0 写时 +1,解除写锁时再 +1 ,该直为奇数时处于写>锁定状态                    
// 读直接访问不需要加锁

#define __SEQLOCK_UNLOCKED(lockname) \
                 { 0, __SPIN_LOCK_UNLOCKED(lockname) }

# define __SPIN_LOCK_UNLOCKED(lockname) \
        (spinlock_t)    {       .raw_lock = __RAW_SPIN_LOCK_UNLOCKED,   \
                                SPIN_DEP_MAP_INIT(lockname) }


/* Start of read calculation -- fetch last complete writer token */
static __always_inline unsigned read_seqbegin(const seqlock_t *sl)
{
        unsigned ret;

repeat:
        ret = sl->sequence;
        smp_rmb();
        if (unlikely(ret & 1)) {
                cpu_relax();
                goto repeat;
        }

        return ret;
}


/*              
 * Test if reader processed invalid data.
 *
 * If sequence value changed then writer changed data while in section.
 */             
static __always_inline int read_seqretry(const seqlock_t *sl, unsigned start)
{      
	// 假设 start = 0
        smp_rmb();
        
        return (sl->sequence != start);
}       




/* Timekeeper helper functions. */
static inline s64 timekeeping_get_ns(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;

        /* read clocksource: */
        clock = timekeeper.clock;
        // 时钟源 PIT/HPET/TSC

        cycle_now = clock->read(clock);
        // 读取时钟周期的当前计数值 假设为 TSC --> read_tsc()

        /* calculate the delta since the last update_wall_time: */
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
	// clock->cycle_last 应该由时钟中断处理程序负责更新.

        /* return delta convert to nanoseconds using ntp adjusted mult. */
        return clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
	//转换成纳秒
}

/* Structure holding internal timekeeping values. */
struct timekeeper {
        /* Current clocksource used for timekeeping. */
        struct clocksource *clock;
        /* The shift value of the current clocksource. */
        int     shift;

        /* Number of clock cycles in one NTP interval. */
        cycle_t cycle_interval; 
        /* Number of clock shifted nano seconds in one NTP interval. */
        u64     xtime_interval;
        /* Raw nano seconds accumulated per NTP interval. */
        u32     raw_interval;

        /* Clock shifted nano seconds remainder not stored in xtime.tv_nsec. */
        u64     xtime_nsec;
        /* Difference between accumulated time and NTP time in ntp
         * shifted nano seconds. */
        s64     ntp_error;
        /* Shift conversion between clock shifted nano seconds and
         * ntp shifted nano seconds. */
        int     ntp_error_shift;
        /* NTP adjusted clock multiplier */
        u32     mult;
};

typedef u64 cycle_t;

/**     
 *  * clocksource_cyc2ns - converts clocksource cycles to nanoseconds
 *   *      
 *    * Converts cycles to nanoseconds, using the given mult and shift.
 *     *      
 *      * XXX - This could use some mult_lxl_ll() asm optimization
 *       */     
static inline s64 clocksource_cyc2ns(cycle_t cycles, u32 mult, u32 shift)
{       
	        return ((u64) cycles * mult) >> shift;
}

/**
 *  * timespec_add_ns - Adds nanoseconds to a timespec
 *   * @a:          pointer to timespec to be incremented
 *    * @ns:         unsigned nanoseconds value to be added
 *     *              
 *      * This must always be inlined because its used from the x86-64 vdso,
 *       * which cannot call other kernel functions.
 *        */             
static __always_inline void timespec_add_ns(struct timespec *a, u64 ns)
{
	        a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, NSEC_PER_SEC, &ns);
                // 返回 秒数 一般情况下返回 0
		a->tv_nsec = ns;
		// 不足 1s 的纳秒数 (此值会一直叠加)
}       


static __always_inline u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{       
	// dividend = a->tv_nsec + ns
        u32 ret = 0;

        while (dividend >= divisor) {
		// 如果 纳秒大于一秒 NSEC_PER_SEC 

                /* The following asm() prevents the compiler from
                   optimising this loop into a modulo operation.  */
                asm("" : "+rm"(dividend));

                dividend -= divisor;
		// 减去一秒

                ret++;
		// 秒数加1
        }
        
        *remainder = dividend;

        return ret;
}

 +-------------------+
 |gettimeofday over! |
 +-------------------+


void __init init_timer_stats(void)
{                                    
        int cpu;
        
        for_each_possible_cpu(cpu)
                spin_lock_init(&per_cpu(lookup_lock, cpu));
}      

/* Need to know about CPUs going up/down? */
int __ref register_cpu_notifier(struct notifier_block *nb)
{
        nb=&timers_nb

        int ret;
        cpu_maps_update_begin();
        ret = raw_notifier_chain_register(&cpu_chain, nb);
        cpu_maps_update_done();
        return ret;
}

static __cpuinitdata RAW_NOTIFIER_HEAD(cpu_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

void cpu_maps_update_begin(void)
{
        mutex_lock(&cpu_add_remove_lock);
}

/**
 *      raw_notifier_chain_register - Add notifier to a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @n: New entry in notifier chain
 *
 *      Adds a notifier to a raw notifier chain.
 *      All locking must be provided by the caller.
 *
 *      Currently always returns zero.
 */
int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    hrtimers_init();

void __init hrtimers_init(void)
{
        hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
                          (void *)(long)smp_processor_id());
        register_cpu_notifier(&hrtimers_nb);
#ifdef CONFIG_HIGH_RES_TIMERS
        open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);
#endif
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    timekeeping_init();

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
        struct clocksource *clock;
        unsigned long flags;
        struct timespec now, boot;

        read_persistent_clock(&now);
	// 读取 RTC chip,get the UTC time
	
        read_boot_clock(&boot);

        write_seqlock_irqsave(&xtime_lock, flags);

        ntp_init();

        clock = clocksource_default_clock();
        // clock = &clocksource_jiffies
    
        if (clock->enable)
                clock->enable(clock);
        timekeeper_setup_internals(clock);

        xtime.tv_sec = now.tv_sec;
        xtime.tv_nsec = now.tv_nsec;
        raw_time.tv_sec = 0;
        raw_time.tv_nsec = 0;
        if (boot.tv_sec == 0 && boot.tv_nsec == 0) {
                boot.tv_sec = xtime.tv_sec;
                boot.tv_nsec = xtime.tv_nsec;
        }
        set_normalized_timespec(&wall_to_monotonic,
                                -boot.tv_sec, -boot.tv_nsec);
        update_xtime_cache(0);
        total_sleep_time.tv_sec = 0;
        total_sleep_time.tv_nsec = 0;
        write_sequnlock_irqrestore(&xtime_lock, flags);
}


/* not static: needed by APM */
void read_persistent_clock(struct timespec *ts)
{
        unsigned long retval, flags;

        spin_lock_irqsave(&rtc_lock, flags);
        retval = x86_platform.get_wallclock();
        // invoke the mach_get_cmos_time()

        spin_unlock_irqrestore(&rtc_lock, flags);

        ts->tv_sec = retval;
        ts->tv_nsec = 0;
}



static cycle_t jiffies_read(struct clocksource *cs)
{
	return (cycle_t) jiffies;
}

struct clocksource clocksource_jiffies = {
	.name		= "jiffies",
	.rating		= 1, /* lowest valid rating*/
	.read		= jiffies_read,
	.mask		= 0xffffffff, /*32bits*/
	.mult		= NSEC_PER_JIFFY << JIFFIES_SHIFT, /* details above */
	.shift		= JIFFIES_SHIFT,
};

#define NSEC_PER_JIFFY	((u32)((((u64)NSEC_PER_SEC)<<8)/ACTHZ))

#define ACTHZ (SH_DIV (CLOCK_TICK_RATE, LATCH, 8))

#define CLOCK_TICK_RATE		PIT_TICK_RATE
#define PIT_TICK_RATE 1193182ul
#define LATCH  ((CLOCK_TICK_RATE + HZ/2) / HZ)	/* For divider */


#define SH_DIV(NOM,DEN,LSH) (   (((NOM) / (DEN)) << (LSH))              \
                             + ((((NOM) % (DEN)) << (LSH)) + (DEN) / 2) / (DEN))

HZ=1000

#define NSEC_PER_SEC	1000000000L


/**
 *  * read_boot_clock -  Return time of the system start.
 *   *
 *    * Weak dummy function for arches that do not yet support it.
 *     * Function to read the exact time the system has been started.
 *      * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported.
 *       *      
 *        *  XXX - Do be sure to remove it once all arches implement it.
 *         */
void __attribute__((weak)) read_boot_clock(struct timespec *ts)
{
	        ts->tv_sec = 0;
		ts->tv_nsec = 0;
}       

void __init ntp_init(void)
{       
        ntp_clear();
        hrtimer_init(&leap_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
        leap_timer.function = ntp_leap_second;
}       

/**
 * ntp_clear - Clears the NTP state variables
 *
 * Must be called while holding a write on the xtime_lock
 */
void ntp_clear(void)
{
        time_adjust     = 0;            /* stop active adjtime() */
        time_status     |= STA_UNSYNC;
        time_maxerror   = NTP_PHASE_LIMIT;
        time_esterror   = NTP_PHASE_LIMIT;

        ntp_update_frequency();

        tick_length     = tick_length_base;
        time_offset     = 0;
}

/**     
 * hrtimer_init - initialize a timer to the given clock
 * @timer:      the timer to be initialized
 * @clock_id:   the clock to be used
 * @mode:       timer mode abs/rel
 */     
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                  enum hrtimer_mode mode)
{
        debug_init(timer, clock_id, mode);
        __hrtimer_init(timer, clock_id, mode);
}       
EXPORT_SYMBOL_GPL(hrtimer_init);

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                           enum hrtimer_mode mode)
{               
        struct hrtimer_cpu_base *cpu_base;
        
        memset(timer, 0, sizeof(struct hrtimer));
        
        cpu_base = &__raw_get_cpu_var(hrtimer_bases);
                
        if (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)
                clock_id = CLOCK_MONOTONIC;
                                         
        timer->base = &cpu_base->clock_base[clock_id];
        hrtimer_init_timer_hres(timer);
        
#ifdef CONFIG_TIMER_STATS  
        timer->start_site = NULL;
        timer->start_pid = -1;
        memset(timer->start_comm, 0, TASK_COMM_LEN);
#endif  
}


struct clocksource * __init __weak clocksource_default_clock(void)
{
        return &clocksource_jiffies;
}


/**
 * timekeeper_setup_internals - Set up internals to use clocksource clock.
 *
 * @clock:              Pointer to clocksource.
 *
 * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
 * pair and interval request.
 *
 * Unless you're the timekeeping code, you should not be using this!
 */
static void timekeeper_setup_internals(struct clocksource *clock)
{
        cycle_t interval;
        u64 tmp;

        timekeeper.clock = clock;
	// 设置时钟源 time_keeper gettimeofday 会通过 timer_keeper layer 获取 wall time
	
        clock->cycle_last = clock->read(clock);

        /* Do the ns -> cycle conversion first, using original mult */
        tmp = NTP_INTERVAL_LENGTH;
        tmp <<= clock->shift;
        tmp += clock->mult/2;
        do_div(tmp, clock->mult);
        if (tmp == 0)
                tmp = 1;

        interval = (cycle_t) tmp;
        timekeeper.cycle_interval = interval;

        /* Go back from cycles -> shifted ns */
        timekeeper.xtime_interval = (u64) interval * clock->mult;
        timekeeper.raw_interval =
                ((u64) interval * clock->mult) >> clock->shift;

        timekeeper.xtime_nsec = 0;
        timekeeper.shift = clock->shift;

         负责更新系统时间。timekeeper.ntp_error = 0;
        timekeeper.ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;

        /*
         * The timekeeper keeps its own mult values for the currently
         * active clocksource. These value will be adjusted via NTP
         * to counteract clock drifting.
         */
        timekeeper.mult = clock->mult;
}



------------------------------------------------------------------------------------------------------------------


/*
 * Initialize TSC and delay the periodic timer init to
 * late x86_late_time_init() so ioremap works.
 */
void __init time_init(void)
{
        late_time_init = x86_late_time_init;
}

start_kernel:
        if (late_time_init)
             late_time_init();

+----------------------------------------------------------------------------------------+
|       /*                                                                               |
|        * The platform setup functions are preset with the default functions            |
|        * for standard PC hardware.                                                     |
|        */                                                                              |
|       struct x86_init_ops x86_init __initdata = {                                      |
|                                                                                        |
|               .resources = {                                                           |
|                       .probe_roms             = probe_roms,                            |
|                       .reserve_resources      = reserve_standard_io_resources,         |
|                       .memory_setup           = default_machine_specific_memory_setup, |
|               },                                                                       |
|                                                                                        |
|               .mpparse = {                                                             |
|                       .mpc_record             = x86_init_uint_noop,                    |
|                       .setup_ioapic_ids       = x86_init_noop,                         |
|                       .mpc_apic_id            = default_mpc_apic_id,                   |
|                       .smp_read_mpc_oem       = default_smp_read_mpc_oem,              |
|                       .mpc_oem_bus_info       = default_mpc_oem_bus_info,              |
|                       .find_smp_config        = default_find_smp_config,               |
|                       .get_smp_config         = default_get_smp_config,                |
|               },                                                                       |
|                                                                                        |
|               .irqs = {                                                                |
|                       .pre_vector_init        = init_ISA_irqs,                         |
|                       .intr_init              = native_init_IRQ,                       |
|                       .trap_init              = x86_init_noop,                         |
|               },                                                                       |
|                                                                                        |
|               .oem = {                                                                 |
|                       .arch_setup             = x86_init_noop,                         |
|                       .banner                 = default_banner,                        |
|               },                                                                       |
|                                                                                        |
|               .paging = {                                                              |
|                       .pagetable_setup_start  = native_pagetable_setup_start,          |
|                       .pagetable_setup_done   = native_pagetable_setup_done,           |
|               },                                                                       |
|                                                                                        |
|               .timers = {                                                              |
|                       .setup_percpu_clockev   = setup_boot_APIC_clock,                 |
|                       .tsc_pre_init           = x86_init_noop,                         |
|                       .timer_init             = hpet_time_init,                        |
|               },                                                                       |
|       };                                                                               |
|                                                                                        |
+----------------------------------------------------------------------------------------+


static __init void x86_late_time_init(void)
{
        x86_init.timers.timer_init();
        tsc_init();
}

/* Default timer init function */
void __init hpet_time_init(void)
{
        if (!hpet_enable())
                setup_pit_timer();
        setup_default_timer_irq();
}

/**
 * hpet_enable - Try to setup the HPET timer. Returns 1 on success.
 */
int __init hpet_enable(void)
{
        unsigned long id;
        int i;

        if (!is_hpet_capable())
                return 0;
                
        hpet_set_mapping();
	// Maybe hpet as a PCI device 获得线性地址

        /*
         * Read the period and check for a sane value:
         */     
        hpet_period = hpet_readl(HPET_PERIOD);
	// 周期
                
        /*      
         * AMD SB700 based systems with spread spectrum enabled use a
         * SMM based HPET emulation to provide proper frequency
         * setting. The SMM code is initialized with the first HPET
         * register access and takes some time to complete. During
         * this time the config register reads 0xffffffff. We check
         * for max. 1000 loops whether the config register reads a non
         * 0xffffffff value to make sure that HPET is up and running
         * before we go further. A counting loop is safe, as the HPET
         * access takes thousands of CPU cycles. On non SB700 based
         * machines this check is only done once and has no side
         * effects.
         */
        for (i = 0; hpet_readl(HPET_CFG) == 0xFFFFFFFF; i++) {
                if (i == 1000) {
                        printk(KERN_WARNING
                               "HPET config register value = 0xFFFFFFFF. "
                               "Disabling HPET\n");
                        goto out_nohpet;
                }
        }
        
        if (hpet_period < HPET_MIN_PERIOD || hpet_period > HPET_MAX_PERIOD)
                goto out_nohpet;

        /*
         * Read the HPET ID register to retrieve the IRQ routing
         * information and the number of channels
         */     
        id = hpet_readl(HPET_ID);
        hpet_print_config();

#ifdef CONFIG_HPET_EMULATE_RTC
        /*
         * The legacy routing mode needs at least two channels, tick timer
         * and the rtc emulation channel.
         */
        if (!(id & HPET_ID_NUMBER))
                goto out_nohpet;
#endif

        if (hpet_clocksource_register())
                goto out_nohpet;

        if (id & HPET_ID_LEGSUP) {
                hpet_legacy_clockevent_register();
                hpet_msi_capability_lookup(2);
                return 1;
        }
        hpet_msi_capability_lookup(0);
        return 0;

out_nohpet:
        hpet_clear_mapping();
        hpet_address = 0;
        return 0;
}

/* Max HPET Period is 10^8 femto sec as in HPET spec */
#define HPET_MAX_PERIOD         100000000UL
/*
 *  * Min HPET period is 10^5 femto sec just for safety. If it is less than this,
 *   * then 32 bit HPET counter wrapsaround in less than 0.5 sec.
 *    */
#define HPET_MIN_PERIOD         100000UL


static int hpet_clocksource_register(void)
{
        u64 start, now;
        cycle_t t1;

        /* Start the counter */
        hpet_restart_counter();

        /* Verify whether hpet counter works */
        t1 = hpet_readl(HPET_COUNTER);
        rdtscll(start);

        /*
         * We don't know the TSC frequency yet, but waiting for
         * 200000 TSC cycles is safe:
         * 4 GHz == 50us
         * 1 GHz == 200us
         */
        do {
                rep_nop();
                rdtscll(now);
        } while ((now - start) < 200000UL);

        if (t1 == hpet_readl(HPET_COUNTER)) {
                printk(KERN_WARNING
                       "HPET counter not counting. HPET disabled\n");
                return -ENODEV;
        }

        /*
         * The definition of mult is (include/linux/clocksource.h)
         * mult/2^shift = ns/cyc and hpet_period is in units of fsec/cyc
         * so we first need to convert hpet_period to ns/cyc units:
         *  mult/2^shift = ns/cyc = hpet_period/10^6
         *  mult = (hpet_period * 2^shift)/10^6
         *  mult = (hpet_period << shift)/FSEC_PER_NSEC
         */
        clocksource_hpet.mult = div_sc(hpet_period, FSEC_PER_NSEC, HPET_SHIFT);

        clocksource_register(&clocksource_hpet);

        return 0;
}

+------------------------------------------------------------------+
|                                                                  |
|       static struct clocksource clocksource_hpet = {             |
|               .name           = "hpet",                          |
|               .rating         = 250,                             |
|               .read           = read_hpet,                       |
|               .mask           = HPET_MASK,                       |
|               .shift          = HPET_SHIFT,                      |
|               .flags          = CLOCK_SOURCE_IS_CONTINUOUS,      |
|               .resume         = hpet_resume_counter,             |
|       #ifdef CONFIG_X86_64                                       |
|               .vread          = vread_hpet,                      |
|       #endif                                                     |
|       };                                                         |
|                                                                  |
+------------------------------------------------------------------+

/**
 * clocksource_register - Used to install new clocksources
 * @t:          clocksource to be registered
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 */     
int clocksource_register(struct clocksource *cs)
{       
        /* calculate max idle time permitted for this clocksource */
        cs->max_idle_ns = clocksource_max_deferment(cs);

        mutex_lock(&clocksource_mutex);
        clocksource_enqueue(cs);
        clocksource_select();
        clocksource_enqueue_watchdog(cs);
        mutex_unlock(&clocksource_mutex);
        return 0;
}       

/*
 * Enqueue the clocksource sorted by rating
 */
static void clocksource_enqueue(struct clocksource *cs)
{
        struct list_head *entry = &clocksource_list;
        // static LIST_HEAD(clocksource_list);
	// head prev 都指向自己

        struct clocksource *tmp;

        list_for_each_entry(tmp, &clocksource_list, list)
                /* Keep track of the place, where to insert */
                if (tmp->rating >= cs->rating)
                        entry = &tmp->list;
        list_add(&cs->list, entry);
	// 插入到双向循环链表中 pit --> hpet --> tsc --> head(clocksource_list)
}


//                           head
                     +-->>+---------+<-+
                     |    |         |  |
                     | +----.next   |  |
                     | |  |         |  |
                     | |  | .prev -----------+
                     | |  |         |  |     |
                     | |  +---------+  |     |
                     | |               |     |
//                   | |     TSC       |     |
                     | +->+---------+<---+   |
                     |    |         |  | |   |
                     | +----.next   |  | |   |
                     | |  |         |  | |   |
                     | |  | .prev -----+ |   |
                     | |  |         |    |   |
                     | |  +---------+    |   |
                     | |                 |   |
//                   | |     HPET        |   |
                     | +->+---------+<-----+ |
                     |    |         |    | | |
                     | +----.next   |    | | |
                     | |  |         |    | | |
                     | |  | .prev -------+ | |
                     | |  |         |      | |
                     | |  +---------+      | |
                     | |                   | |
//                   | |     PIT           | |
                     | +->+---------+<<------+
                     |    |         |      |
                     +------.next   |      |
                          |         |      |
                          | .prev ---------+
                          |         |
                          +---------+

/**
 * clocksource_select - Select the best clocksource available
 *
 * Private function. Must hold clocksource_mutex when called.
 *
 * Select the clocksource with the best rating, or the clocksource,
 * which is selected by userspace override.
 */
static void clocksource_select(void)
{
        struct clocksource *best, *cs;

        if (!finished_booting || list_empty(&clocksource_list))
                return;
        // 此时 finished_booting = 0,所以 closksource_select 函数直接返回。在系统初始化快结束时调用 clocksource_done_booting() 会设置finished_booting = 1,紧接着调用 clocksource_select() 进行真正的时钟源选择。
	
        /* First clocksource on the list has the best rating. */
        best = list_first_entry(&clocksource_list, struct clocksource, list);
        // 如上图中的链表中第一个就是最好的时钟源
	
        /* Check for the override clocksource. */
        list_for_each_entry(cs, &clocksource_list, list) {
                if (strcmp(cs->name, override_name) != 0)
                        continue;
                /*
                 * Check to make sure we don't switch to a non-highres
                 * capable clocksource if the tick code is in oneshot
                 * mode (highres or nohz)
                 */
                if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&
                    tick_oneshot_mode_active()) {
                        /* Override clocksource cannot be used. */
                        printk(KERN_WARNING "Override clocksource %s is not "
                               "HRT compatible. Cannot switch while in "
                               "HRT/NOHZ mode\n", cs->name);
                        override_name[0] = 0;
                } else
                        /* Override clocksource can be used. */
                        best = cs;
                break;
        }
        if (curr_clocksource != best) {
                printk(KERN_INFO "Switching to clocksource %s\n", best->name);
		// Switching to clocksource tsc log from dmesg . :-)

                curr_clocksource = best;
		// 保存当前使用的时钟源

                timekeeping_notify(curr_clocksource);
		// Install a new clocksource
        }
}

static void clocksource_enqueue_watchdog(struct clocksource *cs)
{
        unsigned long flags;
        
        spin_lock_irqsave(&watchdog_lock, flags);
        if (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {
                /* cs is a clocksource to be watched. */
                list_add(&cs->wd_list, &watchdog_list);
                cs->flags &= ~CLOCK_SOURCE_WATCHDOG;
        } else {
                /* cs is a watchdog. */
                if (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)
                        cs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;
                        // 这句最重要

                /* Pick the best watchdog. */
                if (!watchdog || cs->rating > watchdog->rating) {
                        watchdog = cs;
                        /* Reset watchdog cycles */
                        clocksource_reset_watchdog();
                }
        }
        /* Check if the watchdog timer needs to be started. */
        clocksource_start_watchdog();
        spin_unlock_irqrestore(&watchdog_lock, flags);
}


在此之前会注册几种时钟源
比如:
     core_initcall(init_jiffies_clocksource);
     fs_initcall(init_acpi_pm_clocksource);

 +-----------------------------------------+
 | fs_initcall(clocksource_done_booting);  |
 +-----------------------------------------+

#define fs_initcall(fn)                 __define_initcall("5",fn,5)

/*
 * clocksource_done_booting - Called near the end of core bootup
 *
 * Hack to avoid lots of clocksource churn at boot time.
 * We use fs_initcall because we want this to start before
 * device_initcall but after subsys_initcall.
 */
static int __init clocksource_done_booting(void)
{       
        finished_booting = 1;
	// 表示初始化完成
                        
        /*      
         * Run the watchdog first to eliminate unstable clock sources
         */
        clocksource_watchdog_kthread(NULL);

        mutex_lock(&clocksource_mutex);

        clocksource_select();
	// 选择最好的时钟源
	
        mutex_unlock(&clocksource_mutex);
        return 0;       
}                 

LOG:
Switching to clocksource hpet ...... 此时 时钟源的 flag 已经被加上 "cs->flags |= CLOCK_SOURCE_VALID_FOR_HRES",所以此时在相应中断程序下半部中 timekeeping_valid_for_hres() 函数中返回 1 ，这样便可以执行 hrtimer_switch_to_hres() 函数切换到高精度模式，这就是在之前看不到切换的原因，只有在当前时钟源支持高精度，hpet/tsc,在系统启动前期时钟源一直是 PIT,当时钟源切换到 hpet or tsc 时，时钟事件设备才会切换到高精度。这就是为什么每次在clocksource_done_booting（） 函数之后执行切换操作。


/**                     
 * timekeeping_notify - Install a new clock source
 * @clock:              pointer to the clock source
 *      
 * This function is called from clocksource.c after a new, better clock
 * source has been registered. The caller holds the clocksource_mutex.
 */     
void timekeeping_notify(struct clocksource *clock)
{                       
        if (timekeeper.clock == clock)
                return;
	// clock = &clocksource_tsc .
	// 此时 clock = &clocksource_jiffies .
	
        stop_machine(change_clocksource, clock, NULL);
        tick_clock_notify();*
}


/**     
 * Async notification about clocksource changes
 */     
void tick_clock_notify(void)
{
        int cpu;

        for_each_possible_cpu(cpu)
                set_bit(0, &per_cpu(tick_cpu_sched, cpu).check_clocks);
}


/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
        struct clocksource *new, *old;

        new = (struct clocksource *) data;
	// new = $clocksource_tsc .

        timekeeping_forward_now();
	// 更新时间

        if (!new->enable || new->enable(new) == 0) {
		// new->enable == NULL

                old = timekeeper.clock;
                timekeeper_setup_internals(new);
		// 设置新的时钟源 tsc --> timekeeper.

                if (old->disable)
                        old->disable(old);
        }
        return 0;
}


/**             
 * timekeeping_forward_now - update clock to the current time
 *              
 * Forward the current clock to update its state since the last call to
 * update_wall_time(). This is useful before significant clock changes,
 * as it avoids having to deal with this time offset explicitly.
 */
static void timekeeping_forward_now(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;
        s64 nsec;

        clock = timekeeper.clock;
	// clock = &clocksource_jiffies 
	
        cycle_now = clock->read(clock);
	// 返回 jiffies
	
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
	// 经过的时间(jiffies 数)

        clock->cycle_last = cycle_now;
        
        nsec = clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
	// 转换成 纳秒
	//  timekeeper.mult =  
	//  timekeeper.shift = 8 
        
        /* If arch requires, add in gettimeoffset() */
        nsec += arch_gettimeoffset();
	// 空函数

        timespec_add_ns(&xtime, nsec);
	// 加到 xtime 中去 

        nsec = clocksource_cyc2ns(cycle_delta, clock->mult, clock->shift);
        timespec_add_ns(&raw_time, nsec);
}


fs_initcall(hpet_late_init);

/*
 * Needs to be late, as the reserve_timer code calls kalloc !
 *
 * Not a problem on i386 as hpet_enable is called from late_time_init,
 * but on x86_64 it is necessary !
 */
static __init int hpet_late_init(void)
{
        int cpu;

        if (boot_hpet_disable)
                return -ENODEV;

        if (!hpet_address) {
                if (!force_hpet_address)
                        return -ENODEV;

                hpet_address = force_hpet_address;
                hpet_enable();
        }

        if (!hpet_virt_address)
                return -ENODEV;

        hpet_reserve_platform_timers(hpet_readl(HPET_ID));
        hpet_print_config();

        if (hpet_msi_disable)
                return 0;

        for_each_online_cpu(cpu) {
                hpet_cpuhp_notify(NULL, CPU_ONLINE, (void *)(long)cpu);
        }

        /* This notifier should be called after workqueue is ready */
        hotcpu_notifier(hpet_cpuhp_notify, -20);

        return 0;
}


static void hpet_legacy_clockevent_register(void)
{
        /* Start HPET legacy interrupts */
        hpet_enable_legacy_int();

        /*
         * The mult factor is defined as (include/linux/clockchips.h)
         *  mult/2^shift = cyc/ns (in contrast to ns/cyc in clocksource.h)
         * hpet_period is in units of femtoseconds (per cycle), so
         *  mult/2^shift = cyc/ns = 10^6/hpet_period
         *  mult = (10^6 * 2^shift)/hpet_period
         *  mult = (FSEC_PER_NSEC << hpet_clockevent.shift)/hpet_period
         */
        hpet_clockevent.mult = div_sc((unsigned long) FSEC_PER_NSEC,
                                      hpet_period, hpet_clockevent.shift);
        /* Calculate the min / max delta */
        hpet_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
                                                           &hpet_clockevent);
        /* 5 usec minimum reprogramming delta. */
        hpet_clockevent.min_delta_ns = 5000;

        /*
         * Start hpet with the boot cpu mask and make it
         * global after the IO_APIC has been initialized.
         */
        hpet_clockevent.cpumask = cpumask_of(smp_processor_id());
        clockevents_register_device(&hpet_clockevent);
        global_clock_event = &hpet_clockevent;
	// Global clock event
        printk(KERN_DEBUG "hpet clockevent registered\n");
}

#define FSEC_PER_NSEC                   1000000L   // 飞秒   1 纳秒 = 1000000 飞秒


+------------------------------------------------------------------------------------+
|                                                                                    |
|       /*                                                                           |
|        * The hpet clock event device                                               |
|        */                                                                          |
|       static struct clock_event_device hpet_clockevent = {                         |
|       	.name           = "hpet",                                            |
|       	.features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,  |
|       	.set_mode       = hpet_legacy_set_mode,                              |
|       	.set_next_event = hpet_legacy_next_event,                            |
|       	.shift          = 32,                                                |
|       	.irq            = 0,                                                 |
|       	.rating         = 50,                                                |
|       };                                                                           |
+------------------------------------------------------------------------------------+

 +----------------------------------------------------------------------------------------------+
 |                                                                                              |
 |      /**                                                                                     |
 |       * struct clock_event_device - clock event device descriptor                            |
 |       * @name:               ptr to clock event name                                         |
 |       * @features:           features                                                        |
 |       * @max_delta_ns:       maximum delta value in ns                                       |
 |       * @min_delta_ns:       minimum delta value in ns                                       |
 |       * @mult:               nanosecond to cycles multiplier                                 |
 |       * @shift:              nanoseconds to cycles divisor (power of two)                    |
 |       * @rating:             variable to rate clock event devices                            |
 |       * @irq:                IRQ number (only for non CPU local devices)                     |
 |       * @cpumask:            cpumask to indicate for which CPUs this device works            |
 |       * @set_next_event:     set next event function                                         |
 |       * @set_mode:           set mode function                                               |
 |       * @event_handler:      Assigned by the framework to be called by the low               |
 |       *                      level handler of the event source                               |
 |       * @broadcast:          function to broadcast events                                    |
 |       * @list:               list head for the management code                               |
 |       * @mode:               operating mode assigned by the management code                  |
 |       * @next_event:         local storage for the next event in oneshot mode                |
 |       */                                                                                     |
 |      struct clock_event_device {                                                             |
 |      	const char              *name;                                                  |
 |      	unsigned int            features;                                               |
 |      	unsigned long           max_delta_ns;                                           |
 |      	unsigned long           min_delta_ns;                                           |
 |      	unsigned long           mult;                                                   |
 |      	int                     shift;                                                  |
 |      	int                     rating;                                                 |
 |      	int                     irq;                                                    |
 |      	const struct cpumask    *cpumask;                                               |
 |      	int                     (*set_next_event)(unsigned long evt,                    |
 |      						  struct clock_event_device *);         |
 |      	void                    (*set_mode)(enum clock_event_mode mode,                 |
 |      					    struct clock_event_device *);               |
 |      	void                    (*event_handler)(struct clock_event_device *);          |
 |      	void                    (*broadcast)(const struct cpumask *mask);               |
 |      	struct list_head        list;                                                   |
 |      	enum clock_event_mode   mode;                                                   |
 |      	ktime_t                 next_event;                                             |
 |      };                                                                                      |
 |                                                                                              |
 +----------------------------------------------------------------------------------------------+

typedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;


#define DECLARE_BITMAP(name,bits) \
	unsigned long name[BITS_TO_LONGS(bits)]

#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))

#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))

/*
 * Calculate a multiplication factor for scaled math, which is used to convert
 * nanoseconds based values to clock ticks:
 *
 * clock_ticks = (nanoseconds * factor) >> shift.
 *
 * div_sc is the rearranged equation to calculate a factor from a given clock
 * ticks / nanoseconds ratio:
 *
 * factor = (clock_ticks << shift) / nanoseconds
 */
static inline unsigned long div_sc(unsigned long ticks, unsigned long nsec,
                                   int shift)
{
        // ticks = 1000000L
        // nsec = hpet_period
        // shift = 32
        uint64_t tmp = ((uint64_t)ticks) << shift;

        do_div(tmp, nsec);
        return (unsigned long) tmp;
}


#define cpumask_of(cpu) (get_cpu_mask(cpu))

static inline const struct cpumask *get_cpu_mask(unsigned int cpu)
{
        const unsigned long *p = cpu_bit_bitmap[1 + cpu % BITS_PER_LONG];
        p -= cpu / BITS_PER_LONG;
        return to_cpumask(p);
}

const unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {

        MASK_DECLARE_8(0),      MASK_DECLARE_8(8),
        MASK_DECLARE_8(16),     MASK_DECLARE_8(24),
#if BITS_PER_LONG > 32
        MASK_DECLARE_8(32),     MASK_DECLARE_8(40),
        MASK_DECLARE_8(48),     MASK_DECLARE_8(56),
#endif
};

#define MASK_DECLARE_1(x)       [x+1][0] = 1UL << (x)
#define MASK_DECLARE_2(x)       MASK_DECLARE_1(x), MASK_DECLARE_1(x+1)
#define MASK_DECLARE_4(x)       MASK_DECLARE_2(x), MASK_DECLARE_2(x+2)
#define MASK_DECLARE_8(x)       MASK_DECLARE_4(x), MASK_DECLARE_4(x+4)


/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */
#define to_cpumask(bitmap)                                              \
        ((struct cpumask *)(1 ? (bitmap)                                \
                            : (void *)sizeof(__check_is_bitmap(bitmap))))



/*              
 * Initialize the conversion factor and the min/max deltas of the clock event
 * structure and register the clock event source with the framework.
 */             
void __init setup_pit_timer(void)
{               
        /*
         * Start pit with the boot cpu mask and make it global after the
         * IO_APIC has been initialized.
         */
        pit_ce.cpumask = cpumask_of(smp_processor_id());
        pit_ce.mult = div_sc(CLOCK_TICK_RATE, NSEC_PER_SEC, pit_ce.shift);
        pit_ce.max_delta_ns = clockevent_delta2ns(0x7FFF, &pit_ce);
        pit_ce.min_delta_ns = clockevent_delta2ns(0xF, &pit_ce);
                
        clockevents_register_device(&pit_ce);
        global_clock_event = &pit_ce;
}       

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       /*                                                                                |
|        * On UP the PIT can serve all of the possible timer functions. On SMP systems    |
|        * it can be solely used for the global tick.                                     |
|        *                                                                                |
|        * The profiling and update capabilities are switched off once the local apic is  |
|        * registered. This mechanism replaces the previous #ifdef LOCAL_APIC -           |
|        * !using_apic_timer decisions in do_timer_interrupt_hook()                       |
|        */                                                                               |
|       static struct clock_event_device pit_ce = {                                       |
|               .name           = "pit",                                                  |
|               .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,       |
|               .set_mode       = init_pit_timer,                                         |
|               .set_next_event = pit_next_event,                                         |
|               .shift          = 32,                                                     |
|               .irq            = 0,                                                      |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+

void __init setup_default_timer_irq(void)
{       
        setup_irq(0, &irq0);
}

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       static struct irqaction irq0  = {                                                 |
|               .handler = timer_interrupt,                                               |
|               .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,    |
|               .name = "timer"                                                           |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+



//经测试，第一次时钟中断发生在 native_calibrate_tsc() 中 在 global clock event device 已经注册之后(hpet) ,global_clock_event->event_handler(global_clock_event) 为 tick_handle_periodic() 负责更新系统时间。 


void __init tsc_init(void)
{
        u64 lpj;
        int cpu;
                
        x86_init.timers.tsc_pre_init();
                
        if (!cpu_has_tsc)
                return;
        
        tsc_khz = x86_platform.calibrate_tsc();
        // invoke native_calibrate_tsc(); calibrate the tsc on boot


        cpu_khz = tsc_khz;
        
        if (!tsc_khz) {
                mark_tsc_unstable("could not calculate TSC khz");
                return;
        }

        printk("Detected %lu.%03lu MHz processor.\n",
                        (unsigned long)cpu_khz / 1000,
                        (unsigned long)cpu_khz % 1000);
        
        /*
         * Secondary CPUs do not run through tsc_init(), so set up
         * all the scale factors for all CPUs, assuming the same
         * speed as the bootup CPU. (cpufreq notifiers will fix this
         * up if their speed diverges)
         */
        for_each_possible_cpu(cpu)
                set_cyc2ns_scale(cpu_khz, cpu);

        if (tsc_disabled > 0)
                return;
        
        /* now allow native_sched_clock() to use rdtsc */
        tsc_disabled = 0;

        lpj = ((u64)tsc_khz * 1000);
        do_div(lpj, HZ);
        lpj_fine = lpj;
        
        use_tsc_delay();
        /* Check and install the TSC clocksource */
        dmi_check_system(bad_tsc_dmi_table);

        if (unsynchronized_tsc())
                mark_tsc_unstable("TSCs unsynchronized");

        check_system_tsc_reliable();
        init_tsc_clocksource();
}       

struct x86_platform_ops x86_platform = {
        .calibrate_tsc                  = native_calibrate_tsc,
        .get_wallclock                  = mach_get_cmos_time,
        .set_wallclock                  = mach_set_rtc_mmss,
        .is_untracked_pat_range         = is_ISA_range,
        .nmi_init                       = default_nmi_init
};


static void __init init_tsc_clocksource(void)
{
        clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
                        clocksource_tsc.shift);
        if (tsc_clocksource_reliable)
                clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
        /* lower the rating if we already know its unstable: */
        if (check_tsc_unstable()) {
                clocksource_tsc.rating = 0;
                clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
        }
        clocksource_register(&clocksource_tsc);
}

+-------------------------------------------------------------------------+
|                                                                         |
|                                                                         |
|      static struct clocksource clocksource_tsc = {                      |
|               .name                   = "tsc",                          |
|               .rating                 = 300,                            |
|               .read                   = read_tsc,                       |
|               .resume                 = resume_tsc,                     |
|               .mask                   = CLOCKSOURCE_MASK(64),           |
|               .shift                  = 22,                             |
|               .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |    |
|                                         CLOCK_SOURCE_MUST_VERIFY,       |
|       #ifdef CONFIG_X86_64                                              |
|               .vread                  = vread_tsc,                      |
|       #endif                                                            |
|       };                                                                |
|                                                                         |
+-------------------------------------------------------------------------+







   +-------------------------------------------------------------------------+
   |                                                                         |
   |    # cat /proc/timer_list | more                                        |
   |                                                                         |
   |    Tick Device: mode:     1                      --> one-shot 单次      |
   |    Broadcast device                                                     |
   |    Clock Event Device: hpet                                             |
   |     max_delta_ns:   149983013276                                        |
   |     min_delta_ns:   13409                                               |
   |     mult:           61496111                                            |
   |     shift:          32                                                  |
   |     mode:           3                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: hpet_legacy_next_event                              |
   |     set_mode:       hpet_legacy_set_mode                                |
   |     event_handler:  tick_handle_oneshot_broadcast                       |
   |     retries:        1459862                                             |
   |    tick_broadcast_mask: 00000001                                        |
   |    tick_broadcast_oneshot_mask: 00000002                                |
   |                                                                         |
   |                                                                         |
   |    Tick Device: mode:     1                                             |
   |    Per CPU device: 0                                                    |
   |    Clock Event Device: lapic                                            |
   |     max_delta_ns:   128824925406                                        |
   |     min_delta_ns:   1000                                                |
   |     mult:           71596176                                            |
   |     shift:          32                                                  |
   |     mode:           3                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: lapic_next_event                                    |
   |     set_mode:       lapic_timer_setup                                   |
   |     event_handler:  hrtimer_interrupt                                   |
   |     retries:        891537                                              |
   |                                                                         |
   |    Tick Device: mode:     1                                             |
   |    Per CPU device: 1                                                    |
   |    Clock Event Device: lapic                                            |
   |     max_delta_ns:   128824925406                                        |
   |     min_delta_ns:   1000                                                |
   |     mult:           71596176                                            |
   |     shift:          32                                                  |
   |     mode:           1                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: lapic_next_event                                    |
   |     set_mode:       lapic_timer_setup                                   |
   |     event_handler:  hrtimer_interrupt                                   |
   |     retries:        539190                                              |
   +-------------------------------------------------------------------------+

clock event device 有三种 PIT HPET APIC
                          PIT/HPET Global clock event device
			  APIC     Local clock event device
 
setup_APIC_timer()
        |
        +----> clockevents_register_device(levt);


 +----------------------------------------------------------------------------------+
 |      /*                                                                          |
 |       * The local apic timer can be used for any function which is CPU local.    |
 |       */                                                                         |
 |      static struct clock_event_device lapic_clockevent = {                       |
 |              .name           = "lapic",                                          |
 |              .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT  |
 |                              | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_DUMMY,     |
 |              .shift          = 32,                                               |
 |              .set_mode       = lapic_timer_setup,                                |
 |              .set_next_event = lapic_next_event,                                 |
 |              .broadcast      = lapic_timer_broadcast,                            |
 |              .rating         = 100,                                              |
 |              .irq            = -1,                                               |
 |      };                                                                          |
 |                                                                                  |
 +----------------------------------------------------------------------------------+
 

/**
 * clockevents_register_device - register a clock event device
 * @dev:        device to register
 */
void clockevents_register_device(struct clock_event_device *dev)
{
        // dev=&hpet_clockevent

        unsigned long flags;

        BUG_ON(dev->mode != CLOCK_EVT_MODE_UNUSED);
        BUG_ON(!dev->cpumask);

        spin_lock_irqsave(&clockevents_lock, flags);

        list_add(&dev->list, &clockevent_devices);
	// 加入到 clockevent_devices 链表中
        clockevents_do_notify(CLOCK_EVT_NOTIFY_ADD, dev);

        clockevents_notify_released();
	// 当 Global Clock Event Device 注册完 hpet 和 cpu0 注册 lapic 之后会被调用.

        spin_unlock_irqrestore(&clockevents_lock, flags);
}

static LIST_HEAD(clockevent_devices);


/* Clock event notification values */
enum clock_event_nofitiers {
        CLOCK_EVT_NOTIFY_ADD,
        CLOCK_EVT_NOTIFY_BROADCAST_ON,
        CLOCK_EVT_NOTIFY_BROADCAST_OFF,
        CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
        CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
        CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
        CLOCK_EVT_NOTIFY_SUSPEND,
        CLOCK_EVT_NOTIFY_RESUME,
        CLOCK_EVT_NOTIFY_CPU_DYING,
        CLOCK_EVT_NOTIFY_CPU_DEAD,
};

/*
 * Notify about a clock event change. Called with clockevents_lock
 * held.
 */
static void clockevents_do_notify(unsigned long reason, void *dev)
{
	// dev 为时钟事件设备 (hpet) 
        raw_notifier_call_chain(&clockevents_chain, reason, dev);
}

// clockevents_chain 已经在 tick_init 函数中初始化
// clockevents_chain->head = &tick_notifier

struct raw_notifier_head {
	        struct notifier_block *head;
};

int raw_notifier_call_chain(struct raw_notifier_head *nh,
		                unsigned long val, void *v)
{
	        return __raw_notifier_call_chain(nh, val, v, -1, NULL);
}



/**     
 *      __raw_notifier_call_chain - Call functions in a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @val: Value passed unmodified to notifier function
 *      @v: Pointer passed unmodified to notifier function
 *      @nr_to_call: See comment for notifier_call_chain.
 *      @nr_calls: See comment for notifier_call_chain
 *
 *      Calls each function in a notifier chain in turn.  The functions
 *      run in an undefined context.
 *      All locking must be provided by the caller.
 *
 *      If the return value of the notifier can be and'ed
 *      with %NOTIFY_STOP_MASK then raw_notifier_call_chain()
 *      will return immediately, with the return value of
 *      the notifier function which halted execution.
 *      Otherwise the return value is the return value
 *      of the last notifier function called.
 */     
int __raw_notifier_call_chain(struct raw_notifier_head *nh,
                              unsigned long val, void *v,
                              int nr_to_call, int *nr_calls)
{
        return notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
}


/**
 * notifier_call_chain - Informs the registered notifiers about an event.
 *      @nl:            Pointer to head of the blocking notifier chain
 *      @val:           Value passed unmodified to notifier function
 *      @v:             Pointer passed unmodified to notifier function
 *      @nr_to_call:    Number of notifier functions to be called. Don't care
 *                      value of this parameter is -1.
 *      @nr_calls:      Records the number of notifications sent. Don't care
 *                      value of this field is NULL.
 *      @returns:       notifier_call_chain returns the value returned by the
 *                      last notifier function called.
 */
static int __kprobes notifier_call_chain(struct notifier_block **nl,
                                        unsigned long val, void *v,
                                        int nr_to_call, int *nr_calls)
{
        int ret = NOTIFY_DONE;
        struct notifier_block *nb, *next_nb;

        nb = rcu_dereference(*nl);
        // nb 指向 notifier_block

        while (nb && nr_to_call) {
                // nr_to_call = -1

                next_nb = rcu_dereference(nb->next);

#ifdef CONFIG_DEBUG_NOTIFIERS
                if (unlikely(!func_ptr_is_kernel_text(nb->notifier_call))) {
                        WARN(1, "Invalid notifier called!");
                        nb = next_nb;
                        continue;
                }
#endif
                ret = nb->notifier_call(nb, val, v);
                // 调用 tick_notify 函数 val = CLOCK_EVT_NOTIFY_ADD 
                // 返回 ret = NOTIFY_STOP

                if (nr_calls)
                        (*nr_calls)++;
                // nr_calls = NULL;

                if ((ret & NOTIFY_STOP_MASK) == NOTIFY_STOP_MASK)
                        break;
                nb = next_nb;
                nr_to_call--;
        }
        return ret;
}

#define NOTIFY_STOP_MASK        0x8000          /* Don't call further */
#define NOTIFY_DONE             0x0000          /* Don't care */
#define NOTIFY_OK               0x0001          /* Suits me */
#define NOTIFY_STOP             (NOTIFY_OK|NOTIFY_STOP_MASK) 

/*
 * Notification about clock event devices
 */
static int tick_notify(struct notifier_block *nb, unsigned long reason,
                               void *dev)
{
        switch (reason) {

        case CLOCK_EVT_NOTIFY_ADD:
                return tick_check_new_device(dev);
		// 这里

        case CLOCK_EVT_NOTIFY_BROADCAST_ON:
        case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
        case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
                tick_broadcast_on_off(reason, dev);
                break;

        case CLOCK_EVT_NOTIFY_BROADCAST_ENTER:
        case CLOCK_EVT_NOTIFY_BROADCAST_EXIT:
                tick_broadcast_oneshot_control(reason);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DYING:
                tick_handover_do_timer(dev);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DEAD:
                tick_shutdown_broadcast_oneshot(dev);
                tick_shutdown_broadcast(dev);
                tick_shutdown(dev);
                break;

        case CLOCK_EVT_NOTIFY_SUSPEND:
                tick_suspend();
                tick_suspend_broadcast();
                break;

        case CLOCK_EVT_NOTIFY_RESUME:
                tick_resume();
                break;

        default:
                break;
        }

        return NOTIFY_OK;
}

/*      
 * Check, if the new registered device should be used.
 */             
static int tick_check_new_device(struct clock_event_device *newdev)
{       
        // newdev = &hpet_clockevent;

        struct clock_event_device *curdev;
        struct tick_device *td;
        int cpu, ret = NOTIFY_OK;
        unsigned long flags;
        
        spin_lock_irqsave(&tick_device_lock, flags);
                
        cpu = smp_processor_id();
        if (!cpumask_test_cpu(cpu, newdev->cpumask))
                goto out_bc;
	// cpumask:            cpumask to indicate for which CPUs this device works
                
        td = &per_cpu(tick_cpu_device, cpu);
        // tick_cpu_device 是一个各CPU链表，包含了系统中每个CPU对应的struct tick_device 实例.

        curdev = td->evtdev;

        /* cpu local device ? */
        if (!cpumask_equal(newdev->cpumask, cpumask_of(cpu))) {

                /*
                 * If the cpu affinity of the device interrupt can not
                 * be set, ignore it.
                 */
                if (!irq_can_set_affinity(newdev->irq))
                        goto out_bc;

                /*
                 * If we have a cpu local device already, do not replace it
                 * by a non cpu local device
                 */
                if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
                        goto out_bc;
        }

        /*
         * If we have an active device, then check the rating and the oneshot
         * feature.
         */
        if (curdev) {
		// 2.curdev = hpet
		// 2.newdev = lapic

		// 3.curdev = lapic
		// 3.newdev = hpet

                /*
                 * Prefer one shot capable devices !
                 */
                if ((curdev->features & CLOCK_EVT_FEAT_ONESHOT) &&
                    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
                        goto out_bc;
                /*
                 * Check the rating
                 */
                if (curdev->rating >= newdev->rating)
                        goto out_bc;

		// lapic rating = 100,hpet rating = 50 
        }

        /*
         * Replace the eventually existing device by the new
         * device. If the current device is the broadcast device, do
         * not give it back to the clockevents layer !
         */
        if (tick_is_broadcast_device(curdev)) {
                clockevents_shutdown(curdev);
                curdev = NULL;
        }

        clockevents_exchange_device(curdev, newdev);

        tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
        if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
                tick_oneshot_notify();
	        // here

        spin_unlock_irqrestore(&tick_device_lock, flags);
        return NOTIFY_STOP;

out_bc:
        /*
         * Can the new device be used as a broadcast device ?
         */
        if (tick_check_broadcast_device(newdev))
                ret = NOTIFY_STOP;

        spin_unlock_irqrestore(&tick_device_lock, flags);

        return ret;
}

/**
 *  * cpumask_test_cpu - test for a cpu in a cpumask
 *   * @cpu: cpu number (< nr_cpu_ids)
 *    * @cpumask: the cpumask pointer
 *     *
 *      * No static inline type checking - see Subtlety (1) above.
 *       */
#define cpumask_test_cpu(cpu, cpumask) \
	        test_bit(cpumask_check(cpu), cpumask_bits((cpumask)))


/**
 * clockevents_exchange_device - release and request clock devices
 * @old:        device to release (can be NULL)
 * @new:        device to request (can be NULL)
 *
 * Called from the notifier chain. clockevents_lock is held already
 */
void clockevents_exchange_device(struct clock_event_device *old,
                                 struct clock_event_device *new)
{
        //old = NULL
        //new = &hpet_clockevent 
	
	// old = hpet 
	// new = lapic

        unsigned long flags;

        local_irq_save(flags);
        /*
         * Caller releases a clock event device. We queue it into the
         * released list and do a notify add later.
         */
        if (old) {
                clockevents_set_mode(old, CLOCK_EVT_MODE_UNUSED);
                list_del(&old->list);
                list_add(&old->list, &clockevents_released);
		// 加到 clockevents_released 链表中
        }

        if (new) {
                BUG_ON(new->mode != CLOCK_EVT_MODE_UNUSED);
                clockevents_shutdown(new);
        }
        local_irq_restore(flags);
}

/**
 * clockevents_shutdown - shutdown the device and clear next_event
 * @dev:        device to shutdown
 */
void clockevents_shutdown(struct clock_event_device *dev)
{
        clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
        dev->next_event.tv64 = KTIME_MAX;
}

/*
 * Check, if the device can be utilized as broadcast device:
 */
int tick_check_broadcast_device(struct clock_event_device *dev)
{
        if ((tick_broadcast_device.evtdev &&
             tick_broadcast_device.evtdev->rating >= dev->rating) ||
             (dev->features & CLOCK_EVT_FEAT_C3STOP))
                return 0;

        clockevents_exchange_device(NULL, dev);

        tick_broadcast_device.evtdev = dev;
        // 设置为 hpet
	
        if (!cpumask_empty(tick_get_broadcast_mask()))
                tick_broadcast_start_periodic(dev);
        // 设置周期模式
        return 1;
}

static struct tick_device tick_broadcast_device;

/*      
 * Start the device in periodic mode 
 */     
static void tick_broadcast_start_periodic(struct clock_event_device *bc)
{       
        if (bc)
                tick_setup_periodic(bc, 1);
}


/*
 * Setup the tick device
 */
static void tick_setup_device(struct tick_device *td,
                              struct clock_event_device *newdev, int cpu,
                              const struct cpumask *cpumask)
{
        ktime_t next_event;
        void (*handler)(struct clock_vent_device *) = NULL;

        /*
         * First device setup ?
         */
        if (!td->evtdev) {
        // 此时钟事件设备没有相关的时钟设备
	
                /*
                 * If no cpu took the do_timer update, assign it to
                 * this cpu:
                 */
                if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
                        // 如果没有选定时钟设备来承担全局时钟设备的角色，那么将选择当前设备来承担此职责

                        tick_do_timer_cpu = cpu;
                        // 设置为当前设备所属处理器编号
                        tick_next_period = ktime_get();

                        tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
                        // 时钟周期，纳秒
                        HZ = 1000
                }

                /*
                 * Startup in periodic mode first.
                 */
                td->mode = TICKDEV_MODE_PERIODIC;
                // 设备运行模式 --> 周期模式

        } else {
                handler = td->evtdev->event_handler;
                next_event = td->evtdev->next_event;
                td->evtdev->event_handler = clockevents_handle_noop;
        }

        td->evtdev = newdev;
        //为时钟设备指定事件设备

        /*
         * When the device is not per cpu, pin the interrupt to the
         * current cpu:
         */
        if (!cpumask_equal(newdev->cpumask, cpumask))
                irq_set_affinity(newdev->irq, cpumask);

        /*
         * When global broadcasting is active, check if the current
         * device is registered as a placeholder for broadcast mode.
         * This allows us to handle this x86 misfeature in a generic
         * way.
         */
	// check whether enable the broadcast mode,如果系统处于省电模式，而局部时钟停止工作，则会使用广播机制
        if (tick_device_uses_broadcast(newdev, cpu))
                return;

        if (td->mode == TICKDEV_MODE_PERIODIC)
                tick_setup_periodic(newdev, 0);
	        // 周期模式 invoke this ......
        else
                tick_setup_oneshot(newdev, handler, next_event);
	        // 单触发模式
}

#define NSEC_PER_SEC    1000000000L

/*
 * Check, if the device is disfunctional and a place holder, which
 * needs to be handled by the broadcast device.
 */
int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
{               
        unsigned long flags;
        int ret = 0; 
                
        spin_lock_irqsave(&tick_broadcast_lock, flags);
                
        /*
         * Devices might be registered with both periodic and oneshot
         * mode disabled. This signals, that the device needs to be
         * operated from the broadcast device and is a placeholder for
         * the cpu local device.
         */
        if (!tick_device_is_functional(dev)) {
                dev->event_handler = tick_handle_periodic;
                cpumask_set_cpu(cpu, tick_get_broadcast_mask());
                tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
                ret = 1;
        } else {
		// 正常情况下来到这里

                /*
                 * When the new device is not affected by the stop
                 * feature and the cpu is marked in the broadcast mask
                 * then clear the broadcast bit.
                 */
                if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
			// Clockevent source stops in C3 State and needs broadcast support
                        int cpu = smp_processor_id();
        
                        cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
                        tick_broadcast_clear_oneshot(cpu);
                }
        }
        spin_unlock_irqrestore(&tick_broadcast_lock, flags);
        return ret;
}

/*
 * Check, if the device is functional or a dummy for broadcast
 */
static inline int tick_device_is_functional(struct clock_event_device *dev)
{
        return !(dev->features & CLOCK_EVT_FEAT_DUMMY);
}

/*
 * Setup the device for a periodic tick
 */
void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
        tick_set_periodic_handler(dev, broadcast);
	// broadcast = 0

        /* Broadcast setup ? */
        if (!tick_device_is_functional(dev))
                return;

        if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
            !tick_broadcast_oneshot_active()) {
                clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
		// here
		// 设置成周期模式
        } else {
                unsigned long seq;
                ktime_t next;

                do {
                        seq = read_seqbegin(&xtime_lock);
                        next = tick_next_period;
                } while (read_seqretry(&xtime_lock, seq));

                clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);

                for (;;) {
                        if (!clockevents_program_event(dev, next, ktime_get()))
                                return;
                        next = ktime_add(next, tick_period);
                }
        }
}

/*
 * Set the periodic handler depending on broadcast on/off
 */
void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
{
        if (!broadcast)
                dev->event_handler = tick_handle_periodic;
	        // here
        else
                dev->event_handler = tick_handle_periodic_broadcast;
}               

/**
 * clockevents_set_mode - set the operating mode of a clock event device
 * @dev:        device to modify
 * @mode:       new mode
 *      
 * Must be called with interrupts disabled !
 */             
void clockevents_set_mode(struct clock_event_device *dev,
                                 enum clock_event_mode mode)
{               
        if (dev->mode != mode) {
                dev->set_mode(mode, dev);
		// invoke the function hpet_legacy_semode = t_mode()

                dev->mode = mode;
                        
                /*
                 * A nsec2cyc multiplicator of 0 is invalid and we'd crash
                 * on it, so fix it up and emit a warning:
                 */
                if (mode == CLOCK_EVT_MODE_ONESHOT) {
                        if (unlikely(!dev->mult)) {
                                dev->mult = 1;
                                WARN_ON(1);
                        }
                }
        }
}

static void hpet_legacy_set_mode(enum clock_event_mode mode,
                        struct clock_event_device *evt)
{
        hpet_set_mode(mode, evt, 0);
}

static void hpet_set_mode(enum clock_event_mode mode,
                          struct clock_event_device *evt, int timer)
{
        unsigned long cfg, cmp, now;
        uint64_t delta;
        
	// mode = CLOCK_EVT_MODE_PERIODIC
        switch (mode) {
        case CLOCK_EVT_MODE_PERIODIC:
                hpet_stop_counter();
                delta = ((uint64_t)(NSEC_PER_SEC/HZ)) * evt->mult;
                delta >>= evt->shift;
                now = hpet_readl(HPET_COUNTER);
                cmp = now + (unsigned long) delta;
                cfg = hpet_readl(HPET_Tn_CFG(timer));
                /* Make sure we use edge triggered interrupts */
                cfg &= ~HPET_TN_LEVEL;
                cfg |= HPET_TN_ENABLE | HPET_TN_PERIODIC |
                       HPET_TN_SETVAL | HPET_TN_32BIT;
                hpet_writel(cfg, HPET_Tn_CFG(timer));
                hpet_writel(cmp, HPET_Tn_CMP(timer));
                udelay(1);
                /*
                 * HPET on AMD 81xx needs a second write (with HPET_TN_SETVAL
                 * cleared) to T0_CMP to set the period. The HPET_TN_SETVAL
                 * bit is automatically cleared after the first write.
                 * (See AMD-8111 HyperTransport I/O Hub Data Sheet,
                 * Publication # 24674)
                 */
                hpet_writel((unsigned long) delta, HPET_Tn_CMP(timer));
                hpet_start_counter();
                hpet_print_config();
                break;

        case CLOCK_EVT_MODE_ONESHOT:
                cfg = hpet_readl(HPET_Tn_CFG(timer));
                cfg &= ~HPET_TN_PERIODIC;
                cfg |= HPET_TN_ENABLE | HPET_TN_32BIT;
                hpet_writel(cfg, HPET_Tn_CFG(timer));
                break;

        case CLOCK_EVT_MODE_UNUSED:
        case CLOCK_EVT_MODE_SHUTDOWN:
                cfg = hpet_readl(HPET_Tn_CFG(timer));
                cfg &= ~HPET_TN_ENABLE;
                hpet_writel(cfg, HPET_Tn_CFG(timer));
                break;

        case CLOCK_EVT_MODE_RESUME:
                if (timer == 0) {
                        hpet_enable_legacy_int();
                } else {
                        struct hpet_dev *hdev = EVT_TO_HPET_DEV(evt);
                        hpet_setup_msi_irq(hdev->irq);
                        disable_irq(hdev->irq);
                        irq_set_affinity(hdev->irq, cpumask_of(hdev->cpu));
                        enable_irq(hdev->irq);
                }
                hpet_print_config();
                break;
        }
}



/*
 * Called after a notify add to make devices available which were
 * released from the notifier call.
 */
static void clockevents_notify_released(void)
{
        struct clock_event_device *dev;

        while (!list_empty(&clockevents_released)) {
		// 为空 list_empty 返回 1,如果 clockevents_released 链表不为空则进入循环
		// 现在链表中的设备为 hpet

                dev = list_entry(clockevents_released.next,
                                 struct clock_event_device, list);

		// dev 指向 hpet_clockevent  
                list_del(&dev->list);
                list_add(&dev->list, &clockevent_devices);
		// 再次加入到 clockevent_devices 链表
                clockevents_do_notify(CLOCK_EVT_NOTIFY_ADD, dev);
		// 再次设置 event_handler
        }
}
  
static LIST_HEAD(clockevents_released);


 +-------------------------------------------------------------------------------------------------------+
 |      run_local_timers()                                                                               |
 |      trigger->                                                                                        |
 |      run_timer_softirq()                                                                              |
 |         --> hrtimer_run_pending()                                                                     |
 |      	  --> hrtimer_switch_to_hres()                                                           |
 |      		--> tick_init_highres()                                                          |
 |      		      --> tick_switch_to_oneshot()                                               |
 |      			    --> tick_broadcast_switch_to_oneshot()                               |
 |      				  --> tick_broadcast_setup_oneshot()                             |
 |                                                                                                       |
 |      /*                                                                                               |
 |       * Called by the local, per-CPU timer interrupt on SMP.                                          |
 |       */                                                                                              |
 |      void run_local_timers(void)                                                                      |
 |      {                                                                                                |
 |      	hrtimer_run_queues();                                                                    |
 |      	raise_softirq(TIMER_SOFTIRQ);                                                            |
 |      }                                                                                                |
 |                                                                                                       |
 |      /*                                                                                               |
 |       * This function runs timers and the timer-tq in bottom half context.                            |
 |       */                                                                                              |
 |      static void run_timer_softirq(struct softirq_action *h)                                          |
 |      {                                                                                                |
 |      	struct tvec_base *base = __get_cpu_var(tvec_bases);                                      |
 |                                                                                                       |
 |      	perf_event_do_pending();                                                                 |
 |                                                                                                       |
 |      	hrtimer_run_pending();                                                                   |
 |                                                                                                       |
 |      	if (time_after_eq(jiffies, base->timer_jiffies))                                         |
 |      		__run_timers(base);                                                              |
 |      }                                                                                                |
 |                                                                                                       |
 |      /*                                                                                               |
 |       * Called from timer softirq every jiffy, expire hrtimers:                                       |
 |       *                                                                                               |
 |       * For HRT its the fall back code to run the softirq in the timer                                |
 |       * softirq context in case the hrtimer initialization failed or has                              |
 |       * not been done yet.                                                                            |
 |       */                                                                                              |
 |      void hrtimer_run_pending(void)                                                                   |
 |      {                                                                                                |
 |               // 如果此时已经是高精度模式则无需转换                                                   |
 |      	if (hrtimer_hres_active())                                                               |
 |      		return;                                                                          |
 |      		                                                                                 |
 |      	/*                                                                                       |
 |      	 * This _is_ ugly: We have to check in the softirq context,                              |
 |      	 * whether we can switch to highres and / or nohz mode. The                              |
 |      	 * clocksource switch happens in the timer interrupt with                                |
 |      	 * xtime_lock held. Notification from there only sets the                                |
 |      	 * check bit in the tick_oneshot code, otherwise we might                                |
 |      	 * deadlock vs. xtime_lock.                                                              |
 |      	 */                                                                                      |
 |      	if (tick_check_oneshot_change(!hrtimer_is_hres_enabled()))                               |
 |      		hrtimer_switch_to_hres();                                                        |
 |              // 检查系统中是否存在适用于高分辨率定时器的时钟事件设备                                  |
 |      	// whether we can switch to highres and / or nohz mode                                   |
 |      }                                                                                                |
 |                                                                                                       |
 |      /*                                                                                               |
 |       *  * hrtimer_high_res_enabled - query, if the highres mode is enabled                           |
 |       *   */                                                                                          |
 |      static inline int hrtimer_is_hres_enabled(void)                                                  |
 |      {                                                                                                |
 |      		return hrtimer_hres_enabled;                                                     |
 |                      // 正常情况下 为 1                                                               |
 |      }                                                                                                |
 |                                                                                                       |
 |      /**                                                                                              |
 |       * Check, if a change happened, which makes oneshot possible.                                    |
 |       *                                                                                               |
 |       * Called cyclic from the hrtimer softirq (driven by the timer                                   |
 |       * softirq) allow_nohz signals, that we can switch into low-res nohz                             |
 |       * mode, because high resolution timers are disabled (either compile                             |
 |       * or runtime).                                                                                  |
 |       */                                                                                              |
 |      int tick_check_oneshot_change(int allow_nohz)                                                    |
 |      {                                                                                                |
 |      	struct tick_sched *ts = &__get_cpu_var(tick_cpu_sched);                                  |
 |                                                                                                       |
 |      	if (!test_and_clear_bit(0, &ts->check_clocks))                                           |
 |      		return 0;                                                                        |
 |                                                                                                       |
 |      	if (ts->nohz_mode != NOHZ_MODE_INACTIVE)                                                 |
 |      		return 0;                                                                        |
 |                                                                                                       |
 |      	if (!timekeeping_valid_for_hres() || !tick_is_oneshot_available())                       |
 |      		return 0;                                                                        |
 |                                                                                                       |
 |      	if (!allow_nohz)                                                                         |
 |      		return 1;                                                                        |
 |                                                                                                       |
 |              // 如果没有启用 hrtimer 则 switch to nohz mode > all cpu                                 |
 |      	tick_nohz_switch_to_nohz();                                                              |
 |      	return 0;                                                                                |
 |      }                                                                                                |
 |                                                                                                       |
 |      /*                                                                                               |
 |       * Switch to high resolution mode                                                                |
 |       */                                                                                              |
 |      static int hrtimer_switch_to_hres(void)                                                          |
 |      {                                                                                                |
 |      	int cpu = smp_processor_id();                                                            |
 |      	struct hrtimer_cpu_base *base = &per_cpu(hrtimer_bases, cpu);                            |
 |      	unsigned long flags;                                                                     |
 |                                                                                                       |
 |      	if (base->hres_active)                                                                   |
 |      		return 1;                                                                        |
 |                                                                                                       |
 |      	local_irq_save(flags);                                                                   |
 |                                                                                                       |
 |      	if (tick_init_highres()) {                                                               |
 |      		local_irq_restore(flags);                                                        |
 |      		printk(KERN_WARNING "Could not switch to high resolution "                       |
 |      				    "mode on CPU %d\n", cpu);                                    |
 |      		return 0;                                                                        |
 |      	}                                                                                        |
 |      	base->hres_active = 1;                                                                   |
 |      	base->clock_base[CLOCK_REALTIME].resolution = KTIME_HIGH_RES;                            |
 |      	base->clock_base[CLOCK_MONOTONIC].resolution = KTIME_HIGH_RES;                           |
 |                                                                                                       |
 |      	tick_setup_sched_timer();                                                                |
 |                                                                                                       |
 |      	/* "Retrigger" the interrupt to get things going */                                      |
 |      	retrigger_next_event(NULL);                                                              |
 |      	local_irq_restore(flags);                                                                |
 |      	return 1;                                                                                |
 |      }                                                                                                |
 |                                                                                                       |
 |                                                                                                       |
 |      /**                                                                                              |
 |       * tick_init_highres - switch to high resolution mode                                            |
 |       *                                                                                               |
 |       * Called with interrupts disabled.                                                              |
 |       */                                                                                              |
 |      int tick_init_highres(void)                                                                      |
 |      {                                                                                                |
 |      	return tick_switch_to_oneshot(hrtimer_interrupt);                                        |
 |      }                                                                                                |
 |                                                                                                       |
 |      /**                                                                                              |
 |       * tick_switch_to_oneshot - switch to oneshot mode                                               |
 |       */                                                                                              |
 |      int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))                         |
 |      {                                                                                                |
 |      	struct tick_device *td = &__get_cpu_var(tick_cpu_device);                                |
 |      	struct clock_event_device *dev = td->evtdev;                                             |
 |                                                                                                       |
 |      	if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT) ||                                 |
 |      		    !tick_device_is_functional(dev)) {                                           |
 |                                                                                                       |
 |      		printk(KERN_INFO "Clockevents: "                                                 |
 |      		       "could not switch to one-shot mode:");                                    |
 |      		if (!dev) {                                                                      |
 |      			printk(" no tick device\n");                                             |
 |      		} else {                                                                         |
 |      			if (!tick_device_is_functional(dev))                                     |
 |      				printk(" %s is not functional.\n", dev->name);                   |
 |      			else                                                                     |
 |      				printk(" %s does not support one-shot mode.\n",                  |
 |      				       dev->name);                                               |
 |      		}                                                                                |
 |      		return -EINVAL;                                                                  |
 |      	}                                                                                        |
 |                                                                                                       |
 |      	td->mode = TICKDEV_MODE_ONESHOT;                                                         |
 |      	dev->event_handler = handler;                                                            |
 |                                                                                                       |
 |      	clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);                                       |
 |      	tick_broadcast_switch_to_oneshot();                                                      |
 |      	return 0;                                                                                |
 |      }                                                                                                |
 |                                                                                                       |
 |                                                                                                       |
 |      /*                                                                                               |
 |       * Select oneshot operating mode for the broadcast device                                        |
 |       */                                                                                              |
 |      void tick_broadcast_switch_to_oneshot(void)                                                      |
 |      {                                                                                                |
 |      	struct clock_event_device *bc;                                                           |
 |      	unsigned long flags;                                                                     |
 |      				                                                                 |
 |      	spin_lock_irqsave(&tick_broadcast_lock, flags);                                          |
 |      		                                                                                 |
 |      	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;                                       |
 |      	bc = tick_broadcast_device.evtdev;                                                       |
 |              // bc 为 Golbal Event Device.为 hpet                                                     |
 |                                                                                                       |
 |      	if (bc)                                                                                  |
 |      		tick_broadcast_setup_oneshot(bc);                                                |
 |      	spin_unlock_irqrestore(&tick_broadcast_lock, flags);                                     |
 |      }                                                                                                |
 |                                                                                                       |
 |                                                                                                       |
 |      /**                                                                                              |
 |       * tick_broadcast_setup_oneshot - setup the broadcast device                                     |
 |       */                                                                                              |
 |      void tick_broadcast_setup_oneshot(struct clock_event_device *bc)                                 |
 |      {                                                                                                |
 |      	/* Set it up only once ! */                                                              |
 |      	if (bc->event_handler != tick_handle_oneshot_broadcast) {                                |
 |      		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;                          |
 |      		int cpu = smp_processor_id();                                                    |
 |                                                                                                       |
 |      		bc->event_handler = tick_handle_oneshot_broadcast;                               |
                        // 设置 Global Event Device broadcast mode
 |      		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);                                |
 |                                                                                                       |
 |      		/* Take the do_timer update */                                                   |
 |      		tick_do_timer_cpu = cpu;                                                         |
 |                                                                                                       |
 |      		/*                                                                               |
 |      		 * We must be careful here. There might be other CPUs                            |
 |      		 * waiting for periodic broadcast. We need to set the                            |
 |      		 * oneshot_mask bits for those and program the                                   |
 |      		 * broadcast device to fire.                                                     |
 |      		 */                                                                              |
 |      		cpumask_copy(to_cpumask(tmpmask), tick_get_broadcast_mask());                    |
 |      		cpumask_clear_cpu(cpu, to_cpumask(tmpmask));                                     |
 |      		cpumask_or(tick_get_broadcast_oneshot_mask(),                                    |
 |      			   tick_get_broadcast_oneshot_mask(),                                    |
 |      			   to_cpumask(tmpmask));                                                 |
 |                                                                                                       |
 |      		if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {                       |
 |      			tick_broadcast_init_next_event(to_cpumask(tmpmask),                      |
 |      						       tick_next_period);                        |
 |      			tick_broadcast_set_event(tick_next_period, 1);                           |
 |      		} else                                                                           |
 |      			bc->next_event.tv64 = KTIME_MAX;                                         |
 |      	}                                                                                        |
 |      }                                                                                                |
 |                                                                                                       |
 |        最终设置 Global Clock Event Device hpet -> event_handler = tick_handle_oneshot_broadcast       |
 |	          Local APIC                      -> event_handler = hrtimer_interrupt                   |
 |	          Local APIC                      -> event_handler = hrtimer_interrupt                   |
 |	          Local APIC                      -> event_handler = hrtimer_interrupt                   |
 |	          Local APIC                      -> event_handler = hrtimer_interrupt                   |
 |                                                                                                       |
 |     只有在 LAPIC 存在下的情况下 Global 才会是 broadcast mode：                                        |
 |          set boot parameter "nolapic",cpu0 global event_handler:                                      |
 |	                                                           hres:                                 |
 |	                                                           hrtimer_interrupt ()                  |
 |	                                                           lres:                                 |
 |								   tick_nohz_handler ()                  |
 |	                                                           tick_handle_periodic ()               |
 |                                                                                                       |
 |                                                                                                       |
 +-------------------------------------------------------------------------------------------------------+

1.正常情况下
   event_handler:  tick_handle_oneshot_broadcast
   event_handler:  hrtimer_interrupt
   event_handler:  hrtimer_interrupt
   event_handler:  hrtimer_interrupt
   event_handler:  hrtimer_interrupt

   // cpu 0-3 都参与更新 jiffies do_timer 工作 (global 好像不参加)
      smp_apic_timer_interrupt()
       -> irq_enter()
	   -> tick_check_idle()
	       -> tick_check_nohz()
	           -> tick_nohz_update_jiffies()
	               -> tick_do_update_jiffies64()
	                   -> do_timer(++ticks)


2.boot parameter "nohz=off highres=off"
   event_handler:  clockevents_handle_noop
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic

  // tick_do_timer_cpu = 0  CPU0(lapic) 负责更新 jiffies do_timer


3.boot parameter "nohz=off highres=off"
   event_handler:  tick_handle_periodic_broadcast
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic
   event_handler:  tick_handle_periodic

   // 跟 2 一样

4.boot parameter "highres=off"
   event_handler:  tick_handle_oneshot_broadcast
   event_handler:  tick_nohz_handler
   event_handler:  tick_nohz_handler
   event_handler:  tick_nohz_handler
   event_handler:  tick_nohz_handler

   // 跟 1 一样


timer_interrupt() 时钟中断处理程序
	--> global_clock_event->event_handler(global_clock_event); --> tick_handle_oneshot_broadcast
	                                                           --> tick_handle_periodic()   CPU-0 经测试
								       --> tick_periodic()

Red Hat Enterprise Linux 6 kernels operate the HPET timer device in 'one shot' mode (as opposed to periodic mode) to implement timer ticks with variable length intervals. The kernel reads the HPET main counter register, adds a delta and writes the resulting value to a comparator register. The HPET timer device shall generate an interrupt when the main counter reaches the value in the comparator register.
 
There is a possibility that the main counter is already beyond the resulting value at the time when the value is written to the comparator register. The kernel takes precautions to detect this condition. However, these precautions seem to miss some particular scenarios that are possible with certain HPET implementations.

If this condition is not detected, the HPET main counter needs to 'wrap around' in order to reach the value in the comparator register. In HPET 32-bit mode this can take close to 4294967295 cycles, which is roughly equivalent to 5 minutes (at a HPET counter frequency of 14.318180 MHz). During this period it is possible that kernel timers that depend on HPET interrupts do not expire as expected, which can cause adverse effects on performance.

Disabling the HPET timer device may work around the issue on systems that also have a PIT timer device. If the parameter 'hpet=disable' is specified on the kernel command line (for example, in grub.conf) the kernel may alternatively use PIT interrupts to drive the 'global clock event'.

Since the PIT timer device has a lower resolution than the HPET timer device, this change can have some impact on scheduling 'kernel high resolution timers', for example, during periods of low workload where the 'intel_idle' module may decide to 'shut down' CPU-local clock event devices (APIC timers).

Another side effect of disabling the HPET timer device is that it can no longer be used as a clocksource. If, for example, the kernel detects an unstable TSC that should not be used as a clocksource, it attempts to switch to another available clocksource.

However, if HPET is not available, a lower-resolution clocksource such as ACPI PM may be selected. This can have an impact on the resolution of time values returned by some library functions, for example, clock_gettime().



/*              
 * Event handler for periodic ticks
 */             
void tick_handle_periodic(struct clock_event_device *dev)
{       
        int cpu = smp_processor_id();
        ktime_t next;
        
        tick_periodic(cpu);

        if (dev->mode != CLOCK_EVT_MODE_ONESHOT)
                return;

	// 若此时为周期时间，则直接返回
	// 若为 one-shot 模式 ，则需要编程设置下一个时钟事件

        /*
         * Setup the next period for devices, which do not have
         * periodic mode:
         */             
        next = ktime_add(dev->next_event, tick_period);
        for (;;) {
                if (!clockevents_program_event(dev, next, ktime_get()))
                        return;
                /*
                 * Have to be careful here. If we're in oneshot mode,
                 * before we call tick_periodic() in a loop, we need
                 * to be sure we're using a real hardware clocksource.
                 * Otherwise we could get trapped in an infinite
                 * loop, as the tick_periodic() increments jiffies,
                 * when then will increment time, posibly causing
                 * the loop to trigger again and again.
                 */
                if (timekeeping_valid_for_hres())
                        tick_periodic(cpu);
                next = ktime_add(next, tick_period);
        }
}


/*      
 * Periodic tick
 */     
static void tick_periodic(int cpu)
{       
        if (tick_do_timer_cpu == cpu) {
                write_seqlock(&xtime_lock);

                /* Keep track of the next tick event */
                tick_next_period = ktime_add(tick_next_period, tick_period);
        
                do_timer(1);
                write_sequnlock(&xtime_lock);
        }               

        update_process_times(user_mode(get_irq_regs()));
        profile_tick(CPU_PROFILING);
}

/*
 * The 64-bit jiffies value is not atomic - you MUST NOT read it
 * without sampling the sequence number in xtime_lock.
 * jiffies is defined in the linker script...
 */             

void do_timer(unsigned long ticks)
{               
        jiffies_64 += ticks;
        update_wall_time();
        calc_global_load();
}       


smp_apic_timer_interrupt()
	--> local_apic_timer_interrupt()
	            evt->event_handler(evt);  --> hrtimer_interrupt() 



In Summary:
1. Global Clock Event device (hpet) 
2. Local Clock Event device (lapic eg.4 cpu)

首先，初始化全局时钟事件设备 hpet ,之后初始化本地 cpu lapic，假设 4 个 cpu,则 设置4次，都将 event_handler 设置为 tick_handle_periodic

 Running as the following instructions:
 clockevents_register_device()
  -> tick_setup_device() 
      -> tick_setup_periodic()
	 set the event_handler = tick_handle_periodic 


最后：
invoke the tick_broadcast_switch_to_oneshot()
	    -> tick_broadcast_setup_oneshot()
	       // 设置 event_handler 为 tick_handle_oneshot_broadcast 


---------------------------------------------------------------------------------------------------------------------------------------------------------
start_secondary()
{
	......

	x86_cpuinit.setup_percpu_clockev();
	// invoke setup_secondary_APIC_clock 

	.......
}
  
 +--------------------------------------------------------------------------------------+
 |      struct x86_cpuinit_ops x86_cpuinit __cpuinitdata = {                            |
 |      		.setup_percpu_clockev           = setup_secondary_APIC_clock,   |
 |      };                                                                              |
 |                                                                                      |
 +--------------------------------------------------------------------------------------+

void __cpuinit setup_secondary_APIC_clock(void)
{
	        setup_APIC_timer();
		// 注册时钟事件设备
}

[Switching to Thread 3]
do_boot_cpu()
   --> wakeup_secondary_cpu_via_init()
         --> startup_ipi_hook()

[New Thread 4]
[Switching to Thread 4]


[进程 1]
kernel_init()
  --> smp_init()
        --> cpu_up()
	      ...
	      --> __cpu_up() 
                        smp_ops.cpu_up(cpu)
	            --> native_cpu_up()  
	                  --> do_boot_cpu() 
	                        --> wakeup_secondary_cpu_via_init()
				      --> startup_ipi_hook()
	                                  ......
                                          maybe 
					  start_secondary()
	                                          --> setup_secondary_APIC_clock()
	                                                --> setup_APIC_timer()

在 do_boot_cpu() 函数中在每个cpu上建立0号进程，

在 wakeup_secondary_cpu_via_ini 函数中向要启动的 cpu 发送 IPI 中断，AP cpu ,然后 跳转到trampoline.S 的入口装入 gdt 和 idt ,然后 initialize_secondary() 最后根据 current-> thread.esp 最终跳转至 start_secondary. 

static inline void startup_ipi_hook(int phys_apicid, unsigned long start_eip,
                                    unsigned long start_esp)
{
        PVOP_VCALL3(pv_apic_ops.startup_ipi_hook,
                    phys_apicid, start_eip, start_esp);
}


#define PVOP_VCALL3(op, arg1, arg2, arg3)				\
	__PVOP_VCALL(op, "", "", PVOP_CALL_ARG1(arg1),			\
		     PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))
		    

#define PVOP_CALL_ARG1(x)		"D" ((unsigned long)(x))
#define PVOP_CALL_ARG2(x)		"S" ((unsigned long)(x))
#define PVOP_CALL_ARG3(x)		"d" ((unsigned long)(x))


#define __PVOP_VCALL(op, pre, post, ...)				\
	____PVOP_VCALL(op, CLBR_ANY, PVOP_VCALL_CLOBBERS,		\
		       VEXTRA_CLOBBERS,					\
		       pre, post, ##__VA_ARGS__)


#define CLBR_ANY  ((1 << 9) - 1)

#define PVOP_VCALL_CLOBBERS	"=D" (__edi),				\
				"=S" (__esi), "=d" (__edx),		\
				"=c" (__ecx)

#define VEXTRA_CLOBBERS	 , "rax", "r8", "r9", "r10", "r11"


#define ____PVOP_VCALL(op, clbr, call_clbr, extra_clbr, pre, post, ...) \
        ({                                                              \
                PVOP_VCALL_ARGS;                                        \
                PVOP_TEST_NULL(op);                                     \
                asm volatile(pre                                        \
                             paravirt_alt(PARAVIRT_CALL)                \
                             post                                       \
                             : call_clbr                                \
                             : paravirt_type(op),                       \
                               paravirt_clobber(clbr),                  \
                               ##__VA_ARGS__                            \
                             : "memory", "cc" extra_clbr);              \
        })


#define PVOP_VCALL_ARGS					\
	unsigned long __edi = __edi, __esi = __esi,	\
		__edx = __edx, __ecx = __ecx, __eax = __eax



  +-----------------------------------------------------------------------------+
  |                                                                             |
  |     struct smp_ops smp_ops = {                                              |
  |     	.smp_prepare_boot_cpu   = native_smp_prepare_boot_cpu,          |
  |     	.smp_prepare_cpus       = native_smp_prepare_cpus,              |
  |     	.smp_cpus_done          = native_smp_cpus_done,                 |
  |                                                                             |
  |     	.stop_other_cpus        = native_stop_other_cpus,               |
  |     	.smp_send_reschedule    = native_smp_send_reschedule,           |
  |                                                                             |
  |     	.cpu_up                 = native_cpu_up,                        |
  |     	.cpu_die                = native_cpu_die,                       |
  |     	.cpu_disable            = native_cpu_disable,                   |
  |     	.play_dead              = native_play_dead,                     |
  |                                                                             |
  |     	.send_call_func_ipi     = native_send_call_func_ipi,            |
  |     	.send_call_func_single_ipi = native_send_call_func_single_ipi,  |
  |     };                                                                      |
  |                                                                             |
  +-----------------------------------------------------------------------------+


static inline void smp_prepare_cpus(unsigned int max_cpus)
{
	        smp_ops.smp_prepare_cpus(max_cpus);
		// native_smp_prepare_cpus(64)
}


kernel_init()
  --> smp_prepare_cpus(setup_max_cpus) ==> native_smp_prepare_cpus(64)
       -> x86_init.timers.setup_percpu_clockev() ==> setup_boot_APIC_clock()
	 一切尽在不言中 ......
	                       

native_smp_prepare_cpus()

#define for_each_possible_cpu(cpu) for_each_cpu((cpu), cpu_possible_mask)


num_processors = 4   // 初始值为 0
nr_cpu_ids=4
disabled_cpus
total_cpus
possible

// ensure num_processors
setup_arch()
acpi_boot_init()
  --> acpi_process_madt()
	--> acpi_parse_madt_lapic_entries() 

// ensure nr_cpu_ids
__init void prefill_possible_map(void)
{
        int i, possible;

        /* no processor from mptable or madt */
        if (!num_processors)
                num_processors = 1;

        if (setup_possible_cpus == -1)
                possible = num_processors + disabled_cpus;
        else
                possible = setup_possible_cpus;

        total_cpus = max_t(int, possible, num_processors + disabled_cpus);

        /* nr_cpu_ids could be reduced via nr_cpus= */
        if (possible > nr_cpu_ids) {
                printk(KERN_WARNING
                        "%d Processors exceeds NR_CPUS limit of %d\n",
                        possible, nr_cpu_ids);
                possible = nr_cpu_ids;
        }

        printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
                possible, max_t(int, possible - num_processors, 0));

        for (i = 0; i < possible; i++)
                set_cpu_possible(i, true);

        nr_cpu_ids = possible;
}



/* Called by boot processor to activate the rest. */
static void __init smp_init(void)
{
        unsigned int cpu;

        /* FIXME: This should be done in userspace --RR */
        for_each_present_cpu(cpu) {
                if (num_online_cpus() >= setup_max_cpus)
                        break;
                if (!cpu_online(cpu))
                        cpu_up(cpu);
                        // 根据 nr_cpu_ids 循环 up CPU (4 个 CPU up 其他 3 个)
        }

        /* Any cleanup work */
        printk(KERN_INFO "Brought up %ld CPUs\n", (long)num_online_cpus());
        smp_cpus_done(setup_max_cpus);

        printk(KERN_DEBUG "sizeof(vma)=%u bytes\n", (unsigned int) sizeof(struct vm_area_struct));
        printk(KERN_DEBUG "sizeof(page)=%u bytes\n", (unsigned int) sizeof(struct page));
        printk(KERN_DEBUG "sizeof(inode)=%u bytes\n", (unsigned int) sizeof(struct inode));
        printk(KERN_DEBUG "sizeof(dentry)=%u bytes\n", (unsigned int) sizeof(struct dentry));
        printk(KERN_DEBUG "sizeof(ext3inode)=%u bytes\n", (unsigned int) sizeof(struct ext3_inode_info));
        printk(KERN_DEBUG "sizeof(buffer_head)=%u bytes\n", (unsigned int) sizeof(struct buffer_head));
        printk(KERN_DEBUG "sizeof(skbuff)=%u bytes\n", (unsigned int) sizeof(struct sk_buff));
        printk(KERN_DEBUG "sizeof(task_struct)=%u bytes\n", (unsigned int) sizeof(struct task_struct));
}

#define for_each_present_cpu(cpu)  for_each_cpu((cpu), cpu_present_mask)

#define cpu_online(cpu)		cpumask_test_cpu((cpu), cpu_online_mask)

/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
#define for_each_cpu(cpu, mask)                         \
        for ((cpu) = -1;                                \
                (cpu) = cpumask_next((cpu), (mask)),    \
                (cpu) < nr_cpu_ids;)

for (cpu = -1; cpu = cpumask_next(cpu,cpu_present_mask),cpu < 4;)

/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static inline unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	// srcp = cpu_present_mask
	
        /* -1 is a legal arg here. */
        if (n != -1)
                cpumask_check(n);
        return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n+1);
}

typedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;

#define DECLARE_BITMAP(name,bits) \
	unsigned long name[BITS_TO_LONGS(bits)]
// unsigned long bits[4] | unsigned bits name[1]

typedef struct cpumask {
	unsigned long bits[4];
} cpumask_t;

#define cpumask_bits(maskp) ((maskp)->bits)

#define nr_cpumask_bits	nr_cpu_ids
