start_kernel()
   --> tick_init()

/**
 * tick_init - initialize the tick control
 *
 * Register the notifier with the clockevents framework
 */
void __init tick_init(void)
{
        clockevents_register_notifier(&tick_notifier);
}


static struct notifier_block tick_notifier = {
        .notifier_call = tick_notify,
};

/**
 * clockevents_register_notifier - register a clock events change listener
 */
int clockevents_register_notifier(struct notifier_block *nb)
{
        // nb = &tick_notifier
        unsigned long flags;
        int ret;

        spin_lock_irqsave(&clockevents_lock, flags);
        ret = raw_notifier_chain_register(&clockevents_chain, nb);
        spin_unlock_irqrestore(&clockevents_lock, flags);
        
        return ret;
}

/* Notification for clock events */
static RAW_NOTIFIER_HEAD(clockevents_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

#define RAW_NOTIFIER_INIT(name) {                               \
                .head = NULL }

struct raw_notifier_head {
        struct notifier_block *head;
};

struct notifier_block {
        int (*notifier_call)(struct notifier_block *, unsigned long, void *);
        struct notifier_block *next;
        int priority;
};


int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}

/*
 *      Notifier chain core routines.  The exported routines below
 *      are layered on top of these, with appropriate locking added.
 */

static int notifier_chain_register(struct notifier_block **nl,
                struct notifier_block *n)
{               
        // 此时 *nl = NULL

        while ((*nl) != NULL) {
                if (n->priority > (*nl)->priority)
                        break;
                nl = &((*nl)->next);
        }
        n->next = *nl;
        n->next = NULL;

        rcu_assign_pointer(*nl, n);
        *nl = n;
        // clockevents_chain->head = &tick_notifier;

        return 0;
}

/**
 * rcu_assign_pointer - assign (publicize) a pointer to a newly
 * initialized structure that will be dereferenced by RCU read-side
 * critical sections.  Returns the value assigned.
 *
 * Inserts memory barriers on architectures that require them
 * (pretty much all of them other than x86), and also prevents
 * the compiler from reordering the code that initializes the
 * structure after the pointer assignment.  More importantly, this
 * call documents which pointers will be dereferenced by RCU read-side
 * code.
 */             

#define rcu_assign_pointer(p, v) \
        ({ \    
                if (!__builtin_constant_p(v) || \
                    ((v) != NULL)) \
                        smp_wmb(); \
                (p) = (v); \
        })

当时钟事件设备信息发生变化时调用tick_notifier



-----------------------------------------------------------------------------------------------------------------------

start_kernel:
    init_IRQ()

void __init init_IRQ(void)
{
	x86_init.irqs.intr_init();
        // invoke native_init_IRQ() function
}

void __init native_init_IRQ(void)
{                                                                                                                                                          
        int i;

        /* Execute any quirks before the call gates are initialised: */
        x86_init.irqs.pre_vector_init();
        // invoke init_ISA_irqs()

        apic_intr_init();

        /*
         * Cover the whole vector space, no vector can escape
         * us. (some of these will be overridden and become
         * 'special' SMP interrupts)
         */
        for (i = FIRST_EXTERNAL_VECTOR; i < NR_VECTORS; i++) {
                /* IA32_SYSCALL_VECTOR could be used in trap_init already. */
                if (!test_bit(i, used_vectors))
                        set_intr_gate(i, interrupt[i-FIRST_EXTERNAL_VECTOR]);
        }
	// 设置中断门，处理程序指向 interrupt 数组

        if (!acpi_ioapic)
                setup_irq(2, &irq2);

#ifdef CONFIG_X86_32
        /*
         * External FPU? Set up irq13 if so, for
         * original braindamaged IBM FERR coupling.
         */
        if (boot_cpu_data.hard_math && !cpu_has_fpu)
                setup_irq(FPU_IRQ, &fpu_irq);

        irq_ctx_init(smp_processor_id());
#endif
}

#define FIRST_EXTERNAL_VECTOR           0x20
#define NR_VECTORS                       256

void __init init_ISA_irqs(void)
{      
        int i;

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        init_bsp_APIC();
        // 配置本地LAPIC芯片。该函数调用apic_read或apic_write调用全局变量apic的read和write方法
#endif 
        init_8259A(0);

        /*
         * 16 old-style INTA-cycle interrupts:
         */
        for (i = 0; i < NR_IRQS_LEGACY; i++) {
                struct irq_desc *desc = irq_to_desc(i);
               
                desc->status = IRQ_DISABLED;
                desc->action = NULL;
                desc->depth = 1;

                set_irq_chip_and_handler_name(i, &i8259A_chip,
                                              handle_level_irq, "XT");
        }
}

void __init init_bsp_APIC(void)
{
        unsigned int value;
        
        /*
         * Don't do the setup now if we have a SMP BIOS as the
         * through-I/O-APIC virtual wire mode might be active.
         */
        if (smp_found_config || !cpu_has_apic)
                return;
       
        /*
         * Do not trust the local APIC being empty at bootup.
         */
        clear_local_APIC();

        /*
         * Enable APIC.
         */
        value = apic_read(APIC_SPIV);
        value &= ~APIC_VECTOR_MASK;
        value |= APIC_SPIV_APIC_ENABLED;

#ifdef CONFIG_X86_32
        /* This bit is reserved on P4/Xeon and should be cleared */
        if ((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &&
            (boot_cpu_data.x86 == 15))
                value &= ~APIC_SPIV_FOCUS_DISABLED;
        else
#endif
                value |= APIC_SPIV_FOCUS_DISABLED;
        value |= SPURIOUS_APIC_VECTOR;
        apic_write(APIC_SPIV, value);

        /*
         * Set up the virtual wire mode.
         */
        apic_write(APIC_LVT0, APIC_DM_EXTINT);
        value = APIC_DM_NMI;
        if (!lapic_is_integrated())             /* 82489DX */
                value |= APIC_LVT_LEVEL_TRIGGER;
        apic_write(APIC_LVT1, value);
}

// apic 其代表一块LAPIC控制器芯片 init_bsp_APIC函数实际上调用native_apic_mem_read和native_apic_mem_write等方法
struct apic *apic = &apic_default;

static void __init apic_intr_init(void)
{      
        smp_intr_init();

        // 设置终端描述符表 IDT APIC 相关中断服务程序。也就是把位于32~255之间，除去系统调用外其他中断向量的中断处理程序设置为interrupt[i]

#ifdef CONFIG_X86_THERMAL_VECTOR
        alloc_intr_gate(THERMAL_APIC_VECTOR, thermal_interrupt);
#endif
#ifdef CONFIG_X86_MCE_THRESHOLD
        alloc_intr_gate(THRESHOLD_APIC_VECTOR, threshold_interrupt);
#endif
#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_LOCAL_APIC)
        alloc_intr_gate(MCE_SELF_VECTOR, mce_self_interrupt);
#endif  

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        /* self generated IPI for local APIC timer */
        alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
	/*
	 *将中断处理函数直接放在中断门得指向地址，这样只要中断到来，一旦通过中断门将直接跳像中断处理函数，而忽略了irq_d         * esc部分，不需要考虑怎么触发，不需要考虑怎么调度
	 * cat /proc/interrups
	 * LOC:   71070080   67165396   44751028   31680629   Local timer interrupts
	 *
	 * cat /proc/interrupts 也可以看到这些中断与众不同.这些中断左边显示的不是中断请求号，而是一个标识，右边显示的也         * 不是中断控制芯片的信息。
	 * 
	 */
        // 就是这里。。。。。。

        /* IPI for X86 platform specific use */
        alloc_intr_gate(X86_PLATFORM_IPI_VECTOR, x86_platform_ipi);

        /* IPI vectors for APIC spurious and error interrupts */
        alloc_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
        alloc_intr_gate(ERROR_APIC_VECTOR, error_interrupt);

        /* Performance monitoring interrupts: */
# ifdef CONFIG_PERF_EVENTS
        alloc_intr_gate(LOCAL_PENDING_VECTOR, perf_pending_interrupt);
# endif

#endif
}

#define LOCAL_TIMER_VECTOR              0xef   //239

static inline void alloc_intr_gate(unsigned int n, void *addr)
{      
        alloc_system_vector(n);
        set_intr_gate(n, addr);
}

tatic inline void alloc_system_vector(int vector)
{      
        if (!test_bit(vector, used_vectors)) {
                set_bit(vector, used_vectors);
                if (first_system_vector > vector)
                        first_system_vector = vector;
        } else
                BUG();
}

static inline void set_intr_gate(unsigned int n, void *addr)
{      
        BUG_ON((unsigned)n > 0xFF);
        _set_gate(n, GATE_INTERRUPT, addr, 0, 0, __KERNEL_CS);
}

static inline void _set_gate(int gate, unsigned type, void *addr,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate_desc s;
        pack_gate(&s, type, (unsigned long)addr, dpl, ist, seg);
        /*
         * does not need to be atomic because it is only done once at
         * setup time
         */
        write_idt_entry(idt_table, gate, &s);
}

/*
 * X86_64
 */
static inline void pack_gate(gate_desc *gate, unsigned type, unsigned long func,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate->offset_low = PTR_LOW(func);
        gate->segment = __KERNEL_CS;
        gate->ist = ist;
        gate->p = 1;         
        gate->dpl = dpl;     
        gate->zero0 = 0;
        gate->zero1 = 0;
        gate->type = type;
        gate->offset_middle = PTR_MIDDLE(func);
        gate->offset_high = PTR_HIGH(func);
}


BUILD_INTERRUPT(apic_timer_interrupt,LOCAL_TIMER_VECTOR)

#define BUILD_INTERRUPT(name, nr)       BUILD_INTERRUPT3(name, nr, smp_##name)

/*
 *  Irq entries should be protected against kprobes
 */
        .pushsection .kprobes.text, "ax"
#define BUILD_INTERRUPT3(name, nr, fn)  \
ENTRY(name)                             \
        RING0_INT_FRAME;                \
        pushl $~(nr);                   \
        CFI_ADJUST_CFA_OFFSET 4;        \
        SAVE_ALL;                       \
        TRACE_IRQS_OFF                  \
        movl %esp,%eax;                 \
        call fn;                        \
        jmp ret_from_intr;              \
        CFI_ENDPROC;                    \
ENDPROC(name)

当 cpu 执行完一条指令后会见查是否有中断发生，此时 assume Local APIC 发生中断请求，cpu 则根据中断号在中断描述符表中查找到段选择子，进而找到处理程序，此时就是执行 apic_timer_interrupt,然后 call smp_apic_timer_interrupt ...... 于是发生了一些很重要的事情。

-----------------------------------------------------------------------------------------------------------------------

start_kernel:
     init_timers();

void __init init_timers(void)
{       
        int err = timer_cpu_notify(&timers_nb, (unsigned long)CPU_UP_PREPARE,
                                (void *)(long)smp_processor_id());
        
        init_timer_stats();
        
        BUG_ON(err == NOTIFY_BAD);
        register_cpu_notifier(&timers_nb);
        open_softirq(TIMER_SOFTIRQ, run_timer_softirq);
        // 软中断 下半步 高精度 timer.
}

static struct notifier_block __cpuinitdata timers_nb = {
        .notifier_call  = timer_cpu_notify,
};

初始化指定CPU上的软时钟相关的数据结构
static int __cpuinit timer_cpu_notify(struct notifier_block *self,
                                unsigned long action, void *hcpu)
{       
        long cpu = (long)hcpu;
        switch(action) {
        case CPU_UP_PREPARE:
        case CPU_UP_PREPARE_FROZEN:
                if (init_timers_cpu(cpu) < 0)
                        return NOTIFY_BAD;
                break;
#ifdef CONFIG_HOTPLUG_CPU
        case CPU_DEAD:
        case CPU_DEAD_FROZEN:
                migrate_timers(cpu);
                break;
#endif
        default:
                break;
        }
        return NOTIFY_OK;
}

static int __cpuinit init_timers_cpu(int cpu)
{
        int j;
        struct tvec_base *base;
        static char __cpuinitdata tvec_base_done[NR_CPUS];

        if (!tvec_base_done[cpu]) {
                static char boot_done;

                if (boot_done) {
                        /*
                         * The APs use this path later in boot
                         */
                        base = kmalloc_node(sizeof(*base),
                                                GFP_KERNEL | __GFP_ZERO,
                                                cpu_to_node(cpu));
                        if (!base)
                                return -ENOMEM;

                        /* Make sure that tvec_base is 2 byte aligned */
                        if (tbase_get_deferrable(base)) {
                                WARN_ON(1);
                                kfree(base);
                                return -ENOMEM;
                        }
                        per_cpu(tvec_bases, cpu) = base;
                } else {
                        /*
                         * This is for the boot CPU - we use compile-time
                         * static initialisation because per-cpu memory isn't
                         * ready yet and because the memory allocators are not
                         * initialised either.
                         */
                        boot_done = 1;
                        base = &boot_tvec_bases;
                }
                tvec_base_done[cpu] = 1;
        } else {
                base = per_cpu(tvec_bases, cpu);
        }

        spin_lock_init(&base->lock);

        for (j = 0; j < TVN_SIZE; j++) {
                INIT_LIST_HEAD(base->tv5.vec + j);
                INIT_LIST_HEAD(base->tv4.vec + j);
                INIT_LIST_HEAD(base->tv3.vec + j);
                INIT_LIST_HEAD(base->tv2.vec + j);
        }
        for (j = 0; j < TVR_SIZE; j++)
                INIT_LIST_HEAD(base->tv1.vec + j);

        base->timer_jiffies = jiffies;
        // 设置成自系统启动以来产生的节拍数。
        base->next_timer = base->timer_jiffies;
        return 0;
}

struct tvec_base {
        spinlock_t lock;
        struct timer_list *running_timer;
        unsigned long timer_jiffies;
        unsigned long next_timer;
        struct tvec_root tv1;
        struct tvec tv2;
        struct tvec tv3;
        struct tvec tv4;
        struct tvec tv5;
} ____cacheline_aligned;


static inline uint32_t raid6_jiffies(void)
{               
        struct timeval tv;
        gettimeofday(&tv, NULL);
        return tv.tv_sec*1000 + tv.tv_usec/1000;
}    

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
};


static __always_inline int gettimeofday(struct timeval *tv, struct timezone *tz)
{
        int ret;
        asm volatile("syscall"
                : "=a" (ret)
                : "0" (__NR_gettimeofday),"D" (tv),"S" (tz)
                : __syscall_clobber );
        return ret;
}       


 +------------------------------+
 |                              |
 |    系统调用 gettimeofday     |
 |                              |
 +------------------------------+

SYSCALL_DEFINE2(gettimeofday, struct timeval __user *, tv,
                struct timezone __user *, tz)
{      
        if (likely(tv != NULL)) {                                                                                                              
                struct timeval ktv;                                                                                                            
                do_gettimeofday(&ktv);
                if (copy_to_user(tv, &ktv, sizeof(ktv)))
                        return -EFAULT;
                // 操作系统和驱动程序在内核空间运行，应用程序在用户空间运行，两者不能简单地使用指针传递数据. 因为Linux系统使用了虚拟内存机制，用户空间的内存可能被换出，当内核空间使用用户空间指针时，对应的数据可能不在内存中
        }
        if (unlikely(tz != NULL)) {
                if (copy_to_user(tz, &sys_tz, sizeof(sys_tz)))
                        return -EFAULT;
        }
        return 0;
}

#define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)



/**             
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:         pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{       
        struct timespec now;
                
        getnstimeofday(&now);
	// now 中返回当前时间 秒数以及纳秒数
	
        tv->tv_sec = now.tv_sec;
	// 秒数
        tv->tv_usec = now.tv_nsec/1000;
	// now.tv_nsec 不足一秒的纳秒数 转换为微妙数.
}       

CONFIG_GENERIC_TIME=y
// rhel6

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
}; 

/**
 * getnstimeofday - Returns the time of day in a timespec
 * @ts:         pointer to the timespec to be set
 *
 * Returns the time of day in a timespec.
 */
void getnstimeofday(struct timespec *ts)
{       
        unsigned long seq;
        s64 nsecs;

        WARN_ON(timekeeping_suspended);

        do {
                seq = read_seqbegin(&xtime_lock);
		// 加读锁，如果此时已经加上的写锁，则自旋

                *ts = xtime;
		// 此时很有可能更新 xtime.(时钟中断处理程序)
		
                nsecs = timekeeping_get_ns();

                /* If arch requires, add in gettimeoffset() */
                nsecs += arch_gettimeoffset(); // NULL function ?

        } while (read_seqretry(&xtime_lock, seq));
	// 顺序锁，判断是否重读. 因为此时可能某个进程写时钟源.这样读出来的值就不准确需要重读.

        timespec_add_ns(ts, nsecs);
        // add nsecs 到 struct timespec 结构.
}

 +--------------------------------------------------------------------------------------------------------------------+
 |                                                                                                                    |
 |无非三种情况:                                                                                                       |
 |  1 循环只执行一遍，准确获得当前时间                                                                                |
 |  2 顺利加上读锁，但是与此同时时钟中断到来需要更新xtime,加上写锁(有更高的优先级)，让后 read_seqretry 返回 1 表示时间|      已经不准却需要重新计算。在再次循环读取。
 |  3 读锁阻塞，因为此时已经加上了写锁(更新 xtime),等到写锁释放后便可读取.                                            |
 |                                                                                                                    |
 +--------------------------------------------------------------------------------------------------------------------+


truct timespec {
        time_t  tv_sec;         /* seconds */
        long    tv_nsec;        /* nanoseconds */
};

struct timespec xtime __attribute__ ((aligned (16))); 

/*                                                                                      
 * This read-write spinlock protects us from races in SMP while                         
 * playing with xtime.                                                                  
 */                                                                                     
__cacheline_aligned_in_smp DEFINE_SEQLOCK(xtime_lock);                   

#define DEFINE_SEQLOCK(x) \
                seqlock_t x = __SEQLOCK_UNLOCKED(x)                                     

typedef struct {                                                                        
        unsigned sequence;                                                    
        spinlock_t lock;                                                                
} seqlock_t;

// 记录了写着进程访问临界资源的过程 初始化为 0 写时 +1,解除写锁时再 +1 ,该直为奇数时处于写>锁定状态                    
// 读直接访问不需要加锁

#define __SEQLOCK_UNLOCKED(lockname) \
                 { 0, __SPIN_LOCK_UNLOCKED(lockname) }

# define __SPIN_LOCK_UNLOCKED(lockname) \
        (spinlock_t)    {       .raw_lock = __RAW_SPIN_LOCK_UNLOCKED,   \
                                SPIN_DEP_MAP_INIT(lockname) }


/* Start of read calculation -- fetch last complete writer token */
static __always_inline unsigned read_seqbegin(const seqlock_t *sl)
{
        unsigned ret;

repeat:
        ret = sl->sequence;
        smp_rmb();
        if (unlikely(ret & 1)) {
                cpu_relax();
                goto repeat;
        }

        return ret;
}


/*              
 * Test if reader processed invalid data.
 *
 * If sequence value changed then writer changed data while in section.
 */             
static __always_inline int read_seqretry(const seqlock_t *sl, unsigned start)
{      
	// 假设 start = 0
        smp_rmb();
        
        return (sl->sequence != start);
}       




/* Timekeeper helper functions. */
static inline s64 timekeeping_get_ns(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;

        /* read clocksource: */
        clock = timekeeper.clock;
        // 时钟源 PIT/HPET/TSC

        cycle_now = clock->read(clock);
        // 读取时钟周期的当前计数值 假设为 TSC --> read_tsc()

        /* calculate the delta since the last update_wall_time: */
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
	// clock->cycle_last 应该由时钟中断处理程序负责更新.

        /* return delta convert to nanoseconds using ntp adjusted mult. */
        return clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
	//转换成纳秒
}

/* Structure holding internal timekeeping values. */
struct timekeeper {
        /* Current clocksource used for timekeeping. */
        struct clocksource *clock;
        /* The shift value of the current clocksource. */
        int     shift;

        /* Number of clock cycles in one NTP interval. */
        cycle_t cycle_interval; 
        /* Number of clock shifted nano seconds in one NTP interval. */
        u64     xtime_interval;
        /* Raw nano seconds accumulated per NTP interval. */
        u32     raw_interval;

        /* Clock shifted nano seconds remainder not stored in xtime.tv_nsec. */
        u64     xtime_nsec;
        /* Difference between accumulated time and NTP time in ntp
         * shifted nano seconds. */
        s64     ntp_error;
        /* Shift conversion between clock shifted nano seconds and
         * ntp shifted nano seconds. */
        int     ntp_error_shift;
        /* NTP adjusted clock multiplier */
        u32     mult;
};

typedef u64 cycle_t;

/**     
 *  * clocksource_cyc2ns - converts clocksource cycles to nanoseconds
 *   *      
 *    * Converts cycles to nanoseconds, using the given mult and shift.
 *     *      
 *      * XXX - This could use some mult_lxl_ll() asm optimization
 *       */     
static inline s64 clocksource_cyc2ns(cycle_t cycles, u32 mult, u32 shift)
{       
	        return ((u64) cycles * mult) >> shift;
}

/**
 *  * timespec_add_ns - Adds nanoseconds to a timespec
 *   * @a:          pointer to timespec to be incremented
 *    * @ns:         unsigned nanoseconds value to be added
 *     *              
 *      * This must always be inlined because its used from the x86-64 vdso,
 *       * which cannot call other kernel functions.
 *        */             
static __always_inline void timespec_add_ns(struct timespec *a, u64 ns)
{
	        a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, NSEC_PER_SEC, &ns);
                // 返回 秒数 一般情况下返回 0
		a->tv_nsec = ns;
		// 不足 1s 的纳秒数 (此值会一直叠加)
}       


static __always_inline u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{       
	// dividend = a->tv_nsec + ns
        u32 ret = 0;

        while (dividend >= divisor) {
		// 如果 纳秒大于一秒 NSEC_PER_SEC 

                /* The following asm() prevents the compiler from
                   optimising this loop into a modulo operation.  */
                asm("" : "+rm"(dividend));

                dividend -= divisor;
		// 减去一秒

                ret++;
		// 秒数加1
        }
        
        *remainder = dividend;

        return ret;
}

 +-------------------+
 |gettimeofday over! |
 +-------------------+


void __init init_timer_stats(void)
{                                    
        int cpu;
        
        for_each_possible_cpu(cpu)
                spin_lock_init(&per_cpu(lookup_lock, cpu));
}      

/* Need to know about CPUs going up/down? */
int __ref register_cpu_notifier(struct notifier_block *nb)
{
        nb=&timers_nb

        int ret;
        cpu_maps_update_begin();
        ret = raw_notifier_chain_register(&cpu_chain, nb);
        cpu_maps_update_done();
        return ret;
}

static __cpuinitdata RAW_NOTIFIER_HEAD(cpu_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

void cpu_maps_update_begin(void)
{
        mutex_lock(&cpu_add_remove_lock);
}

/**
 *      raw_notifier_chain_register - Add notifier to a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @n: New entry in notifier chain
 *
 *      Adds a notifier to a raw notifier chain.
 *      All locking must be provided by the caller.
 *
 *      Currently always returns zero.
 */
int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    hrtimers_init();

void __init hrtimers_init(void)
{
        hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
                          (void *)(long)smp_processor_id());
        register_cpu_notifier(&hrtimers_nb);
#ifdef CONFIG_HIGH_RES_TIMERS
        open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);
#endif
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    timekeeping_init();

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
        struct clocksource *clock;
        unsigned long flags;
        struct timespec now, boot;

        read_persistent_clock(&now);
        read_boot_clock(&boot);

        write_seqlock_irqsave(&xtime_lock, flags);

        ntp_init();

        clock = clocksource_default_clock();
        // clock = &clocksource_jiffies
    
        if (clock->enable)
                clock->enable(clock);
        timekeeper_setup_internals(clock);

        xtime.tv_sec = now.tv_sec;
        xtime.tv_nsec = now.tv_nsec;
        raw_time.tv_sec = 0;
        raw_time.tv_nsec = 0;
        if (boot.tv_sec == 0 && boot.tv_nsec == 0) {
                boot.tv_sec = xtime.tv_sec;
                boot.tv_nsec = xtime.tv_nsec;
        }
        set_normalized_timespec(&wall_to_monotonic,
                                -boot.tv_sec, -boot.tv_nsec);
        update_xtime_cache(0);
        total_sleep_time.tv_sec = 0;
        total_sleep_time.tv_nsec = 0;
        write_sequnlock_irqrestore(&xtime_lock, flags);
}



static cycle_t jiffies_read(struct clocksource *cs)
{
	return (cycle_t) jiffies;
}

struct clocksource clocksource_jiffies = {
	.name		= "jiffies",
	.rating		= 1, /* lowest valid rating*/
	.read		= jiffies_read,
	.mask		= 0xffffffff, /*32bits*/
	.mult		= NSEC_PER_JIFFY << JIFFIES_SHIFT, /* details above */
	.shift		= JIFFIES_SHIFT,
};

#define NSEC_PER_JIFFY	((u32)((((u64)NSEC_PER_SEC)<<8)/ACTHZ))

#define ACTHZ (SH_DIV (CLOCK_TICK_RATE, LATCH, 8))

#define CLOCK_TICK_RATE		PIT_TICK_RATE
#define PIT_TICK_RATE 1193182ul
#define LATCH  ((CLOCK_TICK_RATE + HZ/2) / HZ)	/* For divider */


#define SH_DIV(NOM,DEN,LSH) (   (((NOM) / (DEN)) << (LSH))              \
                             + ((((NOM) % (DEN)) << (LSH)) + (DEN) / 2) / (DEN))

HZ=1000

#define NSEC_PER_SEC	1000000000L


/**
 *  * read_boot_clock -  Return time of the system start.
 *   *
 *    * Weak dummy function for arches that do not yet support it.
 *     * Function to read the exact time the system has been started.
 *      * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported.
 *       *      
 *        *  XXX - Do be sure to remove it once all arches implement it.
 *         */
void __attribute__((weak)) read_boot_clock(struct timespec *ts)
{
	        ts->tv_sec = 0;
		ts->tv_nsec = 0;
}       

void __init ntp_init(void)
{       
        ntp_clear();
        hrtimer_init(&leap_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
        leap_timer.function = ntp_leap_second;
}       

/**
 * ntp_clear - Clears the NTP state variables
 *
 * Must be called while holding a write on the xtime_lock
 */
void ntp_clear(void)
{
        time_adjust     = 0;            /* stop active adjtime() */
        time_status     |= STA_UNSYNC;
        time_maxerror   = NTP_PHASE_LIMIT;
        time_esterror   = NTP_PHASE_LIMIT;

        ntp_update_frequency();

        tick_length     = tick_length_base;
        time_offset     = 0;
}

/**     
 * hrtimer_init - initialize a timer to the given clock
 * @timer:      the timer to be initialized
 * @clock_id:   the clock to be used
 * @mode:       timer mode abs/rel
 */     
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                  enum hrtimer_mode mode)
{
        debug_init(timer, clock_id, mode);
        __hrtimer_init(timer, clock_id, mode);
}       
EXPORT_SYMBOL_GPL(hrtimer_init);

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                           enum hrtimer_mode mode)
{               
        struct hrtimer_cpu_base *cpu_base;
        
        memset(timer, 0, sizeof(struct hrtimer));
        
        cpu_base = &__raw_get_cpu_var(hrtimer_bases);
                
        if (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)
                clock_id = CLOCK_MONOTONIC;
                                         
        timer->base = &cpu_base->clock_base[clock_id];
        hrtimer_init_timer_hres(timer);
        
#ifdef CONFIG_TIMER_STATS  
        timer->start_site = NULL;
        timer->start_pid = -1;
        memset(timer->start_comm, 0, TASK_COMM_LEN);
#endif  
}


struct clocksource * __init __weak clocksource_default_clock(void)
{
        return &clocksource_jiffies;
}


/**
 * timekeeper_setup_internals - Set up internals to use clocksource clock.
 *
 * @clock:              Pointer to clocksource.
 *
 * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
 * pair and interval request.
 *
 * Unless you're the timekeeping code, you should not be using this!
 */
static void timekeeper_setup_internals(struct clocksource *clock)
{
        cycle_t interval;
        u64 tmp;

        timekeeper.clock = clock;
	// 设置时钟源 time_keeper gettimeofday 会通过 timer_keeper layer 获取 wall time
	
        clock->cycle_last = clock->read(clock);

        /* Do the ns -> cycle conversion first, using original mult */
        tmp = NTP_INTERVAL_LENGTH;
        tmp <<= clock->shift;
        tmp += clock->mult/2;
        do_div(tmp, clock->mult);
        if (tmp == 0)
                tmp = 1;

        interval = (cycle_t) tmp;
        timekeeper.cycle_interval = interval;

        /* Go back from cycles -> shifted ns */
        timekeeper.xtime_interval = (u64) interval * clock->mult;
        timekeeper.raw_interval =
                ((u64) interval * clock->mult) >> clock->shift;

        timekeeper.xtime_nsec = 0;
        timekeeper.shift = clock->shift;

        timekeeper.ntp_error = 0;
        timekeeper.ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;

        /*
         * The timekeeper keeps its own mult values for the currently
         * active clocksource. These value will be adjusted via NTP
         * to counteract clock drifting.
         */
        timekeeper.mult = clock->mult;
}



------------------------------------------------------------------------------------------------------------------


/*
 * Initialize TSC and delay the periodic timer init to
 * late x86_late_time_init() so ioremap works.
 */
void __init time_init(void)
{
        late_time_init = x86_late_time_init;
}

start_kernel:
        if (late_time_init)
             late_time_init();

+----------------------------------------------------------------------------------------+
|       /*                                                                               |
|        * The platform setup functions are preset with the default functions            |
|        * for standard PC hardware.                                                     |
|        */                                                                              |
|       struct x86_init_ops x86_init __initdata = {                                      |
|                                                                                        |
|               .resources = {                                                           |
|                       .probe_roms             = probe_roms,                            |
|                       .reserve_resources      = reserve_standard_io_resources,         |
|                       .memory_setup           = default_machine_specific_memory_setup, |
|               },                                                                       |
|                                                                                        |
|               .mpparse = {                                                             |
|                       .mpc_record             = x86_init_uint_noop,                    |
|                       .setup_ioapic_ids       = x86_init_noop,                         |
|                       .mpc_apic_id            = default_mpc_apic_id,                   |
|                       .smp_read_mpc_oem       = default_smp_read_mpc_oem,              |
|                       .mpc_oem_bus_info       = default_mpc_oem_bus_info,              |
|                       .find_smp_config        = default_find_smp_config,               |
|                       .get_smp_config         = default_get_smp_config,                |
|               },                                                                       |
|                                                                                        |
|               .irqs = {                                                                |
|                       .pre_vector_init        = init_ISA_irqs,                         |
|                       .intr_init              = native_init_IRQ,                       |
|                       .trap_init              = x86_init_noop,                         |
|               },                                                                       |
|                                                                                        |
|               .oem = {                                                                 |
|                       .arch_setup             = x86_init_noop,                         |
|                       .banner                 = default_banner,                        |
|               },                                                                       |
|                                                                                        |
|               .paging = {                                                              |
|                       .pagetable_setup_start  = native_pagetable_setup_start,          |
|                       .pagetable_setup_done   = native_pagetable_setup_done,           |
|               },                                                                       |
|                                                                                        |
|               .timers = {                                                              |
|                       .setup_percpu_clockev   = setup_boot_APIC_clock,                 |
|                       .tsc_pre_init           = x86_init_noop,                         |
|                       .timer_init             = hpet_time_init,                        |
|               },                                                                       |
|       };                                                                               |
|                                                                                        |
+----------------------------------------------------------------------------------------+


static __init void x86_late_time_init(void)
{
        x86_init.timers.timer_init();
        tsc_init();
}

/* Default timer init function */
void __init hpet_time_init(void)
{
        if (!hpet_enable())
                setup_pit_timer();
        setup_default_timer_irq();
}

/**
 * hpet_enable - Try to setup the HPET timer. Returns 1 on success.
 */
int __init hpet_enable(void)
{
        unsigned long id;
        int i;

        if (!is_hpet_capable())
                return 0;
                
        hpet_set_mapping();
	// Maybe hpet as a PCI device 获得线性地址

        /*
         * Read the period and check for a sane value:
         */     
        hpet_period = hpet_readl(HPET_PERIOD);
	// 周期
                
        /*      
         * AMD SB700 based systems with spread spectrum enabled use a
         * SMM based HPET emulation to provide proper frequency
         * setting. The SMM code is initialized with the first HPET
         * register access and takes some time to complete. During
         * this time the config register reads 0xffffffff. We check
         * for max. 1000 loops whether the config register reads a non
         * 0xffffffff value to make sure that HPET is up and running
         * before we go further. A counting loop is safe, as the HPET
         * access takes thousands of CPU cycles. On non SB700 based
         * machines this check is only done once and has no side
         * effects.
         */
        for (i = 0; hpet_readl(HPET_CFG) == 0xFFFFFFFF; i++) {
                if (i == 1000) {
                        printk(KERN_WARNING
                               "HPET config register value = 0xFFFFFFFF. "
                               "Disabling HPET\n");
                        goto out_nohpet;
                }
        }
        
        if (hpet_period < HPET_MIN_PERIOD || hpet_period > HPET_MAX_PERIOD)
                goto out_nohpet;

        /*
         * Read the HPET ID register to retrieve the IRQ routing
         * information and the number of channels
         */     
        id = hpet_readl(HPET_ID);
        hpet_print_config();

#ifdef CONFIG_HPET_EMULATE_RTC
        /*
         * The legacy routing mode needs at least two channels, tick timer
         * and the rtc emulation channel.
         */
        if (!(id & HPET_ID_NUMBER))
                goto out_nohpet;
#endif

        if (hpet_clocksource_register())
                goto out_nohpet;

        if (id & HPET_ID_LEGSUP) {
                hpet_legacy_clockevent_register();
                hpet_msi_capability_lookup(2);
                return 1;
        }
        hpet_msi_capability_lookup(0);
        return 0;

out_nohpet:
        hpet_clear_mapping();
        hpet_address = 0;
        return 0;
}

/* Max HPET Period is 10^8 femto sec as in HPET spec */
#define HPET_MAX_PERIOD         100000000UL
/*
 *  * Min HPET period is 10^5 femto sec just for safety. If it is less than this,
 *   * then 32 bit HPET counter wrapsaround in less than 0.5 sec.
 *    */
#define HPET_MIN_PERIOD         100000UL


static int hpet_clocksource_register(void)
{
        u64 start, now;
        cycle_t t1;

        /* Start the counter */
        hpet_restart_counter();

        /* Verify whether hpet counter works */
        t1 = hpet_readl(HPET_COUNTER);
        rdtscll(start);

        /*
         * We don't know the TSC frequency yet, but waiting for
         * 200000 TSC cycles is safe:
         * 4 GHz == 50us
         * 1 GHz == 200us
         */
        do {
                rep_nop();
                rdtscll(now);
        } while ((now - start) < 200000UL);

        if (t1 == hpet_readl(HPET_COUNTER)) {
                printk(KERN_WARNING
                       "HPET counter not counting. HPET disabled\n");
                return -ENODEV;
        }

        /*
         * The definition of mult is (include/linux/clocksource.h)
         * mult/2^shift = ns/cyc and hpet_period is in units of fsec/cyc
         * so we first need to convert hpet_period to ns/cyc units:
         *  mult/2^shift = ns/cyc = hpet_period/10^6
         *  mult = (hpet_period * 2^shift)/10^6
         *  mult = (hpet_period << shift)/FSEC_PER_NSEC
         */
        clocksource_hpet.mult = div_sc(hpet_period, FSEC_PER_NSEC, HPET_SHIFT);

        clocksource_register(&clocksource_hpet);

        return 0;
}

+------------------------------------------------------------------+
|                                                                  |
|       static struct clocksource clocksource_hpet = {             |
|               .name           = "hpet",                          |
|               .rating         = 250,                             |
|               .read           = read_hpet,                       |
|               .mask           = HPET_MASK,                       |
|               .shift          = HPET_SHIFT,                      |
|               .flags          = CLOCK_SOURCE_IS_CONTINUOUS,      |
|               .resume         = hpet_resume_counter,             |
|       #ifdef CONFIG_X86_64                                       |
|               .vread          = vread_hpet,                      |
|       #endif                                                     |
|       };                                                         |
|                                                                  |
+------------------------------------------------------------------+

/**
 * clocksource_register - Used to install new clocksources
 * @t:          clocksource to be registered
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 */     
int clocksource_register(struct clocksource *cs)
{       
        /* calculate max idle time permitted for this clocksource */
        cs->max_idle_ns = clocksource_max_deferment(cs);

        mutex_lock(&clocksource_mutex);
        clocksource_enqueue(cs);
        clocksource_select();
        clocksource_enqueue_watchdog(cs);
        mutex_unlock(&clocksource_mutex);
        return 0;
}       

/*
 * Enqueue the clocksource sorted by rating
 */
static void clocksource_enqueue(struct clocksource *cs)
{
        struct list_head *entry = &clocksource_list;
        // static LIST_HEAD(clocksource_list);
	// head prev 都指向自己

        struct clocksource *tmp;

        list_for_each_entry(tmp, &clocksource_list, list)
                /* Keep track of the place, where to insert */
                if (tmp->rating >= cs->rating)
                        entry = &tmp->list;
        list_add(&cs->list, entry);
	// 插入到双向循环链表中 pit --> hpet --> tsc --> head(clocksource_list)
}


//                           head
                     +-->>+---------+<-+
                     |    |         |  |
                     | +----.next   |  |
                     | |  |         |  |
                     | |  | .prev -----------+
                     | |  |         |  |     |
                     | |  +---------+  |     |
                     | |               |     |
//                   | |     TSC       |     |
                     | +->+---------+<---+   |
                     |    |         |  | |   |
                     | +----.next   |  | |   |
                     | |  |         |  | |   |
                     | |  | .prev -----+ |   |
                     | |  |         |    |   |
                     | |  +---------+    |   |
                     | |                 |   |
//                   | |     HPET        |   |
                     | +->+---------+<-----+ |
                     |    |         |    | | |
                     | +----.next   |    | | |
                     | |  |         |    | | |
                     | |  | .prev -------+ | |
                     | |  |         |      | |
                     | |  +---------+      | |
                     | |                   | |
//                   | |     PIT           | |
                     | +->+---------+<<------+
                     |    |         |      |
                     +------.next   |      |
                          |         |      |
                          | .prev ---------+
                          |         |
                          +---------+

/**
 * clocksource_select - Select the best clocksource available
 *
 * Private function. Must hold clocksource_mutex when called.
 *
 * Select the clocksource with the best rating, or the clocksource,
 * which is selected by userspace override.
 */
static void clocksource_select(void)
{
        struct clocksource *best, *cs;

        if (!finished_booting || list_empty(&clocksource_list))
                return;
        // 此时 finished_booting = 0,所以 closksource_select 函数直接返回。在系统初始化快结束时调用 clocksource_done_booting() 会设置finished_booting = 1,紧接着调用 clocksource_select() 进行真正的时钟源选择。
	
        /* First clocksource on the list has the best rating. */
        best = list_first_entry(&clocksource_list, struct clocksource, list);
        // 如上图中的链表中第一个就是最好的时钟源
	
        /* Check for the override clocksource. */
        list_for_each_entry(cs, &clocksource_list, list) {
                if (strcmp(cs->name, override_name) != 0)
                        continue;
                /*
                 * Check to make sure we don't switch to a non-highres
                 * capable clocksource if the tick code is in oneshot
                 * mode (highres or nohz)
                 */
                if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&
                    tick_oneshot_mode_active()) {
                        /* Override clocksource cannot be used. */
                        printk(KERN_WARNING "Override clocksource %s is not "
                               "HRT compatible. Cannot switch while in "
                               "HRT/NOHZ mode\n", cs->name);
                        override_name[0] = 0;
                } else
                        /* Override clocksource can be used. */
                        best = cs;
                break;
        }
        if (curr_clocksource != best) {
                printk(KERN_INFO "Switching to clocksource %s\n", best->name);
		// Switching to clocksource tsc log from dmesg . :-)

                curr_clocksource = best;
		// 保存当前使用的时钟源

                timekeeping_notify(curr_clocksource);
		// Install a new clocksource
        }
}


在此之前会注册几种时钟源
比如:
     core_initcall(init_jiffies_clocksource);
     fs_initcall(init_acpi_pm_clocksource);

 +-----------------------------------------+
 | fs_initcall(clocksource_done_booting);  |
 +-----------------------------------------+

#define fs_initcall(fn)                 __define_initcall("5",fn,5)

/*
 * clocksource_done_booting - Called near the end of core bootup
 *
 * Hack to avoid lots of clocksource churn at boot time.
 * We use fs_initcall because we want this to start before
 * device_initcall but after subsys_initcall.
 */
static int __init clocksource_done_booting(void)
{       
        finished_booting = 1;
	// 表示初始化完成
                        
        /*      
         * Run the watchdog first to eliminate unstable clock sources
         */
        clocksource_watchdog_kthread(NULL);

        mutex_lock(&clocksource_mutex);

        clocksource_select();
	// 选择最好的时钟源
	
        mutex_unlock(&clocksource_mutex);
        return 0;       
}                 


/**                     
 * timekeeping_notify - Install a new clock source
 * @clock:              pointer to the clock source
 *      
 * This function is called from clocksource.c after a new, better clock
 * source has been registered. The caller holds the clocksource_mutex.
 */     
void timekeeping_notify(struct clocksource *clock)
{                       
        if (timekeeper.clock == clock)
                return;
	// clock = &clocksource_tsc .
	// 此时 clock = &clocksource_jiffies .
	
        stop_machine(change_clocksource, clock, NULL);
        tick_clock_notify();*
}


/**     
 * Async notification about clocksource changes
 */     
void tick_clock_notify(void)
{
        int cpu;

        for_each_possible_cpu(cpu)
                set_bit(0, &per_cpu(tick_cpu_sched, cpu).check_clocks);
}


/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
        struct clocksource *new, *old;

        new = (struct clocksource *) data;
	// new = $clocksource_tsc .

        timekeeping_forward_now();
	// 更新时间

        if (!new->enable || new->enable(new) == 0) {
		// new->enable == NULL

                old = timekeeper.clock;
                timekeeper_setup_internals(new);
		// 设置新的时钟源 tsc --> timekeeper.

                if (old->disable)
                        old->disable(old);
        }
        return 0;
}


/**             
 * timekeeping_forward_now - update clock to the current time
 *              
 * Forward the current clock to update its state since the last call to
 * update_wall_time(). This is useful before significant clock changes,
 * as it avoids having to deal with this time offset explicitly.
 */
static void timekeeping_forward_now(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;
        s64 nsec;

        clock = timekeeper.clock;
	// clock = &clocksource_jiffies 
	
        cycle_now = clock->read(clock);
	// 返回 jiffies
	
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
	// 经过的时间(jiffies 数)

        clock->cycle_last = cycle_now;
        
        nsec = clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
	// 转换成 纳秒
	//  timekeeper.mult =  
	//  timekeeper.shift = 8 
        
        /* If arch requires, add in gettimeoffset() */
        nsec += arch_gettimeoffset();
	// 空函数

        timespec_add_ns(&xtime, nsec);
	// 加到 xtime 中去 

        nsec = clocksource_cyc2ns(cycle_delta, clock->mult, clock->shift);
        timespec_add_ns(&raw_time, nsec);
}


fs_initcall(hpet_late_init);

/*
 * Needs to be late, as the reserve_timer code calls kalloc !
 *
 * Not a problem on i386 as hpet_enable is called from late_time_init,
 * but on x86_64 it is necessary !
 */
static __init int hpet_late_init(void)
{
        int cpu;

        if (boot_hpet_disable)
                return -ENODEV;

        if (!hpet_address) {
                if (!force_hpet_address)
                        return -ENODEV;

                hpet_address = force_hpet_address;
                hpet_enable();
        }

        if (!hpet_virt_address)
                return -ENODEV;

        hpet_reserve_platform_timers(hpet_readl(HPET_ID));
        hpet_print_config();

        if (hpet_msi_disable)
                return 0;

        for_each_online_cpu(cpu) {
                hpet_cpuhp_notify(NULL, CPU_ONLINE, (void *)(long)cpu);
        }

        /* This notifier should be called after workqueue is ready */
        hotcpu_notifier(hpet_cpuhp_notify, -20);

        return 0;
}


static void hpet_legacy_clockevent_register(void)
{
        /* Start HPET legacy interrupts */
        hpet_enable_legacy_int();

        /*
         * The mult factor is defined as (include/linux/clockchips.h)
         *  mult/2^shift = cyc/ns (in contrast to ns/cyc in clocksource.h)
         * hpet_period is in units of femtoseconds (per cycle), so
         *  mult/2^shift = cyc/ns = 10^6/hpet_period
         *  mult = (10^6 * 2^shift)/hpet_period
         *  mult = (FSEC_PER_NSEC << hpet_clockevent.shift)/hpet_period
         */
        hpet_clockevent.mult = div_sc((unsigned long) FSEC_PER_NSEC,
                                      hpet_period, hpet_clockevent.shift);
        /* Calculate the min / max delta */
        hpet_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
                                                           &hpet_clockevent);
        /* 5 usec minimum reprogramming delta. */
        hpet_clockevent.min_delta_ns = 5000;

        /*
         * Start hpet with the boot cpu mask and make it
         * global after the IO_APIC has been initialized.
         */
        hpet_clockevent.cpumask = cpumask_of(smp_processor_id());
        clockevents_register_device(&hpet_clockevent);
        global_clock_event = &hpet_clockevent;
	// Global clock event
        printk(KERN_DEBUG "hpet clockevent registered\n");
}

#define FSEC_PER_NSEC                   1000000L   // 飞秒   1 纳秒 = 1000000 飞秒


+------------------------------------------------------------------------------------+
|                                                                                    |
|       /*                                                                           |
|        * The hpet clock event device                                               |
|        */                                                                          |
|       static struct clock_event_device hpet_clockevent = {                         |
|       	.name           = "hpet",                                            |
|       	.features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,  |
|       	.set_mode       = hpet_legacy_set_mode,                              |
|       	.set_next_event = hpet_legacy_next_event,                            |
|       	.shift          = 32,                                                |
|       	.irq            = 0,                                                 |
|       	.rating         = 50,                                                |
|       };                                                                           |
+------------------------------------------------------------------------------------+

 +----------------------------------------------------------------------------------------------+
 |                                                                                              |
 |      /**                                                                                     |
 |       * struct clock_event_device - clock event device descriptor                            |
 |       * @name:               ptr to clock event name                                         |
 |       * @features:           features                                                        |
 |       * @max_delta_ns:       maximum delta value in ns                                       |
 |       * @min_delta_ns:       minimum delta value in ns                                       |
 |       * @mult:               nanosecond to cycles multiplier                                 |
 |       * @shift:              nanoseconds to cycles divisor (power of two)                    |
 |       * @rating:             variable to rate clock event devices                            |
 |       * @irq:                IRQ number (only for non CPU local devices)                     |
 |       * @cpumask:            cpumask to indicate for which CPUs this device works            |
 |       * @set_next_event:     set next event function                                         |
 |       * @set_mode:           set mode function                                               |
 |       * @event_handler:      Assigned by the framework to be called by the low               |
 |       *                      level handler of the event source                               |
 |       * @broadcast:          function to broadcast events                                    |
 |       * @list:               list head for the management code                               |
 |       * @mode:               operating mode assigned by the management code                  |
 |       * @next_event:         local storage for the next event in oneshot mode                |
 |       */                                                                                     |
 |      struct clock_event_device {                                                             |
 |      	const char              *name;                                                  |
 |      	unsigned int            features;                                               |
 |      	unsigned long           max_delta_ns;                                           |
 |      	unsigned long           min_delta_ns;                                           |
 |      	unsigned long           mult;                                                   |
 |      	int                     shift;                                                  |
 |      	int                     rating;                                                 |
 |      	int                     irq;                                                    |
 |      	const struct cpumask    *cpumask;                                               |
 |      	int                     (*set_next_event)(unsigned long evt,                    |
 |      						  struct clock_event_device *);         |
 |      	void                    (*set_mode)(enum clock_event_mode mode,                 |
 |      					    struct clock_event_device *);               |
 |      	void                    (*event_handler)(struct clock_event_device *);          |
 |      	void                    (*broadcast)(const struct cpumask *mask);               |
 |      	struct list_head        list;                                                   |
 |      	enum clock_event_mode   mode;                                                   |
 |      	ktime_t                 next_event;                                             |
 |      };                                                                                      |
 |                                                                                              |
 +----------------------------------------------------------------------------------------------+

typedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;


#define DECLARE_BITMAP(name,bits) \
	unsigned long name[BITS_TO_LONGS(bits)]

#define BITS_TO_LONGS(nr)	DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))

#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))

/*
 * Calculate a multiplication factor for scaled math, which is used to convert
 * nanoseconds based values to clock ticks:
 *
 * clock_ticks = (nanoseconds * factor) >> shift.
 *
 * div_sc is the rearranged equation to calculate a factor from a given clock
 * ticks / nanoseconds ratio:
 *
 * factor = (clock_ticks << shift) / nanoseconds
 */
static inline unsigned long div_sc(unsigned long ticks, unsigned long nsec,
                                   int shift)
{
        // ticks = 1000000L
        // nsec = hpet_period
        // shift = 32
        uint64_t tmp = ((uint64_t)ticks) << shift;

        do_div(tmp, nsec);
        return (unsigned long) tmp;
}


#define cpumask_of(cpu) (get_cpu_mask(cpu))

static inline const struct cpumask *get_cpu_mask(unsigned int cpu)
{
        const unsigned long *p = cpu_bit_bitmap[1 + cpu % BITS_PER_LONG];
        p -= cpu / BITS_PER_LONG;
        return to_cpumask(p);
}

const unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {

        MASK_DECLARE_8(0),      MASK_DECLARE_8(8),
        MASK_DECLARE_8(16),     MASK_DECLARE_8(24),
#if BITS_PER_LONG > 32
        MASK_DECLARE_8(32),     MASK_DECLARE_8(40),
        MASK_DECLARE_8(48),     MASK_DECLARE_8(56),
#endif
};

#define MASK_DECLARE_1(x)       [x+1][0] = 1UL << (x)
#define MASK_DECLARE_2(x)       MASK_DECLARE_1(x), MASK_DECLARE_1(x+1)
#define MASK_DECLARE_4(x)       MASK_DECLARE_2(x), MASK_DECLARE_2(x+2)
#define MASK_DECLARE_8(x)       MASK_DECLARE_4(x), MASK_DECLARE_4(x+4)


/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */
#define to_cpumask(bitmap)                                              \
        ((struct cpumask *)(1 ? (bitmap)                                \
                            : (void *)sizeof(__check_is_bitmap(bitmap))))



/*              
 * Initialize the conversion factor and the min/max deltas of the clock event
 * structure and register the clock event source with the framework.
 */             
void __init setup_pit_timer(void)
{               
        /*
         * Start pit with the boot cpu mask and make it global after the
         * IO_APIC has been initialized.
         */
        pit_ce.cpumask = cpumask_of(smp_processor_id());
        pit_ce.mult = div_sc(CLOCK_TICK_RATE, NSEC_PER_SEC, pit_ce.shift);
        pit_ce.max_delta_ns = clockevent_delta2ns(0x7FFF, &pit_ce);
        pit_ce.min_delta_ns = clockevent_delta2ns(0xF, &pit_ce);
                
        clockevents_register_device(&pit_ce);
        global_clock_event = &pit_ce;
}       

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       /*                                                                                |
|        * On UP the PIT can serve all of the possible timer functions. On SMP systems    |
|        * it can be solely used for the global tick.                                     |
|        *                                                                                |
|        * The profiling and update capabilities are switched off once the local apic is  |
|        * registered. This mechanism replaces the previous #ifdef LOCAL_APIC -           |
|        * !using_apic_timer decisions in do_timer_interrupt_hook()                       |
|        */                                                                               |
|       static struct clock_event_device pit_ce = {                                       |
|               .name           = "pit",                                                  |
|               .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,       |
|               .set_mode       = init_pit_timer,                                         |
|               .set_next_event = pit_next_event,                                         |
|               .shift          = 32,                                                     |
|               .irq            = 0,                                                      |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+

void __init setup_default_timer_irq(void)
{       
        setup_irq(0, &irq0);
}

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       static struct irqaction irq0  = {                                                 |
|               .handler = timer_interrupt,                                               |
|               .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,    |
|               .name = "timer"                                                           |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+


void __init tsc_init(void)
{
        u64 lpj;
        int cpu;
                
        x86_init.timers.tsc_pre_init();
                
        if (!cpu_has_tsc)
                return;
        
        tsc_khz = x86_platform.calibrate_tsc();
        // invoke native_calibrate_tsc(); calibrate the tsc on boot


        cpu_khz = tsc_khz;
        
        if (!tsc_khz) {
                mark_tsc_unstable("could not calculate TSC khz");
                return;
        }

        printk("Detected %lu.%03lu MHz processor.\n",
                        (unsigned long)cpu_khz / 1000,
                        (unsigned long)cpu_khz % 1000);
        
        /*
         * Secondary CPUs do not run through tsc_init(), so set up
         * all the scale factors for all CPUs, assuming the same
         * speed as the bootup CPU. (cpufreq notifiers will fix this
         * up if their speed diverges)
         */
        for_each_possible_cpu(cpu)
                set_cyc2ns_scale(cpu_khz, cpu);

        if (tsc_disabled > 0)
                return;
        
        /* now allow native_sched_clock() to use rdtsc */
        tsc_disabled = 0;

        lpj = ((u64)tsc_khz * 1000);
        do_div(lpj, HZ);
        lpj_fine = lpj;
        
        use_tsc_delay();
        /* Check and install the TSC clocksource */
        dmi_check_system(bad_tsc_dmi_table);

        if (unsynchronized_tsc())
                mark_tsc_unstable("TSCs unsynchronized");

        check_system_tsc_reliable();
        init_tsc_clocksource();
}       

struct x86_platform_ops x86_platform = {
        .calibrate_tsc                  = native_calibrate_tsc,
        .get_wallclock                  = mach_get_cmos_time,
        .set_wallclock                  = mach_set_rtc_mmss,
        .is_untracked_pat_range         = is_ISA_range,
        .nmi_init                       = default_nmi_init
};


static void __init init_tsc_clocksource(void)
{
        clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
                        clocksource_tsc.shift);
        if (tsc_clocksource_reliable)
                clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
        /* lower the rating if we already know its unstable: */
        if (check_tsc_unstable()) {
                clocksource_tsc.rating = 0;
                clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
        }
        clocksource_register(&clocksource_tsc);
}

+-------------------------------------------------------------------------+
|                                                                         |
|                                                                         |
|      static struct clocksource clocksource_tsc = {                      |
|               .name                   = "tsc",                          |
|               .rating                 = 300,                            |
|               .read                   = read_tsc,                       |
|               .resume                 = resume_tsc,                     |
|               .mask                   = CLOCKSOURCE_MASK(64),           |
|               .shift                  = 22,                             |
|               .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |    |
|                                         CLOCK_SOURCE_MUST_VERIFY,       |
|       #ifdef CONFIG_X86_64                                              |
|               .vread                  = vread_tsc,                      |
|       #endif                                                            |
|       };                                                                |
|                                                                         |
+-------------------------------------------------------------------------+







   +-------------------------------------------------------------------------+
   |                                                                         |
   |    # cat /proc/timer_list | more                                        |
   |                                                                         |
   |    Tick Device: mode:     1                      --> one-shot 单次      |
   |    Broadcast device                                                     |
   |    Clock Event Device: hpet                                             |
   |     max_delta_ns:   149983013276                                        |
   |     min_delta_ns:   13409                                               |
   |     mult:           61496111                                            |
   |     shift:          32                                                  |
   |     mode:           3                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: hpet_legacy_next_event                              |
   |     set_mode:       hpet_legacy_set_mode                                |
   |     event_handler:  tick_handle_oneshot_broadcast                       |
   |     retries:        1459862                                             |
   |    tick_broadcast_mask: 00000001                                        |
   |    tick_broadcast_oneshot_mask: 00000002                                |
   |                                                                         |
   |                                                                         |
   |    Tick Device: mode:     1                                             |
   |    Per CPU device: 0                                                    |
   |    Clock Event Device: lapic                                            |
   |     max_delta_ns:   128824925406                                        |
   |     min_delta_ns:   1000                                                |
   |     mult:           71596176                                            |
   |     shift:          32                                                  |
   |     mode:           3                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: lapic_next_event                                    |
   |     set_mode:       lapic_timer_setup                                   |
   |     event_handler:  hrtimer_interrupt                                   |
   |     retries:        891537                                              |
   |                                                                         |
   |    Tick Device: mode:     1                                             |
   |    Per CPU device: 1                                                    |
   |    Clock Event Device: lapic                                            |
   |     max_delta_ns:   128824925406                                        |
   |     min_delta_ns:   1000                                                |
   |     mult:           71596176                                            |
   |     shift:          32                                                  |
   |     mode:           1                                                   |
   |     next_event:     21524167000000 nsecs                                |
   |     set_next_event: lapic_next_event                                    |
   |     set_mode:       lapic_timer_setup                                   |
   |     event_handler:  hrtimer_interrupt                                   |
   |     retries:        539190                                              |
   +-------------------------------------------------------------------------+

clock event device 有三种 PIT HPET APIC
                          PIT/HPET Global clock event device
			  APIC     Local clock event device
 
setup_APIC_timer()
        |
        +----> clockevents_register_device(levt);


 +----------------------------------------------------------------------------------+
 |      /*                                                                          |
 |       * The local apic timer can be used for any function which is CPU local.    |
 |       */                                                                         |
 |      static struct clock_event_device lapic_clockevent = {                       |
 |              .name           = "lapic",                                          |
 |              .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT  |
 |                              | CLOCK_EVT_FEAT_C3STOP | CLOCK_EVT_FEAT_DUMMY,     |
 |              .shift          = 32,                                               |
 |              .set_mode       = lapic_timer_setup,                                |
 |              .set_next_event = lapic_next_event,                                 |
 |              .broadcast      = lapic_timer_broadcast,                            |
 |              .rating         = 100,                                              |
 |              .irq            = -1,                                               |
 |      };                                                                          |
 |                                                                                  |
 +----------------------------------------------------------------------------------+
 

/**
 * clockevents_register_device - register a clock event device
 * @dev:        device to register
 */
void clockevents_register_device(struct clock_event_device *dev)
{
        // dev=&hpet_clockevent

        unsigned long flags;

        BUG_ON(dev->mode != CLOCK_EVT_MODE_UNUSED);
        BUG_ON(!dev->cpumask);

        spin_lock_irqsave(&clockevents_lock, flags);

        list_add(&dev->list, &clockevent_devices);
	// 加入到 clockevent_devices 链表中
        clockevents_do_notify(CLOCK_EVT_NOTIFY_ADD, dev);
        clockevents_notify_released();

        spin_unlock_irqrestore(&clockevents_lock, flags);
}

static LIST_HEAD(clockevent_devices);


/* Clock event notification values */
enum clock_event_nofitiers {
        CLOCK_EVT_NOTIFY_ADD,
        CLOCK_EVT_NOTIFY_BROADCAST_ON,
        CLOCK_EVT_NOTIFY_BROADCAST_OFF,
        CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
        CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
        CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
        CLOCK_EVT_NOTIFY_SUSPEND,
        CLOCK_EVT_NOTIFY_RESUME,
        CLOCK_EVT_NOTIFY_CPU_DYING,
        CLOCK_EVT_NOTIFY_CPU_DEAD,
};

/*
 * Notify about a clock event change. Called with clockevents_lock
 * held.
 */
static void clockevents_do_notify(unsigned long reason, void *dev)
{
	// dev 为时钟事件设备 (hpet) 
        raw_notifier_call_chain(&clockevents_chain, reason, dev);
}

// clockevents_chain 已经在 tick_init 函数中初始化
// clockevents_chain->head = &tick_notifier

struct raw_notifier_head {
	        struct notifier_block *head;
};

int raw_notifier_call_chain(struct raw_notifier_head *nh,
		                unsigned long val, void *v)
{
	        return __raw_notifier_call_chain(nh, val, v, -1, NULL);
}



/**     
 *      __raw_notifier_call_chain - Call functions in a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @val: Value passed unmodified to notifier function
 *      @v: Pointer passed unmodified to notifier function
 *      @nr_to_call: See comment for notifier_call_chain.
 *      @nr_calls: See comment for notifier_call_chain
 *
 *      Calls each function in a notifier chain in turn.  The functions
 *      run in an undefined context.
 *      All locking must be provided by the caller.
 *
 *      If the return value of the notifier can be and'ed
 *      with %NOTIFY_STOP_MASK then raw_notifier_call_chain()
 *      will return immediately, with the return value of
 *      the notifier function which halted execution.
 *      Otherwise the return value is the return value
 *      of the last notifier function called.
 */     
int __raw_notifier_call_chain(struct raw_notifier_head *nh,
                              unsigned long val, void *v,
                              int nr_to_call, int *nr_calls)
{
        return notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
}


/**
 * notifier_call_chain - Informs the registered notifiers about an event.
 *      @nl:            Pointer to head of the blocking notifier chain
 *      @val:           Value passed unmodified to notifier function
 *      @v:             Pointer passed unmodified to notifier function
 *      @nr_to_call:    Number of notifier functions to be called. Don't care
 *                      value of this parameter is -1.
 *      @nr_calls:      Records the number of notifications sent. Don't care
 *                      value of this field is NULL.
 *      @returns:       notifier_call_chain returns the value returned by the
 *                      last notifier function called.
 */
static int __kprobes notifier_call_chain(struct notifier_block **nl,
                                        unsigned long val, void *v,
                                        int nr_to_call, int *nr_calls)
{
        int ret = NOTIFY_DONE;
        struct notifier_block *nb, *next_nb;

        nb = rcu_dereference(*nl);
        // nb 指向 notifier_block

        while (nb && nr_to_call) {
                // nr_to_call = -1

                next_nb = rcu_dereference(nb->next);

#ifdef CONFIG_DEBUG_NOTIFIERS
                if (unlikely(!func_ptr_is_kernel_text(nb->notifier_call))) {
                        WARN(1, "Invalid notifier called!");
                        nb = next_nb;
                        continue;
                }
#endif
                ret = nb->notifier_call(nb, val, v);
                // 调用 tick_notify 函数 val = CLOCK_EVT_NOTIFY_ADD 

                if (nr_calls)
                        (*nr_calls)++;

                if ((ret & NOTIFY_STOP_MASK) == NOTIFY_STOP_MASK)
                        break;
                nb = next_nb;
                nr_to_call--;
        }
        return ret;
}

/*
 * Notification about clock event devices
 */
static int tick_notify(struct notifier_block *nb, unsigned long reason,
                               void *dev)
{
        switch (reason) {

        case CLOCK_EVT_NOTIFY_ADD:
                return tick_check_new_device(dev);
		// 这里

        case CLOCK_EVT_NOTIFY_BROADCAST_ON:
        case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
        case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
                tick_broadcast_on_off(reason, dev);
                break;

        case CLOCK_EVT_NOTIFY_BROADCAST_ENTER:
        case CLOCK_EVT_NOTIFY_BROADCAST_EXIT:
                tick_broadcast_oneshot_control(reason);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DYING:
                tick_handover_do_timer(dev);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DEAD:
                tick_shutdown_broadcast_oneshot(dev);
                tick_shutdown_broadcast(dev);
                tick_shutdown(dev);
                break;

        case CLOCK_EVT_NOTIFY_SUSPEND:
                tick_suspend();
                tick_suspend_broadcast();
                break;

        case CLOCK_EVT_NOTIFY_RESUME:
                tick_resume();
                break;

        default:
                break;
        }

        return NOTIFY_OK;
}

/*      
 * Check, if the new registered device should be used.
 */             
static int tick_check_new_device(struct clock_event_device *newdev)
{       
        // newdev = &hpet_clockevent;

        struct clock_event_device *curdev;
        struct tick_device *td;
        int cpu, ret = NOTIFY_OK;
        unsigned long flags;
        
        spin_lock_irqsave(&tick_device_lock, flags);
                
        cpu = smp_processor_id();
        if (!cpumask_test_cpu(cpu, newdev->cpumask))
                goto out_bc;
	// cpumask:            cpumask to indicate for which CPUs this device works
                
        td = &per_cpu(tick_cpu_device, cpu);
        // tick_cpu_device 是一个各CPU链表，包含了系统中每个CPU对应的struct tick_device 实例.

        curdev = td->evtdev;
        curdev = NULL;

        /* cpu local device ? */
        if (!cpumask_equal(newdev->cpumask, cpumask_of(cpu))) {

                /*
                 * If the cpu affinity of the device interrupt can not
                 * be set, ignore it.
                 */
                if (!irq_can_set_affinity(newdev->irq))
                        goto out_bc;

                /*
                 * If we have a cpu local device already, do not replace it
                 * by a non cpu local device
                 */
                if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
                        goto out_bc;
        }

        /*
         * If we have an active device, then check the rating and the oneshot
         * feature.
         */
        if (curdev) {
                /*
                 * Prefer one shot capable devices !
                 */
                if ((curdev->features & CLOCK_EVT_FEAT_ONESHOT) &&
                    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
                        goto out_bc;
                /*
                 * Check the rating
                 */
                if (curdev->rating >= newdev->rating)
                        goto out_bc;
        }

        /*
         * Replace the eventually existing device by the new
         * device. If the current device is the broadcast device, do
         * not give it back to the clockevents layer !
         */
        if (tick_is_broadcast_device(curdev)) {
                clockevents_shutdown(curdev);
                curdev = NULL;
        }
        clockevents_exchange_device(curdev, newdev);
        tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
        if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
                tick_oneshot_notify();
	        // here

        spin_unlock_irqrestore(&tick_device_lock, flags);
        return NOTIFY_STOP;

out_bc:
        /*
         * Can the new device be used as a broadcast device ?
         */
        if (tick_check_broadcast_device(newdev))
                ret = NOTIFY_STOP;

        spin_unlock_irqrestore(&tick_device_lock, flags);

        return ret;
}

/**
 *  * cpumask_test_cpu - test for a cpu in a cpumask
 *   * @cpu: cpu number (< nr_cpu_ids)
 *    * @cpumask: the cpumask pointer
 *     *
 *      * No static inline type checking - see Subtlety (1) above.
 *       */
#define cpumask_test_cpu(cpu, cpumask) \
	        test_bit(cpumask_check(cpu), cpumask_bits((cpumask)))


/**
 * clockevents_exchange_device - release and request clock devices
 * @old:        device to release (can be NULL)
 * @new:        device to request (can be NULL)
 *
 * Called from the notifier chain. clockevents_lock is held already
 */
void clockevents_exchange_device(struct clock_event_device *old,
                                 struct clock_event_device *new)
{
        old = NULL
        new = &hpet_clockevent 

        unsigned long flags;

        local_irq_save(flags);
        /*
         * Caller releases a clock event device. We queue it into the
         * released list and do a notify add later.
         */
        if (old) {
                clockevents_set_mode(old, CLOCK_EVT_MODE_UNUSED);
                list_del(&old->list);
                list_add(&old->list, &clockevents_released);
        }

        if (new) {
                BUG_ON(new->mode != CLOCK_EVT_MODE_UNUSED);
                clockevents_shutdown(new);
        }
        local_irq_restore(flags);
}

/**
 * clockevents_shutdown - shutdown the device and clear next_event
 * @dev:        device to shutdown
 */
void clockevents_shutdown(struct clock_event_device *dev)
{
        clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
        dev->next_event.tv64 = KTIME_MAX;
}


/*
 * Setup the tick device
 */
static void tick_setup_device(struct tick_device *td,
                              struct clock_event_device *newdev, int cpu,
                              const struct cpumask *cpumask)
{
        ktime_t next_event;
        void (*handler)(struct clock_event_device *) = NULL;

        /*
         * First device setup ?
         */
        if (!td->evtdev) {
        // 此时钟设备没有相关的时钟事件设备
                /*
                 * If no cpu took the do_timer update, assign it to
                 * this cpu:
                 */
                if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
                        // 如果没有选定时钟设备来承担全局时钟设备的角色，那么将选择当前设备来承担此职责

                        tick_do_timer_cpu = cpu;
                        // 设置为当前设备所属处理器编号
                        tick_next_period = ktime_get();

                        tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
                        // 时钟周期，纳秒
                        HZ = 1000
                }

                /*
                 * Startup in periodic mode first.
                 */
                td->mode = TICKDEV_MODE_PERIODIC;
                // 设备运行模式 --> 周期模式

        } else {
                handler = td->evtdev->event_handler;
                next_event = td->evtdev->next_event;
                td->evtdev->event_handler = clockevents_handle_noop;
        }

        td->evtdev = newdev;
        //为时钟设备指定事件设备

        /*
         * When the device is not per cpu, pin the interrupt to the
         * current cpu:
         */
        if (!cpumask_equal(newdev->cpumask, cpumask))
                irq_set_affinity(newdev->irq, cpumask);

        /*
         * When global broadcasting is active, check if the current
         * device is registered as a placeholder for broadcast mode.
         * This allows us to handle this x86 misfeature in a generic
         * way.
         */
        if (tick_device_uses_broadcast(newdev, cpu))
                return;

        if (td->mode == TICKDEV_MODE_PERIODIC)
                tick_setup_periodic(newdev, 0);
	        // invoke this ......
        else
                tick_setup_oneshot(newdev, handler, next_event);
}

#define NSEC_PER_SEC    1000000000L

/*
 * Check, if the device is disfunctional and a place holder, which
 * needs to be handled by the broadcast device.
 */
int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
{               
        unsigned long flags;
        int ret = 0; 
                
        spin_lock_irqsave(&tick_broadcast_lock, flags);
                
        /*
         * Devices might be registered with both periodic and oneshot
         * mode disabled. This signals, that the device needs to be
         * operated from the broadcast device and is a placeholder for
         * the cpu local device.
         */
        if (!tick_device_is_functional(dev)) {
                dev->event_handler = tick_handle_periodic;
                cpumask_set_cpu(cpu, tick_get_broadcast_mask());
                tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
                ret = 1;
        } else {                             
                /*
                 * When the new device is not affected by the stop
                 * feature and the cpu is marked in the broadcast mask
                 * then clear the broadcast bit.
                 */
                if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
                        int cpu = smp_processor_id();
        
                        cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
                        tick_broadcast_clear_oneshot(cpu);
                }
        }
        spin_unlock_irqrestore(&tick_broadcast_lock, flags);
        return ret;
}

/*
 * Setup the device for a periodic tick
 */
void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
        tick_set_periodic_handler(dev, broadcast);
	// broadcast = 0

        /* Broadcast setup ? */
        if (!tick_device_is_functional(dev))
                return;

        if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
            !tick_broadcast_oneshot_active()) {
                clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
		// here
        } else {
                unsigned long seq;
                ktime_t next;

                do {
                        seq = read_seqbegin(&xtime_lock);
                        next = tick_next_period;
                } while (read_seqretry(&xtime_lock, seq));

                clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);

                for (;;) {
                        if (!clockevents_program_event(dev, next, ktime_get()))
                                return;
                        next = ktime_add(next, tick_period);
                }
        }
}

/*
 * Set the periodic handler depending on broadcast on/off
 */
void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
{
        if (!broadcast)
                dev->event_handler = tick_handle_periodic;
	        // here
        else
                dev->event_handler = tick_handle_periodic_broadcast;
}               


run_timer_softirq()
   --> hrtimer_run_pending()
	  --> hrtimer_switch_to_hres()
	        --> tick_init_highres()
	              --> tick_switch_to_oneshot()
	                    --> tick_broadcast_switch_to_oneshot()
	                          --> tick_broadcast_setup_oneshot()

/**
 * tick_init_highres - switch to high resolution mode
 *
 * Called with interrupts disabled.
 */
int tick_init_highres(void)
{
        return tick_switch_to_oneshot(hrtimer_interrupt);
}

/**
 * tick_switch_to_oneshot - switch to oneshot mode
 */
int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
        struct tick_device *td = &__get_cpu_var(tick_cpu_device);
        struct clock_event_device *dev = td->evtdev;

        if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT) ||
                    !tick_device_is_functional(dev)) {

                printk(KERN_INFO "Clockevents: "
                       "could not switch to one-shot mode:");
                if (!dev) {
                        printk(" no tick device\n");
                } else {
                        if (!tick_device_is_functional(dev))
                                printk(" %s is not functional.\n", dev->name);
                        else 
                                printk(" %s does not support one-shot mode.\n",
                                       dev->name);
                }    
                return -EINVAL;
        }    

        td->mode = TICKDEV_MODE_ONESHOT;
        dev->event_handler = handler;

        clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
        tick_broadcast_switch_to_oneshot();
        return 0;
}


/**
 * tick_broadcast_setup_oneshot - setup the broadcast device
 */
void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
{
        /* Set it up only once ! */
        if (bc->event_handler != tick_handle_oneshot_broadcast) {
                int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
                int cpu = smp_processor_id();

                bc->event_handler = tick_handle_oneshot_broadcast;
                clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);

                /* Take the do_timer update */
                tick_do_timer_cpu = cpu;

                /*
                 * We must be careful here. There might be other CPUs
                 * waiting for periodic broadcast. We need to set the
                 * oneshot_mask bits for those and program the
                 * broadcast device to fire.
                 */
                cpumask_copy(to_cpumask(tmpmask), tick_get_broadcast_mask());
                cpumask_clear_cpu(cpu, to_cpumask(tmpmask));
                cpumask_or(tick_get_broadcast_oneshot_mask(),
                           tick_get_broadcast_oneshot_mask(),
                           to_cpumask(tmpmask));

                if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {
                        tick_broadcast_init_next_event(to_cpumask(tmpmask),
                                                       tick_next_period);
                        tick_broadcast_set_event(tick_next_period, 1);
                } else
                        bc->next_event.tv64 = KTIME_MAX;
        }
}



timer_interrupt() 时钟中断处理程序
	--> global_clock_event->event_handler(global_clock_event); --> tick_handle_oneshot_broadcast

smp_apic_timer_interrupt()
	--> local_apic_timer_interrupt()
	            evt->event_handler(evt);  --> hrtimer_interrupt() 



---------------------------------------------------------------------------------------------------------------------------------------------------------
start_secondary()
{
	......

	x86_cpuinit.setup_percpu_clockev();
	// invoke setup_secondary_APIC_clock 

	.......
}
  
 +--------------------------------------------------------------------------------------+
 |      struct x86_cpuinit_ops x86_cpuinit __cpuinitdata = {                            |
 |      		.setup_percpu_clockev           = setup_secondary_APIC_clock,   |
 |      };                                                                              |
 |                                                                                      |
 +--------------------------------------------------------------------------------------+

void __cpuinit setup_secondary_APIC_clock(void)
{
	        setup_APIC_timer();
		// 注册时钟事件设备
}

[Switching to Thread 3]
do_boot_cpu()
   --> wakeup_secondary_cpu_via_init()
         --> startup_ipi_hook()

[New Thread 4]
[Switching to Thread 4]


[进程 1]
kernel_init()
  --> smp_init()
        --> cpu_up()
	      ...
	      --> __cpu_up() 
                        smp_ops.cpu_up(cpu)
	            --> native_cpu_up()  
	                  --> do_boot_cpu() 
	                        --> wakeup_secondary_cpu_via_init()
				      --> startup_ipi_hook()
	                                  ......
                                          maybe 
					  start_secondary()
	                                          --> setup_secondary_APIC_clock()
	                                                --> setup_APIC_timer()


static inline void startup_ipi_hook(int phys_apicid, unsigned long start_eip,
                                    unsigned long start_esp)
{
        PVOP_VCALL3(pv_apic_ops.startup_ipi_hook,
                    phys_apicid, start_eip, start_esp);
}


#define PVOP_VCALL3(op, arg1, arg2, arg3)				\
	__PVOP_VCALL(op, "", "", PVOP_CALL_ARG1(arg1),			\
		     PVOP_CALL_ARG2(arg2), PVOP_CALL_ARG3(arg3))
		    

#define PVOP_CALL_ARG1(x)		"D" ((unsigned long)(x))
#define PVOP_CALL_ARG2(x)		"S" ((unsigned long)(x))
#define PVOP_CALL_ARG3(x)		"d" ((unsigned long)(x))


#define __PVOP_VCALL(op, pre, post, ...)				\
	____PVOP_VCALL(op, CLBR_ANY, PVOP_VCALL_CLOBBERS,		\
		       VEXTRA_CLOBBERS,					\
		       pre, post, ##__VA_ARGS__)


#define CLBR_ANY  ((1 << 9) - 1)

#define PVOP_VCALL_CLOBBERS	"=D" (__edi),				\
				"=S" (__esi), "=d" (__edx),		\
				"=c" (__ecx)

#define VEXTRA_CLOBBERS	 , "rax", "r8", "r9", "r10", "r11"


#define ____PVOP_VCALL(op, clbr, call_clbr, extra_clbr, pre, post, ...) \
        ({                                                              \
                PVOP_VCALL_ARGS;                                        \
                PVOP_TEST_NULL(op);                                     \
                asm volatile(pre                                        \
                             paravirt_alt(PARAVIRT_CALL)                \
                             post                                       \
                             : call_clbr                                \
                             : paravirt_type(op),                       \
                               paravirt_clobber(clbr),                  \
                               ##__VA_ARGS__                            \
                             : "memory", "cc" extra_clbr);              \
        })


#define PVOP_VCALL_ARGS					\
	unsigned long __edi = __edi, __esi = __esi,	\
		__edx = __edx, __ecx = __ecx, __eax = __eax



  +-----------------------------------------------------------------------------+
  |                                                                             |
  |     struct smp_ops smp_ops = {                                              |
  |     	.smp_prepare_boot_cpu   = native_smp_prepare_boot_cpu,          |
  |     	.smp_prepare_cpus       = native_smp_prepare_cpus,              |
  |     	.smp_cpus_done          = native_smp_cpus_done,                 |
  |                                                                             |
  |     	.stop_other_cpus        = native_stop_other_cpus,               |
  |     	.smp_send_reschedule    = native_smp_send_reschedule,           |
  |                                                                             |
  |     	.cpu_up                 = native_cpu_up,                        |
  |     	.cpu_die                = native_cpu_die,                       |
  |     	.cpu_disable            = native_cpu_disable,                   |
  |     	.play_dead              = native_play_dead,                     |
  |                                                                             |
  |     	.send_call_func_ipi     = native_send_call_func_ipi,            |
  |     	.send_call_func_single_ipi = native_send_call_func_single_ipi,  |
  |     };                                                                      |
  |                                                                             |
  +-----------------------------------------------------------------------------+


static inline void smp_prepare_cpus(unsigned int max_cpus)
{
	        smp_ops.smp_prepare_cpus(max_cpus);
		// native_smp_prepare_cpus(64)
}


kernel_init()
  --> smp_prepare_cpus(setup_max_cpus) --> native_smp_prepare_cpus(64)

native_smp_prepare_cpus()

#define for_each_possible_cpu(cpu) for_each_cpu((cpu), cpu_possible_mask)


num_processors = 4   // 初始值为 0
nr_cpu_ids=4
disabled_cpus
total_cpus
possible

// ensure num_processors
setup_arch()
acpi_boot_init()
  --> acpi_process_madt()
	--> acpi_parse_madt_lapic_entries() 

// ensure nr_cpu_ids
__init void prefill_possible_map(void)
{
        int i, possible;

        /* no processor from mptable or madt */
        if (!num_processors)
                num_processors = 1;

        if (setup_possible_cpus == -1)
                possible = num_processors + disabled_cpus;
        else
                possible = setup_possible_cpus;

        total_cpus = max_t(int, possible, num_processors + disabled_cpus);

        /* nr_cpu_ids could be reduced via nr_cpus= */
        if (possible > nr_cpu_ids) {
                printk(KERN_WARNING
                        "%d Processors exceeds NR_CPUS limit of %d\n",
                        possible, nr_cpu_ids);
                possible = nr_cpu_ids;
        }

        printk(KERN_INFO "SMP: Allowing %d CPUs, %d hotplug CPUs\n",
                possible, max_t(int, possible - num_processors, 0));

        for (i = 0; i < possible; i++)
                set_cpu_possible(i, true);

        nr_cpu_ids = possible;
}



/* Called by boot processor to activate the rest. */
static void __init smp_init(void)
{
        unsigned int cpu;

        /* FIXME: This should be done in userspace --RR */
        for_each_present_cpu(cpu) {
                if (num_online_cpus() >= setup_max_cpus)
                        break;
                if (!cpu_online(cpu))
                        cpu_up(cpu);
                        // 根据 nr_cpu_ids 循环 up CPU (4 个 CPU up 其他 3 个)
        }

        /* Any cleanup work */
        printk(KERN_INFO "Brought up %ld CPUs\n", (long)num_online_cpus());
        smp_cpus_done(setup_max_cpus);

        printk(KERN_DEBUG "sizeof(vma)=%u bytes\n", (unsigned int) sizeof(struct vm_area_struct));
        printk(KERN_DEBUG "sizeof(page)=%u bytes\n", (unsigned int) sizeof(struct page));
        printk(KERN_DEBUG "sizeof(inode)=%u bytes\n", (unsigned int) sizeof(struct inode));
        printk(KERN_DEBUG "sizeof(dentry)=%u bytes\n", (unsigned int) sizeof(struct dentry));
        printk(KERN_DEBUG "sizeof(ext3inode)=%u bytes\n", (unsigned int) sizeof(struct ext3_inode_info));
        printk(KERN_DEBUG "sizeof(buffer_head)=%u bytes\n", (unsigned int) sizeof(struct buffer_head));
        printk(KERN_DEBUG "sizeof(skbuff)=%u bytes\n", (unsigned int) sizeof(struct sk_buff));
        printk(KERN_DEBUG "sizeof(task_struct)=%u bytes\n", (unsigned int) sizeof(struct task_struct));
}

#define for_each_present_cpu(cpu)  for_each_cpu((cpu), cpu_present_mask)

#define cpu_online(cpu)		cpumask_test_cpu((cpu), cpu_online_mask)

/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
#define for_each_cpu(cpu, mask)                         \
        for ((cpu) = -1;                                \
                (cpu) = cpumask_next((cpu), (mask)),    \
                (cpu) < nr_cpu_ids;)


/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static inline unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
        /* -1 is a legal arg here. */
        if (n != -1)
                cpumask_check(n);
        return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n+1);
}

