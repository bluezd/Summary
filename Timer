start_kernel()
   --> tick_init()

/**
 * tick_init - initialize the tick control
 *
 * Register the notifier with the clockevents framework
 */
void __init tick_init(void)
{
        clockevents_register_notifier(&tick_notifier);
}


static struct notifier_block tick_notifier = {
        .notifier_call = tick_notify,
};

/**
 * clockevents_register_notifier - register a clock events change listener
 */
int clockevents_register_notifier(struct notifier_block *nb)
{
        nb = &tick_notifier
        unsigned long flags;
        int ret;

        spin_lock_irqsave(&clockevents_lock, flags);
        ret = raw_notifier_chain_register(&clockevents_chain, nb);
        spin_unlock_irqrestore(&clockevents_lock, flags);
        
        return ret;
}

/* Notification for clock events */
static RAW_NOTIFIER_HEAD(clockevents_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

#define RAW_NOTIFIER_INIT(name) {                               \
                .head = NULL }

struct raw_notifier_head {
        struct notifier_block *head;
};

struct notifier_block {
        int (*notifier_call)(struct notifier_block *, unsigned long, void *);
        struct notifier_block *next;
        int priority;
};


int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}

/*
 *      Notifier chain core routines.  The exported routines below
 *      are layered on top of these, with appropriate locking added.
 */

static int notifier_chain_register(struct notifier_block **nl,
                struct notifier_block *n)
{               
        此时 *nl = NULL

        while ((*nl) != NULL) {
                if (n->priority > (*nl)->priority)
                        break;
                nl = &((*nl)->next);
        }
        n->next = *nl;
        n->next = NULL;

        rcu_assign_pointer(*nl, n);
        *nl = n;
        clockevents_chain->head = &tick_notifier;

        return 0;
}

/**
 * rcu_assign_pointer - assign (publicize) a pointer to a newly
 * initialized structure that will be dereferenced by RCU read-side
 * critical sections.  Returns the value assigned.
 *
 * Inserts memory barriers on architectures that require them
 * (pretty much all of them other than x86), and also prevents
 * the compiler from reordering the code that initializes the
 * structure after the pointer assignment.  More importantly, this
 * call documents which pointers will be dereferenced by RCU read-side
 * code.
 */             

#define rcu_assign_pointer(p, v) \
        ({ \    
                if (!__builtin_constant_p(v) || \
                    ((v) != NULL)) \
                        smp_wmb(); \
                (p) = (v); \
        })

当时钟事件设备信息发生变化时调用tick_notifier



-----------------------------------------------------------------------------------------------------------------------

start_kernel:
    init_IRQ()

void __init init_IRQ(void)
{
	x86_init.irqs.intr_init();
        // invoke native_init_IRQ() function
}

void __init native_init_IRQ(void)
{                                                                                                                                                          
        int i;

        /* Execute any quirks before the call gates are initialised: */
        x86_init.irqs.pre_vector_init();
        // invoke init_ISA_irqs()

        apic_intr_init();

        /*
         * Cover the whole vector space, no vector can escape
         * us. (some of these will be overridden and become
         * 'special' SMP interrupts)
         */
        for (i = FIRST_EXTERNAL_VECTOR; i < NR_VECTORS; i++) {
                /* IA32_SYSCALL_VECTOR could be used in trap_init already. */
                if (!test_bit(i, used_vectors))
                        set_intr_gate(i, interrupt[i-FIRST_EXTERNAL_VECTOR]);
        }
	// 设置中断门，处理程序指向 interrupt 数组

        if (!acpi_ioapic)
                setup_irq(2, &irq2);

#ifdef CONFIG_X86_32
        /*
         * External FPU? Set up irq13 if so, for
         * original braindamaged IBM FERR coupling.
         */
        if (boot_cpu_data.hard_math && !cpu_has_fpu)
                setup_irq(FPU_IRQ, &fpu_irq);

        irq_ctx_init(smp_processor_id());
#endif
}

#define FIRST_EXTERNAL_VECTOR           0x20
#define NR_VECTORS                       256

void __init init_ISA_irqs(void)
{      
        int i;

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        init_bsp_APIC();
        // 配置本地LAPIC芯片。该函数调用apic_read或apic_write调用全局变量apic的read和write方法
#endif 
        init_8259A(0);

        /*
         * 16 old-style INTA-cycle interrupts:
         */
        for (i = 0; i < NR_IRQS_LEGACY; i++) {
                struct irq_desc *desc = irq_to_desc(i);
               
                desc->status = IRQ_DISABLED;
                desc->action = NULL;
                desc->depth = 1;

                set_irq_chip_and_handler_name(i, &i8259A_chip,
                                              handle_level_irq, "XT");
        }
}

void __init init_bsp_APIC(void)
{
        unsigned int value;
        
        /*
         * Don't do the setup now if we have a SMP BIOS as the
         * through-I/O-APIC virtual wire mode might be active.
         */
        if (smp_found_config || !cpu_has_apic)
                return;
       
        /*
         * Do not trust the local APIC being empty at bootup.
         */
        clear_local_APIC();

        /*
         * Enable APIC.
         */
        value = apic_read(APIC_SPIV);
        value &= ~APIC_VECTOR_MASK;
        value |= APIC_SPIV_APIC_ENABLED;

#ifdef CONFIG_X86_32
        /* This bit is reserved on P4/Xeon and should be cleared */
        if ((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &&
            (boot_cpu_data.x86 == 15))
                value &= ~APIC_SPIV_FOCUS_DISABLED;
        else
#endif
                value |= APIC_SPIV_FOCUS_DISABLED;
        value |= SPURIOUS_APIC_VECTOR;
        apic_write(APIC_SPIV, value);

        /*
         * Set up the virtual wire mode.
         */
        apic_write(APIC_LVT0, APIC_DM_EXTINT);
        value = APIC_DM_NMI;
        if (!lapic_is_integrated())             /* 82489DX */
                value |= APIC_LVT_LEVEL_TRIGGER;
        apic_write(APIC_LVT1, value);
}

// apic 其代表一块LAPIC控制器芯片 init_bsp_APIC函数实际上调用native_apic_mem_read和native_apic_mem_write等方法
struct apic *apic = &apic_default;

static void __init apic_intr_init(void)
{      
        smp_intr_init();

        // 设置终端描述符表 IDT APIC 相关中断服务程序。也就是把位于32~255之间，除去系统调用外其他中断向量的中断处理程序设置为interrupt[i]

#ifdef CONFIG_X86_THERMAL_VECTOR
        alloc_intr_gate(THERMAL_APIC_VECTOR, thermal_interrupt);
#endif
#ifdef CONFIG_X86_MCE_THRESHOLD
        alloc_intr_gate(THRESHOLD_APIC_VECTOR, threshold_interrupt);
#endif
#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_LOCAL_APIC)
        alloc_intr_gate(MCE_SELF_VECTOR, mce_self_interrupt);
#endif  

#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)
        /* self generated IPI for local APIC timer */
        alloc_intr_gate(LOCAL_TIMER_VECTOR, apic_timer_interrupt);
	/*
	 *将中断处理函数直接放在中断门得指向地址，这样只要中断到来，一旦通过中断门将直接跳像中断处理函数，而忽略了irq_d         * esc部分，不需要考虑怎么触发，不需要考虑怎么调度
	 * cat /proc/interrups
	 * LOC:   71070080   67165396   44751028   31680629   Local timer interrupts
	 *
	 * cat /proc/interrupts 也可以看到这些中断与众不同.这些中断左边显示的不是中断请求号，而是一个标识，右边显示的也         * 不是中断控制芯片的信息。
	 * 
	 */
        // 就是这里。。。。。。

        /* IPI for X86 platform specific use */
        alloc_intr_gate(X86_PLATFORM_IPI_VECTOR, x86_platform_ipi);

        /* IPI vectors for APIC spurious and error interrupts */
        alloc_intr_gate(SPURIOUS_APIC_VECTOR, spurious_interrupt);
        alloc_intr_gate(ERROR_APIC_VECTOR, error_interrupt);

        /* Performance monitoring interrupts: */
# ifdef CONFIG_PERF_EVENTS
        alloc_intr_gate(LOCAL_PENDING_VECTOR, perf_pending_interrupt);
# endif

#endif
}

#define LOCAL_TIMER_VECTOR              0xef   //239

static inline void alloc_intr_gate(unsigned int n, void *addr)
{      
        alloc_system_vector(n);
        set_intr_gate(n, addr);
}

tatic inline void alloc_system_vector(int vector)
{      
        if (!test_bit(vector, used_vectors)) {
                set_bit(vector, used_vectors);
                if (first_system_vector > vector)
                        first_system_vector = vector;
        } else
                BUG();
}

static inline void set_intr_gate(unsigned int n, void *addr)
{      
        BUG_ON((unsigned)n > 0xFF);
        _set_gate(n, GATE_INTERRUPT, addr, 0, 0, __KERNEL_CS);
}

static inline void _set_gate(int gate, unsigned type, void *addr,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate_desc s;
        pack_gate(&s, type, (unsigned long)addr, dpl, ist, seg);
        /*
         * does not need to be atomic because it is only done once at
         * setup time
         */
        write_idt_entry(idt_table, gate, &s);
}

/*
 * X86_64
 */
static inline void pack_gate(gate_desc *gate, unsigned type, unsigned long func,
                             unsigned dpl, unsigned ist, unsigned seg)
{      
        gate->offset_low = PTR_LOW(func);
        gate->segment = __KERNEL_CS;
        gate->ist = ist;
        gate->p = 1;         
        gate->dpl = dpl;     
        gate->zero0 = 0;
        gate->zero1 = 0;
        gate->type = type;
        gate->offset_middle = PTR_MIDDLE(func);
        gate->offset_high = PTR_HIGH(func);
}


BUILD_INTERRUPT(apic_timer_interrupt,LOCAL_TIMER_VECTOR)

#define BUILD_INTERRUPT(name, nr)       BUILD_INTERRUPT3(name, nr, smp_##name)

/*
 *  Irq entries should be protected against kprobes
 */
        .pushsection .kprobes.text, "ax"
#define BUILD_INTERRUPT3(name, nr, fn)  \
ENTRY(name)                             \
        RING0_INT_FRAME;                \
        pushl $~(nr);                   \
        CFI_ADJUST_CFA_OFFSET 4;        \
        SAVE_ALL;                       \
        TRACE_IRQS_OFF                  \
        movl %esp,%eax;                 \
        call fn;                        \
        jmp ret_from_intr;              \
        CFI_ENDPROC;                    \
ENDPROC(name)

当 cpu 执行完一条指令后会见查是否有中断发生，此时 assume Local APIC 发生中断请求，cpu 则根据中断号在中断描述符表中查找到段选择子，进而找到处理程序，此时就是执行 apic_timer_interrupt,然后 call smp_apic_timer_interrupt ...... 于是发生了一些很重要的事情。

-----------------------------------------------------------------------------------------------------------------------

start_kernel:
     init_timers();

void __init init_timers(void)
{       
        int err = timer_cpu_notify(&timers_nb, (unsigned long)CPU_UP_PREPARE,
                                (void *)(long)smp_processor_id());
        
        init_timer_stats();
        
        BUG_ON(err == NOTIFY_BAD);
        register_cpu_notifier(&timers_nb);
        open_softirq(TIMER_SOFTIRQ, run_timer_softirq);
        // 软中断 下半步 高精度 timer.
}

static struct notifier_block __cpuinitdata timers_nb = {
        .notifier_call  = timer_cpu_notify,
};

初始化指定CPU上的软时钟相关的数据结构
static int __cpuinit timer_cpu_notify(struct notifier_block *self,
                                unsigned long action, void *hcpu)
{       
        long cpu = (long)hcpu;
        switch(action) {
        case CPU_UP_PREPARE:
        case CPU_UP_PREPARE_FROZEN:
                if (init_timers_cpu(cpu) < 0)
                        return NOTIFY_BAD;
                break;
#ifdef CONFIG_HOTPLUG_CPU
        case CPU_DEAD:
        case CPU_DEAD_FROZEN:
                migrate_timers(cpu);
                break;
#endif
        default:
                break;
        }
        return NOTIFY_OK;
}

static int __cpuinit init_timers_cpu(int cpu)
{
        int j;
        struct tvec_base *base;
        static char __cpuinitdata tvec_base_done[NR_CPUS];

        if (!tvec_base_done[cpu]) {
                static char boot_done;

                if (boot_done) {
                        /*
                         * The APs use this path later in boot
                         */
                        base = kmalloc_node(sizeof(*base),
                                                GFP_KERNEL | __GFP_ZERO,
                                                cpu_to_node(cpu));
                        if (!base)
                                return -ENOMEM;

                        /* Make sure that tvec_base is 2 byte aligned */
                        if (tbase_get_deferrable(base)) {
                                WARN_ON(1);
                                kfree(base);
                                return -ENOMEM;
                        }
                        per_cpu(tvec_bases, cpu) = base;
                } else {
                        /*
                         * This is for the boot CPU - we use compile-time
                         * static initialisation because per-cpu memory isn't
                         * ready yet and because the memory allocators are not
                         * initialised either.
                         */
                        boot_done = 1;
                        base = &boot_tvec_bases;
                }
                tvec_base_done[cpu] = 1;
        } else {
                base = per_cpu(tvec_bases, cpu);
        }

        spin_lock_init(&base->lock);

        for (j = 0; j < TVN_SIZE; j++) {
                INIT_LIST_HEAD(base->tv5.vec + j);
                INIT_LIST_HEAD(base->tv4.vec + j);
                INIT_LIST_HEAD(base->tv3.vec + j);
                INIT_LIST_HEAD(base->tv2.vec + j);
        }
        for (j = 0; j < TVR_SIZE; j++)
                INIT_LIST_HEAD(base->tv1.vec + j);

        base->timer_jiffies = jiffies;
        // 设置成自系统启动以来产生的节拍数。
        base->next_timer = base->timer_jiffies;
        return 0;
}

struct tvec_base {
        spinlock_t lock;
        struct timer_list *running_timer;
        unsigned long timer_jiffies;
        unsigned long next_timer;
        struct tvec_root tv1;
        struct tvec tv2;
        struct tvec tv3;
        struct tvec tv4;
        struct tvec tv5;
} ____cacheline_aligned;


static inline uint32_t raid6_jiffies(void)
{               
        struct timeval tv;
        gettimeofday(&tv, NULL);
        return tv.tv_sec*1000 + tv.tv_usec/1000;
}    

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
};


static __always_inline int gettimeofday(struct timeval *tv, struct timezone *tz)
{
        int ret;
        asm volatile("syscall"
                : "=a" (ret)
                : "0" (__NR_gettimeofday),"D" (tv),"S" (tz)
                : __syscall_clobber );
        return ret;
}       

/**             
 * do_gettimeofday - Returns the time of day in a timeval
 * @tv:         pointer to the timeval to be set
 *
 * NOTE: Users should be converted to using getnstimeofday()
 */
void do_gettimeofday(struct timeval *tv)
{       
        struct timespec now;
                
        getnstimeofday(&now);
	// now 中返回当前时间 秒数以及纳秒数
	
        tv->tv_sec = now.tv_sec;
	// 秒数
        tv->tv_usec = now.tv_nsec/1000;
	// now.tv_nsec 不足一秒的纳秒数 转换为微妙数.
}       

struct timeval {
        __kernel_time_t         tv_sec;         /* seconds */
        __kernel_suseconds_t    tv_usec;        /* microseconds */
}; 

/**
 * getnstimeofday - Returns the time of day in a timespec
 * @ts:         pointer to the timespec to be set
 *
 * Returns the time of day in a timespec.
 */
void getnstimeofday(struct timespec *ts)
{       
        unsigned long seq;
        s64 nsecs;

        WARN_ON(timekeeping_suspended);

        do {
                seq = read_seqbegin(&xtime_lock);

                *ts = xtime;
                nsecs = timekeeping_get_ns();

                /* If arch requires, add in gettimeoffset() */
                nsecs += arch_gettimeoffset(); // NULL function ?

        } while (read_seqretry(&xtime_lock, seq));

        timespec_add_ns(ts, nsecs);
        // add nsecs 到 struct timespec 结构.
}

truct timespec {
        time_t  tv_sec;         /* seconds */
        long    tv_nsec;        /* nanoseconds */
};

/* Timekeeper helper functions. */
static inline s64 timekeeping_get_ns(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;

        /* read clocksource: */
        clock = timekeeper.clock;
        // 时钟源 PIT/HPET/TSC

        cycle_now = clock->read(clock);
        // 读取时钟周期的当前计数值

        /* calculate the delta since the last update_wall_time: */
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;

        /* return delta convert to nanoseconds using ntp adjusted mult. */
        return clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
	//转换成纳秒
}

/* Structure holding internal timekeeping values. */
struct timekeeper {
        /* Current clocksource used for timekeeping. */
        struct clocksource *clock;
        /* The shift value of the current clocksource. */
        int     shift;

        /* Number of clock cycles in one NTP interval. */
        cycle_t cycle_interval; 
        /* Number of clock shifted nano seconds in one NTP interval. */
        u64     xtime_interval;
        /* Raw nano seconds accumulated per NTP interval. */
        u32     raw_interval;

        /* Clock shifted nano seconds remainder not stored in xtime.tv_nsec. */
        u64     xtime_nsec;
        /* Difference between accumulated time and NTP time in ntp
         * shifted nano seconds. */
        s64     ntp_error;
        /* Shift conversion between clock shifted nano seconds and
         * ntp shifted nano seconds. */
        int     ntp_error_shift;
        /* NTP adjusted clock multiplier */
        u32     mult;
};

typedef u64 cycle_t;

/**     
 *  * clocksource_cyc2ns - converts clocksource cycles to nanoseconds
 *   *      
 *    * Converts cycles to nanoseconds, using the given mult and shift.
 *     *      
 *      * XXX - This could use some mult_lxl_ll() asm optimization
 *       */     
static inline s64 clocksource_cyc2ns(cycle_t cycles, u32 mult, u32 shift)
{       
	        return ((u64) cycles * mult) >> shift;
}

/**
 *  * timespec_add_ns - Adds nanoseconds to a timespec
 *   * @a:          pointer to timespec to be incremented
 *    * @ns:         unsigned nanoseconds value to be added
 *     *              
 *      * This must always be inlined because its used from the x86-64 vdso,
 *       * which cannot call other kernel functions.
 *        */             
static __always_inline void timespec_add_ns(struct timespec *a, u64 ns)
{
	        a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, NSEC_PER_SEC, &ns);
                // 返回 秒数 一般情况下返回 0
		a->tv_nsec = ns;
		// 不足 1s 的纳秒数 (此值会一直叠加)
}       


static __always_inline u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{       
	// dividend = a->tv_nsec + ns
        u32 ret = 0;

        while (dividend >= divisor) {
		// 如果 纳秒大于一秒 NSEC_PER_SEC 

                /* The following asm() prevents the compiler from
                   optimising this loop into a modulo operation.  */
                asm("" : "+rm"(dividend));

                dividend -= divisor;
		// 减去一秒

                ret++;
		// 秒数加1
        }
        
        *remainder = dividend;

        return ret;
}

void __init init_timer_stats(void)
{                                    
        int cpu;
        
        for_each_possible_cpu(cpu)
                spin_lock_init(&per_cpu(lookup_lock, cpu));
}      

/* Need to know about CPUs going up/down? */
int __ref register_cpu_notifier(struct notifier_block *nb)
{
        nb=&timers_nb

        int ret;
        cpu_maps_update_begin();
        ret = raw_notifier_chain_register(&cpu_chain, nb);
        cpu_maps_update_done();
        return ret;
}

static __cpuinitdata RAW_NOTIFIER_HEAD(cpu_chain);

#define RAW_NOTIFIER_HEAD(name)                                 \
        struct raw_notifier_head name =                         \
                RAW_NOTIFIER_INIT(name)

void cpu_maps_update_begin(void)
{
        mutex_lock(&cpu_add_remove_lock);
}

/**
 *      raw_notifier_chain_register - Add notifier to a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @n: New entry in notifier chain
 *
 *      Adds a notifier to a raw notifier chain.
 *      All locking must be provided by the caller.
 *
 *      Currently always returns zero.
 */
int raw_notifier_chain_register(struct raw_notifier_head *nh,
                struct notifier_block *n)
{
        return notifier_chain_register(&nh->head, n);
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    hrtimers_init();

void __init hrtimers_init(void)
{
        hrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,
                          (void *)(long)smp_processor_id());
        register_cpu_notifier(&hrtimers_nb);
#ifdef CONFIG_HIGH_RES_TIMERS
        open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);
#endif
}


------------------------------------------------------------------------------------------------------------------


start_kernel:
    timekeeping_init();

/*
 * timekeeping_init - Initializes the clocksource and common timekeeping values
 */
void __init timekeeping_init(void)
{
        struct clocksource *clock;
        unsigned long flags;
        struct timespec now, boot;

        read_persistent_clock(&now);
        read_boot_clock(&boot);

        write_seqlock_irqsave(&xtime_lock, flags);

        ntp_init();

        clock = clocksource_default_clock();
        // clock = &clocksource_jiffies
    
        if (clock->enable)
                clock->enable(clock);
        timekeeper_setup_internals(clock);

        xtime.tv_sec = now.tv_sec;
        xtime.tv_nsec = now.tv_nsec;
        raw_time.tv_sec = 0;
        raw_time.tv_nsec = 0;
        if (boot.tv_sec == 0 && boot.tv_nsec == 0) {
                boot.tv_sec = xtime.tv_sec;
                boot.tv_nsec = xtime.tv_nsec;
        }
        set_normalized_timespec(&wall_to_monotonic,
                                -boot.tv_sec, -boot.tv_nsec);
        update_xtime_cache(0);
        total_sleep_time.tv_sec = 0;
        total_sleep_time.tv_nsec = 0;
        write_sequnlock_irqrestore(&xtime_lock, flags);
}


/**
 *  * read_boot_clock -  Return time of the system start.
 *   *
 *    * Weak dummy function for arches that do not yet support it.
 *     * Function to read the exact time the system has been started.
 *      * Returns a timespec with tv_sec=0 and tv_nsec=0 if unsupported.
 *       *      
 *        *  XXX - Do be sure to remove it once all arches implement it.
 *         */
void __attribute__((weak)) read_boot_clock(struct timespec *ts)
{
	        ts->tv_sec = 0;
		ts->tv_nsec = 0;
}       

void __init ntp_init(void)
{       
        ntp_clear();
        hrtimer_init(&leap_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
        leap_timer.function = ntp_leap_second;
}       

/**
 * ntp_clear - Clears the NTP state variables
 *
 * Must be called while holding a write on the xtime_lock
 */
void ntp_clear(void)
{
        time_adjust     = 0;            /* stop active adjtime() */
        time_status     |= STA_UNSYNC;
        time_maxerror   = NTP_PHASE_LIMIT;
        time_esterror   = NTP_PHASE_LIMIT;

        ntp_update_frequency();

        tick_length     = tick_length_base;
        time_offset     = 0;
}

/**     
 * hrtimer_init - initialize a timer to the given clock
 * @timer:      the timer to be initialized
 * @clock_id:   the clock to be used
 * @mode:       timer mode abs/rel
 */     
void hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                  enum hrtimer_mode mode)
{
        debug_init(timer, clock_id, mode);
        __hrtimer_init(timer, clock_id, mode);
}       
EXPORT_SYMBOL_GPL(hrtimer_init);

static void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,
                           enum hrtimer_mode mode)
{               
        struct hrtimer_cpu_base *cpu_base;
        
        memset(timer, 0, sizeof(struct hrtimer));
        
        cpu_base = &__raw_get_cpu_var(hrtimer_bases);
                
        if (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)
                clock_id = CLOCK_MONOTONIC;
                                         
        timer->base = &cpu_base->clock_base[clock_id];
        hrtimer_init_timer_hres(timer);
        
#ifdef CONFIG_TIMER_STATS  
        timer->start_site = NULL;
        timer->start_pid = -1;
        memset(timer->start_comm, 0, TASK_COMM_LEN);
#endif  
}


struct clocksource * __init __weak clocksource_default_clock(void)
{
        return &clocksource_jiffies;
}


/**
 * timekeeper_setup_internals - Set up internals to use clocksource clock.
 *
 * @clock:              Pointer to clocksource.
 *
 * Calculates a fixed cycle/nsec interval for a given clocksource/adjustment
 * pair and interval request.
 *
 * Unless you're the timekeeping code, you should not be using this!
 */
static void timekeeper_setup_internals(struct clocksource *clock)
{
        cycle_t interval;
        u64 tmp;

        timekeeper.clock = clock;
	// 设置时钟源 time_keeper gettimeofday 会通过 timer_keeper layer 获取 wall time
	
        clock->cycle_last = clock->read(clock);

        /* Do the ns -> cycle conversion first, using original mult */
        tmp = NTP_INTERVAL_LENGTH;
        tmp <<= clock->shift;
        tmp += clock->mult/2;
        do_div(tmp, clock->mult);
        if (tmp == 0)
                tmp = 1;

        interval = (cycle_t) tmp;
        timekeeper.cycle_interval = interval;

        /* Go back from cycles -> shifted ns */
        timekeeper.xtime_interval = (u64) interval * clock->mult;
        timekeeper.raw_interval =
                ((u64) interval * clock->mult) >> clock->shift;

        timekeeper.xtime_nsec = 0;
        timekeeper.shift = clock->shift;

        timekeeper.ntp_error = 0;
        timekeeper.ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;

        /*
         * The timekeeper keeps its own mult values for the currently
         * active clocksource. These value will be adjusted via NTP
         * to counteract clock drifting.
         */
        timekeeper.mult = clock->mult;
}



------------------------------------------------------------------------------------------------------------------


/*
 * Initialize TSC and delay the periodic timer init to
 * late x86_late_time_init() so ioremap works.
 */
void __init time_init(void)
{
        late_time_init = x86_late_time_init;
}

start_kernel:
        if (late_time_init)
             late_time_init();

+----------------------------------------------------------------------------------------+
|       /*                                                                               |
|        * The platform setup functions are preset with the default functions            |
|        * for standard PC hardware.                                                     |
|        */                                                                              |
|       struct x86_init_ops x86_init __initdata = {                                      |
|                                                                                        |
|               .resources = {                                                           |
|                       .probe_roms             = probe_roms,                            |
|                       .reserve_resources      = reserve_standard_io_resources,         |
|                       .memory_setup           = default_machine_specific_memory_setup, |
|               },                                                                       |
|                                                                                        |
|               .mpparse = {                                                             |
|                       .mpc_record             = x86_init_uint_noop,                    |
|                       .setup_ioapic_ids       = x86_init_noop,                         |
|                       .mpc_apic_id            = default_mpc_apic_id,                   |
|                       .smp_read_mpc_oem       = default_smp_read_mpc_oem,              |
|                       .mpc_oem_bus_info       = default_mpc_oem_bus_info,              |
|                       .find_smp_config        = default_find_smp_config,               |
|                       .get_smp_config         = default_get_smp_config,                |
|               },                                                                       |
|                                                                                        |
|               .irqs = {                                                                |
|                       .pre_vector_init        = init_ISA_irqs,                         |
|                       .intr_init              = native_init_IRQ,                       |
|                       .trap_init              = x86_init_noop,                         |
|               },                                                                       |
|                                                                                        |
|               .oem = {                                                                 |
|                       .arch_setup             = x86_init_noop,                         |
|                       .banner                 = default_banner,                        |
|               },                                                                       |
|                                                                                        |
|               .paging = {                                                              |
|                       .pagetable_setup_start  = native_pagetable_setup_start,          |
|                       .pagetable_setup_done   = native_pagetable_setup_done,           |
|               },                                                                       |
|                                                                                        |
|               .timers = {                                                              |
|                       .setup_percpu_clockev   = setup_boot_APIC_clock,                 |
|                       .tsc_pre_init           = x86_init_noop,                         |
|                       .timer_init             = hpet_time_init,                        |
|               },                                                                       |
|       };                                                                               |
|                                                                                        |
+----------------------------------------------------------------------------------------+


static __init void x86_late_time_init(void)
{
        x86_init.timers.timer_init();
        tsc_init();
}

/* Default timer init function */
void __init hpet_time_init(void)
{
        if (!hpet_enable())
                setup_pit_timer();
        setup_default_timer_irq();
}

/**
 * hpet_enable - Try to setup the HPET timer. Returns 1 on success.
 */
int __init hpet_enable(void)
{
        unsigned long id;
        int i;

        if (!is_hpet_capable())
                return 0;
                
        hpet_set_mapping();

        /*
         * Read the period and check for a sane value:
         */     
        hpet_period = hpet_readl(HPET_PERIOD);
                
        /*      
         * AMD SB700 based systems with spread spectrum enabled use a
         * SMM based HPET emulation to provide proper frequency
         * setting. The SMM code is initialized with the first HPET
         * register access and takes some time to complete. During
         * this time the config register reads 0xffffffff. We check
         * for max. 1000 loops whether the config register reads a non
         * 0xffffffff value to make sure that HPET is up and running
         * before we go further. A counting loop is safe, as the HPET
         * access takes thousands of CPU cycles. On non SB700 based
         * machines this check is only done once and has no side
         * effects.
         */
        for (i = 0; hpet_readl(HPET_CFG) == 0xFFFFFFFF; i++) {
                if (i == 1000) {
                        printk(KERN_WARNING
                               "HPET config register value = 0xFFFFFFFF. "
                               "Disabling HPET\n");
                        goto out_nohpet;
                }
        }
        
        if (hpet_period < HPET_MIN_PERIOD || hpet_period > HPET_MAX_PERIOD)
                goto out_nohpet;

        /*
         * Read the HPET ID register to retrieve the IRQ routing
         * information and the number of channels
         */     
        id = hpet_readl(HPET_ID);
        hpet_print_config();

#ifdef CONFIG_HPET_EMULATE_RTC
        /*
         * The legacy routing mode needs at least two channels, tick timer
         * and the rtc emulation channel.
         */
        if (!(id & HPET_ID_NUMBER))
                goto out_nohpet;
#endif

        if (hpet_clocksource_register())
                goto out_nohpet;

        if (id & HPET_ID_LEGSUP) {
                hpet_legacy_clockevent_register();
                hpet_msi_capability_lookup(2);
                return 1;
        }
        hpet_msi_capability_lookup(0);
        return 0;

out_nohpet:
        hpet_clear_mapping();
        hpet_address = 0;
        return 0;
}

static int hpet_clocksource_register(void)
{
        u64 start, now;
        cycle_t t1;

        /* Start the counter */
        hpet_restart_counter();

        /* Verify whether hpet counter works */
        t1 = hpet_readl(HPET_COUNTER);
        rdtscll(start);

        /*
         * We don't know the TSC frequency yet, but waiting for
         * 200000 TSC cycles is safe:
         * 4 GHz == 50us
         * 1 GHz == 200us
         */
        do {
                rep_nop();
                rdtscll(now);
        } while ((now - start) < 200000UL);

        if (t1 == hpet_readl(HPET_COUNTER)) {
                printk(KERN_WARNING
                       "HPET counter not counting. HPET disabled\n");
                return -ENODEV;
        }

        /*
         * The definition of mult is (include/linux/clocksource.h)
         * mult/2^shift = ns/cyc and hpet_period is in units of fsec/cyc
         * so we first need to convert hpet_period to ns/cyc units:
         *  mult/2^shift = ns/cyc = hpet_period/10^6
         *  mult = (hpet_period * 2^shift)/10^6
         *  mult = (hpet_period << shift)/FSEC_PER_NSEC
         */
        clocksource_hpet.mult = div_sc(hpet_period, FSEC_PER_NSEC, HPET_SHIFT);

        clocksource_register(&clocksource_hpet);

        return 0;
}

+------------------------------------------------------------------+
|                                                                  |
|       static struct clocksource clocksource_hpet = {             |
|               .name           = "hpet",                          |
|               .rating         = 250,                             |
|               .read           = read_hpet,                       |
|               .mask           = HPET_MASK,                       |
|               .shift          = HPET_SHIFT,                      |
|               .flags          = CLOCK_SOURCE_IS_CONTINUOUS,      |
|               .resume         = hpet_resume_counter,             |
|       #ifdef CONFIG_X86_64                                       |
|               .vread          = vread_hpet,                      |
|       #endif                                                     |
|       };                                                         |
|                                                                  |
+------------------------------------------------------------------+

/**
 * clocksource_register - Used to install new clocksources
 * @t:          clocksource to be registered
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 */     
int clocksource_register(struct clocksource *cs)
{       
        /* calculate max idle time permitted for this clocksource */
        cs->max_idle_ns = clocksource_max_deferment(cs);

        mutex_lock(&clocksource_mutex);
        clocksource_enqueue(cs);
        clocksource_select();
        clocksource_enqueue_watchdog(cs);
        mutex_unlock(&clocksource_mutex);
        return 0;
}       

/*
 * Enqueue the clocksource sorted by rating
 */
static void clocksource_enqueue(struct clocksource *cs)
{
        struct list_head *entry = &clocksource_list;
        // static LIST_HEAD(clocksource_list);
	// head prev 都指向自己

        struct clocksource *tmp;

        list_for_each_entry(tmp, &clocksource_list, list)
                /* Keep track of the place, where to insert */
                if (tmp->rating >= cs->rating)
                        entry = &tmp->list;
        list_add(&cs->list, entry);
	// 插入到双向循环链表中 pit --> hpet --> tsc --> head(clocksource_list)
}


                             head
                     +-->>+---------+<-+
                     |    |         |  |
                     | +----.next   |  |
                     | |  |         |  |
                     | |  | .prev -----------+
                     | |  |         |  |     |
                     | |  +---------+  |     |
                     | |               |     |
                     | |     TSC       |     |
                     | +->+---------+<---+   |
                     |    |         |  | |   |
                     | +----.next   |  | |   |
                     | |  |         |  | |   |
                     | |  | .prev -----+ |   |
                     | |  |         |    |   |
                     | |  +---------+    |   |
                     | |                 |   |
                     | |     HPET        |   |
                     | +->+---------+<-----+ |
                     |    |         |    | | |
                     | +----.next   |    | | |
                     | |  |         |    | | |
                     | |  | .prev -------+ | |
                     | |  |         |      | |
                     | |  +---------+      | |
                     | |                   | |
                     | |     PIT           | |
                     | +->+---------+<<------+
                     |    |         |      |
                     +------.next   |      |
                          |         |      |
                          | .prev ---------+
                          |         |
                          +---------+

/**
 * clocksource_select - Select the best clocksource available
 *
 * Private function. Must hold clocksource_mutex when called.
 *
 * Select the clocksource with the best rating, or the clocksource,
 * which is selected by userspace override.
 */
static void clocksource_select(void)
{
        struct clocksource *best, *cs;

        if (!finished_booting || list_empty(&clocksource_list))
                return;
        // 此时 finished_booting = 0,所以 closksource_select 函数直接返回。在系统初始化快结束时调用 clocksource_done_booting() 会设置finished_booting = 1,紧接着调用 clocksource_select() 进行真正的时钟源选择。
	
        /* First clocksource on the list has the best rating. */
        best = list_first_entry(&clocksource_list, struct clocksource, list);
        // 如上图中的链表中第一个就是最好的时钟源
	
        /* Check for the override clocksource. */
        list_for_each_entry(cs, &clocksource_list, list) {
                if (strcmp(cs->name, override_name) != 0)
                        continue;
                /*
                 * Check to make sure we don't switch to a non-highres
                 * capable clocksource if the tick code is in oneshot
                 * mode (highres or nohz)
                 */
                if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&
                    tick_oneshot_mode_active()) {
                        /* Override clocksource cannot be used. */
                        printk(KERN_WARNING "Override clocksource %s is not "
                               "HRT compatible. Cannot switch while in "
                               "HRT/NOHZ mode\n", cs->name);
                        override_name[0] = 0;
                } else
                        /* Override clocksource can be used. */
                        best = cs;
                break;
        }
        if (curr_clocksource != best) {
                printk(KERN_INFO "Switching to clocksource %s\n", best->name);
		// Switching to clocksource tsc log from dmesg . :-)

                curr_clocksource = best;
		// 保存当前使用的时钟源

                timekeeping_notify(curr_clocksource);
		// Install a new clocksource
        }
}


在此之前会注册几种时钟源
比如:
     core_initcall(init_jiffies_clocksource);
     fs_initcall(init_acpi_pm_clocksource);

 +-----------------------------------------+
 | fs_initcall(clocksource_done_booting);  |
 +-----------------------------------------+

#define fs_initcall(fn)                 __define_initcall("5",fn,5)

/*
 * clocksource_done_booting - Called near the end of core bootup
 *
 * Hack to avoid lots of clocksource churn at boot time.
 * We use fs_initcall because we want this to start before
 * device_initcall but after subsys_initcall.
 */
static int __init clocksource_done_booting(void)
{       
        finished_booting = 1;
	// 表示初始化完成
                        
        /*      
         * Run the watchdog first to eliminate unstable clock sources
         */
        clocksource_watchdog_kthread(NULL);

        mutex_lock(&clocksource_mutex);

        clocksource_select();
	// 选择最好的时钟源
	
        mutex_unlock(&clocksource_mutex);
        return 0;       
}                 


/**                     
 * timekeeping_notify - Install a new clock source
 * @clock:              pointer to the clock source
 *      
 * This function is called from clocksource.c after a new, better clock
 * source has been registered. The caller holds the clocksource_mutex.
 */     
void timekeeping_notify(struct clocksource *clock)
{                       
        if (timekeeper.clock == clock)
                return;
	// clock = &clocksource_tsc .
	// 此时 clock = &clocksource_jiffies .
	
        stop_machine(change_clocksource, clock, NULL);
        tick_clock_notify();*
}


/**     
 * Async notification about clocksource changes
 */     
void tick_clock_notify(void)
{
        int cpu;

        for_each_possible_cpu(cpu)
                set_bit(0, &per_cpu(tick_cpu_sched, cpu).check_clocks);
}


/**
 * change_clocksource - Swaps clocksources if a new one is available
 *
 * Accumulates current time interval and initializes new clocksource
 */
static int change_clocksource(void *data)
{
        struct clocksource *new, *old;

        new = (struct clocksource *) data;
	// new = $clocksource_tsc .

        timekeeping_forward_now();
	// 更新时间

        if (!new->enable || new->enable(new) == 0) {
		// new->enable == NULL

                old = timekeeper.clock;
                timekeeper_setup_internals(new);
		// 设置新的时钟源 tsc

                if (old->disable)
                        old->disable(old);
        }
        return 0;
}


/**             
 * timekeeping_forward_now - update clock to the current time
 *              
 * Forward the current clock to update its state since the last call to
 * update_wall_time(). This is useful before significant clock changes,
 * as it avoids having to deal with this time offset explicitly.
 */
static void timekeeping_forward_now(void)
{
        cycle_t cycle_now, cycle_delta;
        struct clocksource *clock;
        s64 nsec;

        clock = timekeeper.clock;
        cycle_now = clock->read(clock);
        cycle_delta = (cycle_now - clock->cycle_last) & clock->mask;
        clock->cycle_last = cycle_now;
        
        nsec = clocksource_cyc2ns(cycle_delta, timekeeper.mult,
                                  timekeeper.shift);
        
        /* If arch requires, add in gettimeoffset() */
        nsec += arch_gettimeoffset();

        timespec_add_ns(&xtime, nsec);

        nsec = clocksource_cyc2ns(cycle_delta, clock->mult, clock->shift);
        timespec_add_ns(&raw_time, nsec);
}



static void hpet_legacy_clockevent_register(void)
{
        /* Start HPET legacy interrupts */
        hpet_enable_legacy_int();

        /*
         * The mult factor is defined as (include/linux/clockchips.h)
         *  mult/2^shift = cyc/ns (in contrast to ns/cyc in clocksource.h)
         * hpet_period is in units of femtoseconds (per cycle), so
         *  mult/2^shift = cyc/ns = 10^6/hpet_period
         *  mult = (10^6 * 2^shift)/hpet_period
         *  mult = (FSEC_PER_NSEC << hpet_clockevent.shift)/hpet_period
         */
        hpet_clockevent.mult = div_sc((unsigned long) FSEC_PER_NSEC,
                                      hpet_period, hpet_clockevent.shift);
        /* Calculate the min / max delta */
        hpet_clockevent.max_delta_ns = clockevent_delta2ns(0x7FFFFFFF,
                                                           &hpet_clockevent);
        /* 5 usec minimum reprogramming delta. */
        hpet_clockevent.min_delta_ns = 5000;

        /*
         * Start hpet with the boot cpu mask and make it
         * global after the IO_APIC has been initialized.
         */
        hpet_clockevent.cpumask = cpumask_of(smp_processor_id());
        clockevents_register_device(&hpet_clockevent);
        global_clock_event = &hpet_clockevent;
        printk(KERN_DEBUG "hpet clockevent registered\n");
}

+------------------------------------------------------------------------------------+
|                                                                                    |
|       /*                                                                           |
|        * The hpet clock event device                                               |
|        */                                                                          |
|       static struct clock_event_device hpet_clockevent = {                         |
|       	.name           = "hpet",                                            |
|       	.features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,  |
|       	.set_mode       = hpet_legacy_set_mode,                              |
|       	.set_next_event = hpet_legacy_next_event,                            |
|       	.shift          = 32,                                                |
|       	.irq            = 0,                                                 |
|       	.rating         = 50,                                                |
|       };                                                                           |
+------------------------------------------------------------------------------------+

/*              
 * Initialize the conversion factor and the min/max deltas of the clock event
 * structure and register the clock event source with the framework.
 */             
void __init setup_pit_timer(void)
{               
        /*
         * Start pit with the boot cpu mask and make it global after the
         * IO_APIC has been initialized.
         */
        pit_ce.cpumask = cpumask_of(smp_processor_id());
        pit_ce.mult = div_sc(CLOCK_TICK_RATE, NSEC_PER_SEC, pit_ce.shift);
        pit_ce.max_delta_ns = clockevent_delta2ns(0x7FFF, &pit_ce);
        pit_ce.min_delta_ns = clockevent_delta2ns(0xF, &pit_ce);
                
        clockevents_register_device(&pit_ce);
        global_clock_event = &pit_ce;
}       

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       /*                                                                                |
|        * On UP the PIT can serve all of the possible timer functions. On SMP systems    |
|        * it can be solely used for the global tick.                                     |
|        *                                                                                |
|        * The profiling and update capabilities are switched off once the local apic is  |
|        * registered. This mechanism replaces the previous #ifdef LOCAL_APIC -           |
|        * !using_apic_timer decisions in do_timer_interrupt_hook()                       |
|        */                                                                               |
|       static struct clock_event_device pit_ce = {                                       |
|               .name           = "pit",                                                  |
|               .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,       |
|               .set_mode       = init_pit_timer,                                         |
|               .set_next_event = pit_next_event,                                         |
|               .shift          = 32,                                                     |
|               .irq            = 0,                                                      |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+

void __init setup_default_timer_irq(void)
{       
        setup_irq(0, &irq0);
}

+-----------------------------------------------------------------------------------------+
|                                                                                         |
|       static struct irqaction irq0  = {                                                 |
|               .handler = timer_interrupt,                                               |
|               .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,    |
|               .name = "timer"                                                           |
|       };                                                                                |
+-----------------------------------------------------------------------------------------+


void __init tsc_init(void)
{
        u64 lpj;
        int cpu;
                
        x86_init.timers.tsc_pre_init();
                
        if (!cpu_has_tsc)
                return;
        
        tsc_khz = x86_platform.calibrate_tsc();
        // invoke native_calibrate_tsc(); calibrate the tsc on boot


        cpu_khz = tsc_khz;
        
        if (!tsc_khz) {
                mark_tsc_unstable("could not calculate TSC khz");
                return;
        }

        printk("Detected %lu.%03lu MHz processor.\n",
                        (unsigned long)cpu_khz / 1000,
                        (unsigned long)cpu_khz % 1000);
        
        /*
         * Secondary CPUs do not run through tsc_init(), so set up
         * all the scale factors for all CPUs, assuming the same
         * speed as the bootup CPU. (cpufreq notifiers will fix this
         * up if their speed diverges)
         */
        for_each_possible_cpu(cpu)
                set_cyc2ns_scale(cpu_khz, cpu);

        if (tsc_disabled > 0)
                return;
        
        /* now allow native_sched_clock() to use rdtsc */
        tsc_disabled = 0;

        lpj = ((u64)tsc_khz * 1000);
        do_div(lpj, HZ);
        lpj_fine = lpj;
        
        use_tsc_delay();
        /* Check and install the TSC clocksource */
        dmi_check_system(bad_tsc_dmi_table);

        if (unsynchronized_tsc())
                mark_tsc_unstable("TSCs unsynchronized");

        check_system_tsc_reliable();
        init_tsc_clocksource();
}       

struct x86_platform_ops x86_platform = {
        .calibrate_tsc                  = native_calibrate_tsc,
        .get_wallclock                  = mach_get_cmos_time,
        .set_wallclock                  = mach_set_rtc_mmss,
        .is_untracked_pat_range         = is_ISA_range,
        .nmi_init                       = default_nmi_init
};


static void __init init_tsc_clocksource(void)
{
        clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
                        clocksource_tsc.shift);
        if (tsc_clocksource_reliable)
                clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
        /* lower the rating if we already know its unstable: */
        if (check_tsc_unstable()) {
                clocksource_tsc.rating = 0;
                clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
        }
        clocksource_register(&clocksource_tsc);
}

+-------------------------------------------------------------------------+
|                                                                         |
|                                                                         |
|      static struct clocksource clocksource_tsc = {                      |
|               .name                   = "tsc",                          |
|               .rating                 = 300,                            |
|               .read                   = read_tsc,                       |
|               .resume                 = resume_tsc,                     |
|               .mask                   = CLOCKSOURCE_MASK(64),           |
|               .shift                  = 22,                             |
|               .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |    |
|                                         CLOCK_SOURCE_MUST_VERIFY,       |
|       #ifdef CONFIG_X86_64                                              |
|               .vread                  = vread_tsc,                      |
|       #endif                                                            |
|       };                                                                |
|                                                                         |
+-------------------------------------------------------------------------+

        # cat /proc/timer_list | more

        Tick Device: mode:     1                      --> one-shot 单次 
        Broadcast device
        Clock Event Device: hpet
         max_delta_ns:   149983013276
         min_delta_ns:   13409
         mult:           61496111
         shift:          32
         mode:           3
         next_event:     21524167000000 nsecs
         set_next_event: hpet_legacy_next_event
         set_mode:       hpet_legacy_set_mode
         event_handler:  tick_handle_oneshot_broadcast
         retries:        1459862
        tick_broadcast_mask: 00000001
        tick_broadcast_oneshot_mask: 00000002


        Tick Device: mode:     1
        Per CPU device: 0
        Clock Event Device: lapic
         max_delta_ns:   128824925406
         min_delta_ns:   1000
         mult:           71596176
         shift:          32
         mode:           3
         next_event:     21524167000000 nsecs
         set_next_event: lapic_next_event
         set_mode:       lapic_timer_setup
         event_handler:  hrtimer_interrupt
         retries:        891537

        Tick Device: mode:     1
        Per CPU device: 1
        Clock Event Device: lapic
         max_delta_ns:   128824925406
         min_delta_ns:   1000
         mult:           71596176
         shift:          32
         mode:           1
         next_event:     21524167000000 nsecs
         set_next_event: lapic_next_event
         set_mode:       lapic_timer_setup
         event_handler:  hrtimer_interrupt
         retries:        539190

 
/**
 * clockevents_register_device - register a clock event device
 * @dev:        device to register
 */
void clockevents_register_device(struct clock_event_device *dev)
{
        dev=&hpet_clockevent

        unsigned long flags;

        BUG_ON(dev->mode != CLOCK_EVT_MODE_UNUSED);
        BUG_ON(!dev->cpumask);

        spin_lock_irqsave(&clockevents_lock, flags);

        list_add(&dev->list, &clockevent_devices);
        clockevents_do_notify(CLOCK_EVT_NOTIFY_ADD, dev);
        clockevents_notify_released();

        spin_unlock_irqrestore(&clockevents_lock, flags);
}

/* Clock event notification values */
enum clock_event_nofitiers {
        CLOCK_EVT_NOTIFY_ADD,
        CLOCK_EVT_NOTIFY_BROADCAST_ON,
        CLOCK_EVT_NOTIFY_BROADCAST_OFF,
        CLOCK_EVT_NOTIFY_BROADCAST_FORCE,
        CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
        CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
        CLOCK_EVT_NOTIFY_SUSPEND,
        CLOCK_EVT_NOTIFY_RESUME,
        CLOCK_EVT_NOTIFY_CPU_DYING,
        CLOCK_EVT_NOTIFY_CPU_DEAD,
};

/*
 * Notify about a clock event change. Called with clockevents_lock
 * held.
 */
static void clockevents_do_notify(unsigned long reason, void *dev)
{
        raw_notifier_call_chain(&clockevents_chain, reason, dev);
}



/**     
 *      __raw_notifier_call_chain - Call functions in a raw notifier chain
 *      @nh: Pointer to head of the raw notifier chain
 *      @val: Value passed unmodified to notifier function
 *      @v: Pointer passed unmodified to notifier function
 *      @nr_to_call: See comment for notifier_call_chain.
 *      @nr_calls: See comment for notifier_call_chain
 *
 *      Calls each function in a notifier chain in turn.  The functions
 *      run in an undefined context.
 *      All locking must be provided by the caller.
 *
 *      If the return value of the notifier can be and'ed
 *      with %NOTIFY_STOP_MASK then raw_notifier_call_chain()
 *      will return immediately, with the return value of
 *      the notifier function which halted execution.
 *      Otherwise the return value is the return value
 *      of the last notifier function called.
 */     
int __raw_notifier_call_chain(struct raw_notifier_head *nh,
                              unsigned long val, void *v,
                              int nr_to_call, int *nr_calls)
{
        return notifier_call_chain(&nh->head, val, v, nr_to_call, nr_calls);
}
EXPORT_SYMBOL_GPL(__raw_notifier_call_chain);

int raw_notifier_call_chain(struct raw_notifier_head *nh,
                unsigned long val, void *v)
{
        return __raw_notifier_call_chain(nh, val, v, -1, NULL);
}

/**
 * notifier_call_chain - Informs the registered notifiers about an event.
 *      @nl:            Pointer to head of the blocking notifier chain
 *      @val:           Value passed unmodified to notifier function
 *      @v:             Pointer passed unmodified to notifier function
 *      @nr_to_call:    Number of notifier functions to be called. Don't care
 *                      value of this parameter is -1.
 *      @nr_calls:      Records the number of notifications sent. Don't care
 *                      value of this field is NULL.
 *      @returns:       notifier_call_chain returns the value returned by the
 *                      last notifier function called.
 */
static int __kprobes notifier_call_chain(struct notifier_block **nl,
                                        unsigned long val, void *v,
                                        int nr_to_call, int *nr_calls)
{
        int ret = NOTIFY_DONE;
        struct notifier_block *nb, *next_nb;

        nb = rcu_dereference(*nl);
        nb 指向 notifier_block

        while (nb && nr_to_call) {
                nr_to_call = -1

                next_nb = rcu_dereference(nb->next);

#ifdef CONFIG_DEBUG_NOTIFIERS
                if (unlikely(!func_ptr_is_kernel_text(nb->notifier_call))) {
                        WARN(1, "Invalid notifier called!");
                        nb = next_nb;
                        continue;
                }
#endif
                ret = nb->notifier_call(nb, val, v);
                调用 tick_notify 函数 val = CLOCK_EVT_NOTIFY_ADD 

                if (nr_calls)
                        (*nr_calls)++;

                if ((ret & NOTIFY_STOP_MASK) == NOTIFY_STOP_MASK)
                        break;
                nb = next_nb;
                nr_to_call--;
        }
        return ret;
}

/*
 * Notification about clock event devices
 */
static int tick_notify(struct notifier_block *nb, unsigned long reason,
                               void *dev)
{
        switch (reason) {

        case CLOCK_EVT_NOTIFY_ADD:
                return tick_check_new_device(dev);

        case CLOCK_EVT_NOTIFY_BROADCAST_ON:
        case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
        case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
                tick_broadcast_on_off(reason, dev);
                break;

        case CLOCK_EVT_NOTIFY_BROADCAST_ENTER:
        case CLOCK_EVT_NOTIFY_BROADCAST_EXIT:
                tick_broadcast_oneshot_control(reason);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DYING:
                tick_handover_do_timer(dev);
                break;

        case CLOCK_EVT_NOTIFY_CPU_DEAD:
                tick_shutdown_broadcast_oneshot(dev);
                tick_shutdown_broadcast(dev);
                tick_shutdown(dev);
                break;

        case CLOCK_EVT_NOTIFY_SUSPEND:
                tick_suspend();
                tick_suspend_broadcast();
                break;

        case CLOCK_EVT_NOTIFY_RESUME:
                tick_resume();
                break;

        default:
                break;
        }

        return NOTIFY_OK;
}

/*      
 * Check, if the new registered device should be used.
 */             
static int tick_check_new_device(struct clock_event_device *newdev)
{       
        newdev = &hpet_clockevent;

        struct clock_event_device *curdev;
        struct tick_device *td;
        int cpu, ret = NOTIFY_OK;
        unsigned long flags;
        
        spin_lock_irqsave(&tick_device_lock, flags);
                
        cpu = smp_processor_id();
        if (!cpumask_test_cpu(cpu, newdev->cpumask))
                goto out_bc;
                
        td = &per_cpu(tick_cpu_device, cpu);
        tick_cpu_device 是一个各CPU链表，包含了系统中每个CPU对应的struct tick_device 实例.

        curdev = td->evtdev;
        curdev = NULL;

        /* cpu local device ? */
        if (!cpumask_equal(newdev->cpumask, cpumask_of(cpu))) {

                /*
                 * If the cpu affinity of the device interrupt can not
                 * be set, ignore it.
                 */
                if (!irq_can_set_affinity(newdev->irq))
                        goto out_bc;

                /*
                 * If we have a cpu local device already, do not replace it
                 * by a non cpu local device
                 */
                if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
                        goto out_bc;
        }

        /*
         * If we have an active device, then check the rating and the oneshot
         * feature.
         */
        if (curdev) {
                /*
                 * Prefer one shot capable devices !
                 */
                if ((curdev->features & CLOCK_EVT_FEAT_ONESHOT) &&
                    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
                        goto out_bc;
                /*
                 * Check the rating
                 */
                if (curdev->rating >= newdev->rating)
                        goto out_bc;
        }

        /*
         * Replace the eventually existing device by the new
         * device. If the current device is the broadcast device, do
         * not give it back to the clockevents layer !
         */
        if (tick_is_broadcast_device(curdev)) {
                clockevents_shutdown(curdev);
                curdev = NULL;
        }
        clockevents_exchange_device(curdev, newdev);
        tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
        if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
                tick_oneshot_notify();

        spin_unlock_irqrestore(&tick_device_lock, flags);
        return NOTIFY_STOP;

out_bc:
        /*
         * Can the new device be used as a broadcast device ?
         */
        if (tick_check_broadcast_device(newdev))
                ret = NOTIFY_STOP;

        spin_unlock_irqrestore(&tick_device_lock, flags);

        return ret;
}

/**
 * clockevents_exchange_device - release and request clock devices
 * @old:        device to release (can be NULL)
 * @new:        device to request (can be NULL)
 *
 * Called from the notifier chain. clockevents_lock is held already
 */
void clockevents_exchange_device(struct clock_event_device *old,
                                 struct clock_event_device *new)
{
        old = NULL
        new = &hpet_clockevent 

        unsigned long flags;

        local_irq_save(flags);
        /*
         * Caller releases a clock event device. We queue it into the
         * released list and do a notify add later.
         */
        if (old) {
                clockevents_set_mode(old, CLOCK_EVT_MODE_UNUSED);
                list_del(&old->list);
                list_add(&old->list, &clockevents_released);
        }

        if (new) {
                BUG_ON(new->mode != CLOCK_EVT_MODE_UNUSED);
                clockevents_shutdown(new);
        }
        local_irq_restore(flags);
}

/**
 * clockevents_shutdown - shutdown the device and clear next_event
 * @dev:        device to shutdown
 */
void clockevents_shutdown(struct clock_event_device *dev)
{
        clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
        dev->next_event.tv64 = KTIME_MAX;
}


/*
 * Setup the tick device
 */
static void tick_setup_device(struct tick_device *td,
                              struct clock_event_device *newdev, int cpu,
                              const struct cpumask *cpumask)
{
        ktime_t next_event;
        void (*handler)(struct clock_event_device *) = NULL;

        /*
         * First device setup ?
         */
        if (!td->evtdev) {
        此时钟设备没有相关的时钟事件设备
                /*
                 * If no cpu took the do_timer update, assign it to
                 * this cpu:
                 */
                if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
                        如果没有选定时钟设备来承担全局时钟设备的角色，那么将选择当前设备来承担此职责

                        tick_do_timer_cpu = cpu;
                        设置为当前设备所属处理器编号
                        tick_next_period = ktime_get();

                        tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
                        时钟周期，纳秒
                        HZ = 1000
                }

                /*
                 * Startup in periodic mode first.
                 */
                td->mode = TICKDEV_MODE_PERIODIC;
                设备运行模式 --> 周期模式

        } else {
                handler = td->evtdev->event_handler;
                next_event = td->evtdev->next_event;
                td->evtdev->event_handler = clockevents_handle_noop;
        }

        td->evtdev = newdev;
        为时钟设备指定事件设备

        /*
         * When the device is not per cpu, pin the interrupt to the
         * current cpu:
         */
        if (!cpumask_equal(newdev->cpumask, cpumask))
                irq_set_affinity(newdev->irq, cpumask);

        /*
         * When global broadcasting is active, check if the current
         * device is registered as a placeholder for broadcast mode.
         * This allows us to handle this x86 misfeature in a generic
         * way.
         */
        if (tick_device_uses_broadcast(newdev, cpu))
                return;

        if (td->mode == TICKDEV_MODE_PERIODIC)
                tick_setup_periodic(newdev, 0);
        else
                tick_setup_oneshot(newdev, handler, next_event);
}

#define NSEC_PER_SEC    1000000000L

/*
 * Check, if the device is disfunctional and a place holder, which
 * needs to be handled by the broadcast device.
 */
int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
{               
        unsigned long flags;
        int ret = 0; 
                
        spin_lock_irqsave(&tick_broadcast_lock, flags);
                
        /*
         * Devices might be registered with both periodic and oneshot
         * mode disabled. This signals, that the device needs to be
         * operated from the broadcast device and is a placeholder for
         * the cpu local device.
         */
        if (!tick_device_is_functional(dev)) {
                dev->event_handler = tick_handle_periodic;
                cpumask_set_cpu(cpu, tick_get_broadcast_mask());
                tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
                ret = 1;
        } else {                             
                /*
                 * When the new device is not affected by the stop
                 * feature and the cpu is marked in the broadcast mask
                 * then clear the broadcast bit.
                 */
                if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
                        int cpu = smp_processor_id();
        
                        cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
                        tick_broadcast_clear_oneshot(cpu);
                }
        }
        spin_unlock_irqrestore(&tick_broadcast_lock, flags);
        return ret;
}

/*
 * Setup the device for a periodic tick
 */
void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
{
        tick_set_periodic_handler(dev, broadcast);

        /* Broadcast setup ? */
        if (!tick_device_is_functional(dev))
                return;

        if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
            !tick_broadcast_oneshot_active()) {
                clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
        } else {
                unsigned long seq;
                ktime_t next;

                do {
                        seq = read_seqbegin(&xtime_lock);
                        next = tick_next_period;
                } while (read_seqretry(&xtime_lock, seq));

                clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);

                for (;;) {
                        if (!clockevents_program_event(dev, next, ktime_get()))
                                return;
                        next = ktime_add(next, tick_period);
                }
        }
}

/*
 * Set the periodic handler depending on broadcast on/off
 */
void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
{
        if (!broadcast)
                dev->event_handler = tick_handle_periodic;
        else
                dev->event_handler = tick_handle_periodic_broadcast;
}               


run_timer_softirq()
   --> hrtimer_run_pending()
	  --> hrtimer_switch_to_hres()
	        --> tick_init_highres()
	              --> tick_switch_to_oneshot()
	                    --> tick_broadcast_switch_to_oneshot()
	                          --> tick_broadcast_setup_oneshot()

/**
 * tick_init_highres - switch to high resolution mode
 *
 * Called with interrupts disabled.
 */
int tick_init_highres(void)
{
        return tick_switch_to_oneshot(hrtimer_interrupt);
}

/**
 * tick_switch_to_oneshot - switch to oneshot mode
 */
int tick_switch_to_oneshot(void (*handler)(struct clock_event_device *))
{
        struct tick_device *td = &__get_cpu_var(tick_cpu_device);
        struct clock_event_device *dev = td->evtdev;

        if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT) ||
                    !tick_device_is_functional(dev)) {

                printk(KERN_INFO "Clockevents: "
                       "could not switch to one-shot mode:");
                if (!dev) {
                        printk(" no tick device\n");
                } else {
                        if (!tick_device_is_functional(dev))
                                printk(" %s is not functional.\n", dev->name);
                        else 
                                printk(" %s does not support one-shot mode.\n",
                                       dev->name);
                }    
                return -EINVAL;
        }    

        td->mode = TICKDEV_MODE_ONESHOT;
        dev->event_handler = handler;

        clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
        tick_broadcast_switch_to_oneshot();
        return 0;
}


/**
 * tick_broadcast_setup_oneshot - setup the broadcast device
 */
void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
{
        /* Set it up only once ! */
        if (bc->event_handler != tick_handle_oneshot_broadcast) {
                int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
                int cpu = smp_processor_id();

                bc->event_handler = tick_handle_oneshot_broadcast;
                clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);

                /* Take the do_timer update */
                tick_do_timer_cpu = cpu;

                /*
                 * We must be careful here. There might be other CPUs
                 * waiting for periodic broadcast. We need to set the
                 * oneshot_mask bits for those and program the
                 * broadcast device to fire.
                 */
                cpumask_copy(to_cpumask(tmpmask), tick_get_broadcast_mask());
                cpumask_clear_cpu(cpu, to_cpumask(tmpmask));
                cpumask_or(tick_get_broadcast_oneshot_mask(),
                           tick_get_broadcast_oneshot_mask(),
                           to_cpumask(tmpmask));

                if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {
                        tick_broadcast_init_next_event(to_cpumask(tmpmask),
                                                       tick_next_period);
                        tick_broadcast_set_event(tick_next_period, 1);
                } else
                        bc->next_event.tv64 = KTIME_MAX;
        }
}



timer_interrupt() 时钟中断处理程序
	--> global_clock_event->event_handler(global_clock_event); --> tick_handle_oneshot_broadcast

smp_apic_timer_interrupt()
	--> local_apic_timer_interrupt()
	            evt->event_handler(evt);  --> hrtimer_interrupt() 

