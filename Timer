/*
 * Initialize TSC and delay the periodic timer init to
 * late x86_late_time_init() so ioremap works.
 */
void __init time_init(void)
{
        late_time_init = x86_late_time_init;
}

start_kernel:
        if (late_time_init)
             late_time_init();

/*              
 * The platform setup functions are preset with the default functions
 * for standard PC hardware.
 */     
struct x86_init_ops x86_init __initdata = {
        
        .resources = {
                .probe_roms             = probe_roms,
                .reserve_resources      = reserve_standard_io_resources,
                .memory_setup           = default_machine_specific_memory_setup,
        },

        .mpparse = {
                .mpc_record             = x86_init_uint_noop,
                .setup_ioapic_ids       = x86_init_noop,
                .mpc_apic_id            = default_mpc_apic_id,
                .smp_read_mpc_oem       = default_smp_read_mpc_oem,
                .mpc_oem_bus_info       = default_mpc_oem_bus_info,
                .find_smp_config        = default_find_smp_config,
                .get_smp_config         = default_get_smp_config,
        },

        .irqs = {
                .pre_vector_init        = init_ISA_irqs,
                .intr_init              = native_init_IRQ,
                .trap_init              = x86_init_noop,
        }, 
                
        .oem = {
                .arch_setup             = x86_init_noop,
                .banner                 = default_banner,
        },

        .paging = {
                .pagetable_setup_start  = native_pagetable_setup_start,
                .pagetable_setup_done   = native_pagetable_setup_done,
        },

        .timers = {
                .setup_percpu_clockev   = setup_boot_APIC_clock,
                .tsc_pre_init           = x86_init_noop,
                .timer_init             = hpet_time_init,
        },
};


static __init void x86_late_time_init(void)
{
        x86_init.timers.timer_init();
        tsc_init();
}

/* Default timer init function */
void __init hpet_time_init(void)
{
        if (!hpet_enable())
                setup_pit_timer();
        setup_default_timer_irq();
}

/**
 * hpet_enable - Try to setup the HPET timer. Returns 1 on success.
 */
int __init hpet_enable(void)
{
        unsigned long id;
        int i;

        if (!is_hpet_capable())
                return 0;
                
        hpet_set_mapping();

        /*
         * Read the period and check for a sane value:
         */     
        hpet_period = hpet_readl(HPET_PERIOD);
                
        /*      
         * AMD SB700 based systems with spread spectrum enabled use a
         * SMM based HPET emulation to provide proper frequency
         * setting. The SMM code is initialized with the first HPET
         * register access and takes some time to complete. During
         * this time the config register reads 0xffffffff. We check
         * for max. 1000 loops whether the config register reads a non
         * 0xffffffff value to make sure that HPET is up and running
         * before we go further. A counting loop is safe, as the HPET
         * access takes thousands of CPU cycles. On non SB700 based
         * machines this check is only done once and has no side
         * effects.
         */
        for (i = 0; hpet_readl(HPET_CFG) == 0xFFFFFFFF; i++) {
                if (i == 1000) {
                        printk(KERN_WARNING
                               "HPET config register value = 0xFFFFFFFF. "
                               "Disabling HPET\n");
                        goto out_nohpet;
                }
        }
        
        if (hpet_period < HPET_MIN_PERIOD || hpet_period > HPET_MAX_PERIOD)
                goto out_nohpet;

        /*
         * Read the HPET ID register to retrieve the IRQ routing
         * information and the number of channels
         */     
        id = hpet_readl(HPET_ID);
        hpet_print_config();

#ifdef CONFIG_HPET_EMULATE_RTC
        /*
         * The legacy routing mode needs at least two channels, tick timer
         * and the rtc emulation channel.
         */
        if (!(id & HPET_ID_NUMBER))
                goto out_nohpet;
#endif

        if (hpet_clocksource_register())
                goto out_nohpet;

        if (id & HPET_ID_LEGSUP) {
                hpet_legacy_clockevent_register();
                hpet_msi_capability_lookup(2);
                return 1;
        }
        hpet_msi_capability_lookup(0);
        return 0;

out_nohpet:
        hpet_clear_mapping();
        hpet_address = 0;
        return 0;
}

static int hpet_clocksource_register(void)
{
        u64 start, now;
        cycle_t t1;

        /* Start the counter */
        hpet_restart_counter();

        /* Verify whether hpet counter works */
        t1 = hpet_readl(HPET_COUNTER);
        rdtscll(start);

        /*
         * We don't know the TSC frequency yet, but waiting for
         * 200000 TSC cycles is safe:
         * 4 GHz == 50us
         * 1 GHz == 200us
         */
        do {
                rep_nop();
                rdtscll(now);
        } while ((now - start) < 200000UL);

        if (t1 == hpet_readl(HPET_COUNTER)) {
                printk(KERN_WARNING
                       "HPET counter not counting. HPET disabled\n");
                return -ENODEV;
        }

        /*
         * The definition of mult is (include/linux/clocksource.h)
         * mult/2^shift = ns/cyc and hpet_period is in units of fsec/cyc
         * so we first need to convert hpet_period to ns/cyc units:
         *  mult/2^shift = ns/cyc = hpet_period/10^6
         *  mult = (hpet_period * 2^shift)/10^6
         *  mult = (hpet_period << shift)/FSEC_PER_NSEC
         */
        clocksource_hpet.mult = div_sc(hpet_period, FSEC_PER_NSEC, HPET_SHIFT);

        clocksource_register(&clocksource_hpet);

        return 0;
}

static struct clocksource clocksource_hpet = {
        .name           = "hpet",
        .rating         = 250,
        .read           = read_hpet,
        .mask           = HPET_MASK,
        .shift          = HPET_SHIFT,
        .flags          = CLOCK_SOURCE_IS_CONTINUOUS,
        .resume         = hpet_resume_counter,
#ifdef CONFIG_X86_64
        .vread          = vread_hpet,
#endif          
};    


/**
 * clocksource_register - Used to install new clocksources
 * @t:          clocksource to be registered
 *
 * Returns -EBUSY if registration fails, zero otherwise.
 */     
int clocksource_register(struct clocksource *cs)
{       
        /* calculate max idle time permitted for this clocksource */
        cs->max_idle_ns = clocksource_max_deferment(cs);

        mutex_lock(&clocksource_mutex);
        clocksource_enqueue(cs);
        clocksource_select();
        clocksource_enqueue_watchdog(cs);
        mutex_unlock(&clocksource_mutex);
        return 0;
}       

/*
 * Enqueue the clocksource sorted by rating
 */
static void clocksource_enqueue(struct clocksource *cs)
{
        struct list_head *entry = &clocksource_list;
        // static LIST_HEAD(clocksource_list);

        struct clocksource *tmp;

        list_for_each_entry(tmp, &clocksource_list, list)
                /* Keep track of the place, where to insert */
                if (tmp->rating >= cs->rating)
                        entry = &tmp->list;
        list_add(&cs->list, entry);
}


/**
 * clocksource_select - Select the best clocksource available
 *
 * Private function. Must hold clocksource_mutex when called.
 *
 * Select the clocksource with the best rating, or the clocksource,
 * which is selected by userspace override.
 */
static void clocksource_select(void)
{
        struct clocksource *best, *cs;

        if (!finished_booting || list_empty(&clocksource_list))
                return;
        /* First clocksource on the list has the best rating. */
        best = list_first_entry(&clocksource_list, struct clocksource, list);
        /* Check for the override clocksource. */
        list_for_each_entry(cs, &clocksource_list, list) {
                if (strcmp(cs->name, override_name) != 0)
                        continue;
                /*
                 * Check to make sure we don't switch to a non-highres
                 * capable clocksource if the tick code is in oneshot
                 * mode (highres or nohz)
                 */
                if (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&
                    tick_oneshot_mode_active()) {
                        /* Override clocksource cannot be used. */
                        printk(KERN_WARNING "Override clocksource %s is not "
                               "HRT compatible. Cannot switch while in "
                               "HRT/NOHZ mode\n", cs->name);
                        override_name[0] = 0;
                } else
                        /* Override clocksource can be used. */
                        best = cs;
                break;
        }
        if (curr_clocksource != best) {
                printk(KERN_INFO "Switching to clocksource %s\n", best->name);
                curr_clocksource = best;
                timekeeping_notify(curr_clocksource);
        }
}

/**                     
 * timekeeping_notify - Install a new clock source
 * @clock:              pointer to the clock source
 *      
 * This function is called from clocksource.c after a new, better clock
 * source has been registered. The caller holds the clocksource_mutex.
 */     
void timekeeping_notify(struct clocksource *clock)
{                       
        if (timekeeper.clock == clock)
                return;
        stop_machine(change_clocksource, clock, NULL);
        tick_clock_notify();
}


/*              
 * Initialize the conversion factor and the min/max deltas of the clock event
 * structure and register the clock event source with the framework.
 */             
void __init setup_pit_timer(void)
{               
        /*
         * Start pit with the boot cpu mask and make it global after the
         * IO_APIC has been initialized.
         */
        pit_ce.cpumask = cpumask_of(smp_processor_id());
        pit_ce.mult = div_sc(CLOCK_TICK_RATE, NSEC_PER_SEC, pit_ce.shift);
        pit_ce.max_delta_ns = clockevent_delta2ns(0x7FFF, &pit_ce);
        pit_ce.min_delta_ns = clockevent_delta2ns(0xF, &pit_ce);
                
        clockevents_register_device(&pit_ce);
        global_clock_event = &pit_ce;
}       

/*
 * On UP the PIT can serve all of the possible timer functions. On SMP systems
 * it can be solely used for the global tick.
 *
 * The profiling and update capabilities are switched off once the local apic is
 * registered. This mechanism replaces the previous #ifdef LOCAL_APIC -
 * !using_apic_timer decisions in do_timer_interrupt_hook()
 */
static struct clock_event_device pit_ce = {
        .name           = "pit",
        .features       = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT,
        .set_mode       = init_pit_timer,
        .set_next_event = pit_next_event,
        .shift          = 32,
        .irq            = 0,
};

void __init setup_default_timer_irq(void)
{       
        setup_irq(0, &irq0);
}

static struct irqaction irq0  = {
        .handler = timer_interrupt,
        .flags = IRQF_DISABLED | IRQF_NOBALANCING | IRQF_IRQPOLL | IRQF_TIMER,
        .name = "timer"
};

void __init tsc_init(void)
{
        u64 lpj;
        int cpu;
                
        x86_init.timers.tsc_pre_init();
                
        if (!cpu_has_tsc)
                return;
        
        tsc_khz = x86_platform.calibrate_tsc();
        // invoke native_calibrate_tsc(); calibrate the tsc on boot


        cpu_khz = tsc_khz;
        
        if (!tsc_khz) {
                mark_tsc_unstable("could not calculate TSC khz");
                return;
        }

        printk("Detected %lu.%03lu MHz processor.\n",
                        (unsigned long)cpu_khz / 1000,
                        (unsigned long)cpu_khz % 1000);
        
        /*
         * Secondary CPUs do not run through tsc_init(), so set up
         * all the scale factors for all CPUs, assuming the same
         * speed as the bootup CPU. (cpufreq notifiers will fix this
         * up if their speed diverges)
         */
        for_each_possible_cpu(cpu)
                set_cyc2ns_scale(cpu_khz, cpu);

        if (tsc_disabled > 0)
                return;
        
        /* now allow native_sched_clock() to use rdtsc */
        tsc_disabled = 0;

        lpj = ((u64)tsc_khz * 1000);
        do_div(lpj, HZ);
        lpj_fine = lpj;
        
        use_tsc_delay();
        /* Check and install the TSC clocksource */
        dmi_check_system(bad_tsc_dmi_table);

        if (unsynchronized_tsc())
                mark_tsc_unstable("TSCs unsynchronized");

        check_system_tsc_reliable();
        init_tsc_clocksource();
}       

struct x86_platform_ops x86_platform = {
        .calibrate_tsc                  = native_calibrate_tsc,
        .get_wallclock                  = mach_get_cmos_time,
        .set_wallclock                  = mach_set_rtc_mmss,
        .is_untracked_pat_range         = is_ISA_range,
        .nmi_init                       = default_nmi_init
};


static void __init init_tsc_clocksource(void)
{
        clocksource_tsc.mult = clocksource_khz2mult(tsc_khz,
                        clocksource_tsc.shift);
        if (tsc_clocksource_reliable)
                clocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;
        /* lower the rating if we already know its unstable: */
        if (check_tsc_unstable()) {
                clocksource_tsc.rating = 0;
                clocksource_tsc.flags &= ~CLOCK_SOURCE_IS_CONTINUOUS;
        }
        clocksource_register(&clocksource_tsc);
}


static struct clocksource clocksource_tsc = {
        .name                   = "tsc",
        .rating                 = 300,
        .read                   = read_tsc,
        .resume                 = resume_tsc,
        .mask                   = CLOCKSOURCE_MASK(64),
        .shift                  = 22,
        .flags                  = CLOCK_SOURCE_IS_CONTINUOUS |
                                  CLOCK_SOURCE_MUST_VERIFY,
#ifdef CONFIG_X86_64
        .vread                  = vread_tsc,
#endif
};
