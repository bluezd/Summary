void __init setup_per_cpu_areas(void)                       
{
        unsigned int cpu;
        unsigned long delta;
        int rc;         

        pr_info("NR_CPUS:%d nr_cpumask_bits:%d nr_cpu_ids:%d nr_node_ids:%d\n",
                NR_CPUS, nr_cpumask_bits, nr_cpu_ids, nr_node_ids);

        /*              
         * Allocate percpu area.  Embedding allocator is our favorite;
         * however, on NUMA configurations, it can result in very
         * sparse unit mapping and vmalloc area isn't spacious enough
         * on 32bit.  Use page in that case.                
         */             
#ifdef CONFIG_X86_32
        if (pcpu_chosen_fc == PCPU_FC_AUTO && pcpu_need_numa())
                pcpu_chosen_fc = PCPU_FC_PAGE;              
#endif      
        rc = -EINVAL;
        if (pcpu_chosen_fc != PCPU_FC_PAGE) {
                const size_t atom_size = cpu_has_pse ? PMD_SIZE : PAGE_SIZE;
                const size_t dyn_size = PERCPU_MODULE_RESERVE +
                        PERCPU_DYNAMIC_RESERVE - PERCPU_FIRST_CHUNK_RESERVE;

                rc = pcpu_embed_first_chunk(PERCPU_FIRST_CHUNK_RESERVE,
                                            dyn_size, atom_size,
                                            pcpu_cpu_distance,
                                            pcpu_fc_alloc, pcpu_fc_free);
                if (rc < 0)
                        pr_warning("PERCPU: %s allocator failed (%d), "
                                   "falling back to page size\n",
                                   pcpu_fc_names[pcpu_chosen_fc], rc);
        }
        if (rc < 0)
                rc = pcpu_page_first_chunk(PERCPU_FIRST_CHUNK_RESERVE,
                                           pcpu_fc_alloc, pcpu_fc_free,
                                           pcpup_populate_pte);
        if (rc < 0)
                panic("cannot initialize percpu area (err=%d)", rc);

        /* alrighty, percpu areas up and running */
        delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
        for_each_possible_cpu(cpu) {
                per_cpu_offset(cpu) = delta + pcpu_unit_offsets[cpu];
                per_cpu(this_cpu_off, cpu) = per_cpu_offset(cpu);
                per_cpu(cpu_number, cpu) = cpu;
                setup_percpu_segment(cpu);
                setup_stack_canary_segment(cpu);
                /*
                 * Copy data used in early init routines from the
                 * initial arrays to the per cpu data areas.  These
                 * arrays then become expendable and the *_early_ptr's
                 * are zeroed indicating that the static arrays are
                 * gone.
                 */
#ifdef CONFIG_X86_LOCAL_APIC
                per_cpu(x86_cpu_to_apicid, cpu) =
                        early_per_cpu_map(x86_cpu_to_apicid, cpu);
                per_cpu(x86_bios_cpu_apicid, cpu) =
                        early_per_cpu_map(x86_bios_cpu_apicid, cpu);
#endif
#ifdef CONFIG_X86_64
                per_cpu(irq_stack_ptr, cpu) =
                        per_cpu(irq_stack_union.irq_stack, cpu) +
                        IRQ_STACK_SIZE - 64;
#ifdef CONFIG_NUMA
                per_cpu(x86_cpu_to_node_map, cpu) =
                        early_per_cpu_map(x86_cpu_to_node_map, cpu);
#endif
#endif
                /*
                 * Up to this point, the boot CPU has been using .data.init
                 * area.  Reload any changed state for the boot CPU.
                 */
                if (cpu == boot_cpu_id)
                        switch_to_new_gdt(cpu);
        }

        /* indicate the early static arrays will soon be gone */
#ifdef CONFIG_X86_LOCAL_APIC
        early_per_cpu_ptr(x86_cpu_to_apicid) = NULL;
        early_per_cpu_ptr(x86_bios_cpu_apicid) = NULL;
#endif
#if defined(CONFIG_X86_64) && defined(CONFIG_NUMA)
        early_per_cpu_ptr(x86_cpu_to_node_map) = NULL;
#endif

#if defined(CONFIG_X86_64) && defined(CONFIG_NUMA)
        /*
         * make sure boot cpu node_number is right, when boot cpu is on the
         * node that doesn't have mem installed
         */
        per_cpu(node_number, boot_cpu_id) = cpu_to_node(boot_cpu_id);
#endif

        /* Setup node to cpumask map */
        setup_node_to_cpumask_map();

        /* Setup cpu initialized, callin, callout masks */
        setup_cpu_local_masks();
}

smp_processor_id()

# define smp_processor_id() raw_smp_processor_id()

CONFIG_X86_64_SMP
CONFIG_X86_32_SMP

#define raw_smp_processor_id() (percpu_read(cpu_number))

#define percpu_read(var)	percpu_from_op("mov", per_cpu__##var,	\
					       "m" (per_cpu__##var))

#define percpu_from_op(op, var, constraint)             \
({                                                      \
        typeof(var) ret__;                              \
        switch (sizeof(var)) {                          \
        case 1:                                         \
                asm(op "b "__percpu_arg(1)",%0"         \
                    : "=q" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 2:                                         \
                asm(op "w "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 4:                                         \
                asm(op "l "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 8:                                         \
                asm(op "q "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        default: __bad_percpu_size();                   \
        }                                               \
        ret__;                                          \
})

#define __percpu_arg(x)		"%%"__stringify(__percpu_seg)":%P" #x

asm("movb %%fs:%P1, %0"
	:"=q"(ret__)
	:"m"(per_cpu__cpu_number));

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

DEFINE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases) =
{       
        
        .clock_base =
        {       
                {       
                        .index = CLOCK_REALTIME,
                        .get_time = &ktime_get_real,
                        .resolution = KTIME_LOW_RES,
                },
                {       
                        .index = CLOCK_MONOTONIC,
                        .get_time = &ktime_get,
                        .resolution = KTIME_LOW_RES,
                },
        }
};


#define DEFINE_PER_CPU(type, name)                                      \
        DEFINE_PER_CPU_SECTION(type, name, "")

#define DEFINE_PER_CPU_SECTION(type, name, sec)                         \
        __PCPU_ATTRS(sec) PER_CPU_DEF_ATTRIBUTES                        \
        __typeof__(type) per_cpu__##name


#define __PCPU_ATTRS(sec)                                               \
        __attribute__((section(PER_CPU_BASE_SECTION sec)))              \
        PER_CPU_ATTRIBUTES
        // NULL

#define PER_CPU_BASE_SECTION ".data.percpu"

// __attribute__((section(.data.percpu "")

DEFINE_PER_CPU(struct hrtimer_cpu_base, hrtimer_bases)

:
__attribute__((section(.data.percpu))) __typeof__(hrtimer_cpu_base) per_cpu__hrtimer_bases
// 在 .data section 定义一个变量 per_cpu__hrtimer_bases 类型为 struct hrtimer_cpu_base





-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

cpu_base = &__raw_get_cpu_var(hrtimer_bases);

#define __raw_get_cpu_var(var) \
	        (*SHIFT_PERCPU_PTR(&per_cpu_var(var), __my_cpu_offset)) 
#define per_cpu_var(var) per_cpu__##var   // per_cpu_hrtimer_bases 

#define SHIFT_PERCPU_PTR(__p, __offset) RELOC_HIDE((__p), (__offset))

#define __my_cpu_offset per_cpu_offset(raw_smp_processor_id())


extern unsigned long __per_cpu_offset[NR_CPUS];

#define per_cpu_offset(x) (__per_cpu_offset[x])

unsigned long __per_cpu_offset[NR_CPUS] __read_mostly = {
	        [0 ... NR_CPUS-1] = BOOT_PERCPU_OFFSET,
};
EXPORT_SYMBOL(__per_cpu_offset);

初始化在 setup_per_cpu_areas() 

#define BOOT_PERCPU_OFFSET ((unsigned long)__per_cpu_load)


 +-------------------------------------------------------------------------------------+
 |      arch/x86/kernel/vmlinux.lds.S                                                  |
 |      /*                                                                             |
 |       *  * Per-cpu symbols which need to be offset from __per_cpu_load              |
 |       *   * for the boot processor.                                                 |
 |       *    */                                                                       |
 |      #define INIT_PER_CPU(x) init_per_cpu__##x = per_cpu__##x + __per_cpu_load      |
 |      INIT_PER_CPU(gdt_page);                                                        |
 |      INIT_PER_CPU(irq_stack_union);                                                 |
 |                                                                                     |
 +-------------------------------------------------------------------------------------+
 
#define __raw_get_cpu_var(var) \
	 (*SHIFT_PERCPU_PTR(&per_cpu_hrtimer_bases, __per_cpu_offset[CURRENT_CPU_NUMBER]))

=> RELOC_HIDE((&per_cpu_hrtimer_bases),(__per_cpu_offset[CURRENT_CPU_NUMBER]))

# define RELOC_HIDE(ptr, off)                                   \
  ({ unsigned long __ptr;                                       \
     __ptr = (unsigned long) (ptr);                             \
    (typeof(ptr)) (__ptr + (off)); })

*((struct hrtimer_cpu_base) (&per_cpu_hrtimer_bases + __per_cpu_offset[CURRENT_CPU_NUMBER]))

// 这样就获得了 cpu X 的 per cpu 变量 per_cpu_hrtimer_bases 。

__per_cpu_offset 保存的是 per cpu 变量的本地拷贝和 .data.percpu 段的地址的差值。


#define percpu_read(var)        percpu_from_op("mov", per_cpu__##var,   \
                                               "m" (per_cpu__##var))

#define percpu_from_op(op, var, constraint)             \
({                                                      \
        typeof(var) ret__;                              \
        switch (sizeof(var)) {                          \
        case 1:                                         \
                asm(op "b "__percpu_arg(1)",%0"         \
                    : "=q" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 2:                                         \
                asm(op "w "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 4:                                         \
                asm(op "l "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 8:                                         \
                asm(op "q "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        default: __bad_percpu_size();                   \
        }                                               \
        ret__;                                          \
})


asm("movb "__percpu_arg(1)",%0"        
     : "=q" (pfo_ret__)                  
     : constraint);

#define __percpu_arg(x)         "%%"__stringify(__percpu_seg)":%P" #x

#ifdef CONFIG_X86_64
#define __percpu_seg            gs
#define __percpu_mov_op         movq
#else
#define __percpu_seg            fs
#define __percpu_mov_op         movl
#endif

所以__percpu_arg(x)翻译过来就是：
"%%"__stringify(gs)":%P" #x

"%%"__stringify(gs)":%P" "1"

最终宏  __percpu_arg() "%%" "gs:%P" "1"


asm("movl %%fs:%P1, %0"
        :"=q"(ret__)
        :"m"(per_cpu__cpu_number));
// fs -> per_cpu_offset[cpu]
// per_cpu_offset[cpu] + &per_cpu__cpu_number 便得到了相应 cpu 的每 per_cpu__cpu_number.

+----+
| 32 |
+----+

static inline void setup_percpu_segment(int cpu)
{
#ifdef CONFIG_X86_32
        struct desc_struct gdt;

        pack_descriptor(&gdt, per_cpu_offset(cpu), 0xFFFFF,
                        0x2 | DESCTYPE_S, 0x8);
        gdt.s = 1;
        write_gdt_entry(get_cpu_gdt_table(cpu),
                        GDT_ENTRY_PERCPU, &gdt, DESCTYPE_S);
#endif
}
// 设置 GDT 描述符在 GDT 表中的偏移 GDT_ENTRY_PERCPU ，将相应 cpu 的每 cpu 变量写到 GDT 描述符地址字段。


static inline struct desc_struct *get_cpu_gdt_table(unsigned int cpu)
{
        return per_cpu(gdt_page, cpu).gdt;
}

DECLARE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page);

struct gdt_page {
        struct desc_struct gdt[GDT_ENTRIES];
} __attribute__((aligned(PAGE_SIZE)));

DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
#ifdef CONFIG_X86_64
        /*
         * We need valid kernel segments for data and code in long mode too
         * IRET will check the segment types  kkeil 2000/10/28
         * Also sysret mandates a special GDT layout
         *
         * TLS descriptors are currently at a different place compared to i386.
         * Hopefully nobody expects them at a fixed place (Wine?)
         */
        [GDT_ENTRY_KERNEL32_CS]         = GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
        [GDT_ENTRY_KERNEL_CS]           = GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
        [GDT_ENTRY_KERNEL_DS]           = GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
        [GDT_ENTRY_DEFAULT_USER32_CS]   = GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
        [GDT_ENTRY_DEFAULT_USER_DS]     = GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
        [GDT_ENTRY_DEFAULT_USER_CS]     = GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
#else
        [GDT_ENTRY_KERNEL_CS]           = GDT_ENTRY_INIT(0xc09a, 0, 0xfffff),
        [GDT_ENTRY_KERNEL_DS]           = GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
        [GDT_ENTRY_DEFAULT_USER_CS]     = GDT_ENTRY_INIT(0xc0fa, 0, 0xfffff),
        [GDT_ENTRY_DEFAULT_USER_DS]     = GDT_ENTRY_INIT(0xc0f2, 0, 0xfffff),
        /*
         * Segments used for calling PnP BIOS have byte granularity.
         * They code segments and data segments have fixed 64k limits,
         * the transfer segment sizes are set at run time.
         */
        /* 32-bit code */
        [GDT_ENTRY_PNPBIOS_CS32]        = GDT_ENTRY_INIT(0x409a, 0, 0xffff),
        /* 16-bit code */
        [GDT_ENTRY_PNPBIOS_CS16]        = GDT_ENTRY_INIT(0x009a, 0, 0xffff),
        /* 16-bit data */
        [GDT_ENTRY_PNPBIOS_DS]          = GDT_ENTRY_INIT(0x0092, 0, 0xffff),
        /* 16-bit data */
        [GDT_ENTRY_PNPBIOS_TS1]         = GDT_ENTRY_INIT(0x0092, 0, 0),
        /* 16-bit data */
        [GDT_ENTRY_PNPBIOS_TS2]         = GDT_ENTRY_INIT(0x0092, 0, 0),
        /*
         * The APM segments have byte granularity and their bases
         * are set at run time.  All have 64k limits.
         */
        /* 32-bit code */
        [GDT_ENTRY_APMBIOS_BASE]        = GDT_ENTRY_INIT(0x409a, 0, 0xffff),
        /* 16-bit code */
        [GDT_ENTRY_APMBIOS_BASE+1]      = GDT_ENTRY_INIT(0x009a, 0, 0xffff),
        /* data */
        [GDT_ENTRY_APMBIOS_BASE+2]      = GDT_ENTRY_INIT(0x4092, 0, 0xffff),

        [GDT_ENTRY_ESPFIX_SS]           = GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
        [GDT_ENTRY_PERCPU]              = GDT_ENTRY_INIT(0xc092, 0, 0xfffff),
        GDT_STACK_CANARY_INIT
#endif
} };


cpu 0 

void switch_to_new_gdt(int cpu)
{
        struct desc_ptr gdt_descr;
                        
        gdt_descr.address = (long)get_cpu_gdt_table(cpu);
        gdt_descr.size = GDT_SIZE - 1;
        load_gdt(&gdt_descr);
        /* Reload the per-cpu base */

        // 载入 cpu 0 的 GDT 描述符地址到 GDTR 寄存器中

        load_percpu_segment(cpu);
}       


void load_percpu_segment(int cpu)
{
#ifdef CONFIG_X86_32
        loadsegment(fs, __KERNEL_PERCPU);
        // 载入到段选择子 fs 中 
#else
        loadsegment(gs, 0);
        wrmsrl(MSR_GS_BASE, (unsigned long)per_cpu(irq_stack_union.gs_base, cpu));
#endif          
        load_stack_canary_segment();
}      



+--------+
| 64 bit |
+--------+

do_boot_cpu()

  --> initial_gs = per_cpu_offset(cpu);

......


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------

per_cpu(cpu_number, cpu) = cpu; // 假设 cpu 为 1


#define per_cpu(var, cpu) \        
        (*SHIFT_PERCPU_PTR(&per_cpu_var(var), per_cpu_offset(cpu)))

(*SHIFT_PERCPU_PTR(&per_cpu__cpu_number, __per_cpu_offset[1]))

// 将 cpu 1 的 per cpu 变量 per_cpu__cpu_number 的值设为 1

