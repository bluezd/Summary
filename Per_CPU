void __init setup_per_cpu_areas(void)                       
{
        unsigned int cpu;
        unsigned long delta;
        int rc;         

        pr_info("NR_CPUS:%d nr_cpumask_bits:%d nr_cpu_ids:%d nr_node_ids:%d\n",
                NR_CPUS, nr_cpumask_bits, nr_cpu_ids, nr_node_ids);

        /*              
         * Allocate percpu area.  Embedding allocator is our favorite;
         * however, on NUMA configurations, it can result in very
         * sparse unit mapping and vmalloc area isn't spacious enough
         * on 32bit.  Use page in that case.                
         */             
#ifdef CONFIG_X86_32
        if (pcpu_chosen_fc == PCPU_FC_AUTO && pcpu_need_numa())
                pcpu_chosen_fc = PCPU_FC_PAGE;              
#endif      
        rc = -EINVAL;
        if (pcpu_chosen_fc != PCPU_FC_PAGE) {
                const size_t atom_size = cpu_has_pse ? PMD_SIZE : PAGE_SIZE;
                const size_t dyn_size = PERCPU_MODULE_RESERVE +
                        PERCPU_DYNAMIC_RESERVE - PERCPU_FIRST_CHUNK_RESERVE;

                rc = pcpu_embed_first_chunk(PERCPU_FIRST_CHUNK_RESERVE,
                                            dyn_size, atom_size,
                                            pcpu_cpu_distance,
                                            pcpu_fc_alloc, pcpu_fc_free);
                if (rc < 0)
                        pr_warning("PERCPU: %s allocator failed (%d), "
                                   "falling back to page size\n",
                                   pcpu_fc_names[pcpu_chosen_fc], rc);
        }
        if (rc < 0)
                rc = pcpu_page_first_chunk(PERCPU_FIRST_CHUNK_RESERVE,
                                           pcpu_fc_alloc, pcpu_fc_free,
                                           pcpup_populate_pte);
        if (rc < 0)
                panic("cannot initialize percpu area (err=%d)", rc);

        /* alrighty, percpu areas up and running */
        delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
        for_each_possible_cpu(cpu) {
                per_cpu_offset(cpu) = delta + pcpu_unit_offsets[cpu];
                per_cpu(this_cpu_off, cpu) = per_cpu_offset(cpu);
                per_cpu(cpu_number, cpu) = cpu;
                setup_percpu_segment(cpu);
                setup_stack_canary_segment(cpu);
                /*
                 * Copy data used in early init routines from the
                 * initial arrays to the per cpu data areas.  These
                 * arrays then become expendable and the *_early_ptr's
                 * are zeroed indicating that the static arrays are
                 * gone.
                 */
#ifdef CONFIG_X86_LOCAL_APIC
                per_cpu(x86_cpu_to_apicid, cpu) =
                        early_per_cpu_map(x86_cpu_to_apicid, cpu);
                per_cpu(x86_bios_cpu_apicid, cpu) =
                        early_per_cpu_map(x86_bios_cpu_apicid, cpu);
#endif
#ifdef CONFIG_X86_64
                per_cpu(irq_stack_ptr, cpu) =
                        per_cpu(irq_stack_union.irq_stack, cpu) +
                        IRQ_STACK_SIZE - 64;
#ifdef CONFIG_NUMA
                per_cpu(x86_cpu_to_node_map, cpu) =
                        early_per_cpu_map(x86_cpu_to_node_map, cpu);
#endif
#endif
                /*
                 * Up to this point, the boot CPU has been using .data.init
                 * area.  Reload any changed state for the boot CPU.
                 */
                if (cpu == boot_cpu_id)
                        switch_to_new_gdt(cpu);
        }

        /* indicate the early static arrays will soon be gone */
#ifdef CONFIG_X86_LOCAL_APIC
        early_per_cpu_ptr(x86_cpu_to_apicid) = NULL;
        early_per_cpu_ptr(x86_bios_cpu_apicid) = NULL;
#endif
#if defined(CONFIG_X86_64) && defined(CONFIG_NUMA)
        early_per_cpu_ptr(x86_cpu_to_node_map) = NULL;
#endif

#if defined(CONFIG_X86_64) && defined(CONFIG_NUMA)
        /*
         * make sure boot cpu node_number is right, when boot cpu is on the
         * node that doesn't have mem installed
         */
        per_cpu(node_number, boot_cpu_id) = cpu_to_node(boot_cpu_id);
#endif

        /* Setup node to cpumask map */
        setup_node_to_cpumask_map();

        /* Setup cpu initialized, callin, callout masks */
        setup_cpu_local_masks();
}

smp_processor_id()

# define smp_processor_id() raw_smp_processor_id()

CONFIG_X86_64_SMP
CONFIG_X86_32_SMP

#define raw_smp_processor_id() (percpu_read(cpu_number))

#define percpu_read(var)	percpu_from_op("mov", per_cpu__##var,	\
					       "m" (per_cpu__##var))

#define percpu_from_op(op, var, constraint)             \
({                                                      \
        typeof(var) ret__;                              \
        switch (sizeof(var)) {                          \
        case 1:                                         \
                asm(op "b "__percpu_arg(1)",%0"         \
                    : "=q" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 2:                                         \
                asm(op "w "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 4:                                         \
                asm(op "l "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        case 8:                                         \
                asm(op "q "__percpu_arg(1)",%0"         \
                    : "=r" (ret__)                      \
                    : constraint);                      \
                break;                                  \
        default: __bad_percpu_size();                   \
        }                                               \
        ret__;                                          \
})

#define __percpu_arg(x)		"%%"__stringify(__percpu_seg)":%P" #x

asm("movb %%fs:%P1, %0"
	:"=q"(ret__)
	:"m"(per_cpu__cpu_number));
